I"–*<div class="imgcap">
    <div>
        <img src="/assets/3-entropy/entropy_background.png" width="400" />
    </div>
    <div class="thecap">H1: LÃ½ thuyáº¿t thÃ´ng tin </div>
</div>
<hr />

<p><a name="--information-theory-and-entropy"></a></p>

<h2 id="1-cÆ¡-báº£n-vá»-lÃ½-thuyáº¿t-thÃ´ng-tin-vÃ -khÃ¡i-niá»‡m-entropy">1. CÆ¡ báº£n vá» lÃ½ thuyáº¿t thÃ´ng tin vÃ  khÃ¡i niá»‡m entropy</h2>

<!-- What is entropy -->
<p><a name="-what-is-entropy"><a></a></a></p>

<h3 id="11-entropy-nghÄ©a-lÃ -gÃ¬">1.1 Entropy nghÄ©a lÃ  gÃ¬?</h3>

<p>Ã tÆ°á»Ÿng vá» Entropy ban Ä‘áº§u khÃ¡ khÃ³ hiá»ƒu, cÃ³ khÃ¡ nhiá»u tá»« Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ mÃ´ táº£ nÃ³: Ä‘Ã³ lÃ  sá»± há»—n loáº¡n, khÃ´ng cháº¯c cháº¯c, Ä‘á»™ báº¥t Ä‘á»‹nh, Ä‘á»™ báº¥t ngá», lÆ°á»£ng thÃ´ng tin hay nhá»¯ng Ä‘á»‹nh nghÄ©a tÆ°Æ¡ng tá»± tháº¿. Náº¿u cho Ä‘áº¿n giá» báº¡n váº«n chÆ°a rÃµ vá» nÃ³, thÃ¬ báº¡n tá»›i Ä‘Ãºng nÆ¡i rá»“i Ä‘áº¥y. TÃ´i sáº½ giÃºp báº¡n hiá»ƒu rÃµ vá» Entropy.</p>

<div class="imgcap">
<div>
   <img src="/assets/3-entropy/shanon.jpg" width="300" />
</div>
<div class="thecap">H2: Cha Ä‘áº» cá»§a LÃ½ thuyáº¿t thÃ´ng tin: Claude Shannon </div>
</div>

<!-- Nguá»“n gá»‘c entropy -->
<p><a name="-entropy-root"></a></p>

<h3 id="12-ai-nghÄ©-ra-entropy-vÃ -vÃ¬-lÃ½-do-gÃ¬-">1.2 Ai nghÄ© ra Entropy vÃ  vÃ¬ lÃ½ do gÃ¬ ?</h3>
<p>NÄƒm 1948, Claude Shannon láº§n Ä‘áº§u nháº¯c tá»›i khÃ¡i niá»‡m information entropy trong bÃ i viáº¿t â€œâ€œA Mathematical Theory of Communicationâ€. Nhiá»u ngÆ°á»i dá»‹ch information entropy lÃ  entropy thÃ´ng tin, Ä‘á»™ báº¥t Ä‘á»‹nh thÃ´ng tin - tuy nhiÃªn mÃ¬nh nghÄ© dÃ¹ng tá»« gá»‘c sáº½ tá»‘t hÆ¡n vÃ¬ dÃ¢n láº­p trÃ¬nh thÃ¬ nÃªn lÃ m quen vá»›i cÃ¡c khÃ¡i niá»‡m tiáº¿ng Anh hÆ¡n.</p>

<p>Shannon Ä‘Ã£ nghiÃªn cá»©u vá» cÃ¡ch Ä‘á»ƒ truyá»n tin hiá»‡u quáº£ mÃ  khÃ´ng bá»‹ máº¥t mÃ¡t thÃ´ng tin. Khi má»™t thÃ´ng tin Ä‘Æ°á»£c truyá»n Ä‘i giá»¯a cÃ¡c nÆ¡i, nÃ³ cáº§n mÃ£ hoÃ¡ thÃ nh 1 cáº¥u trÃºc dá»¯ liá»‡u nhá» nháº¥t. Äá»“ng thá»i, viá»‡c mÃ£ hÃ³a Ä‘Ã³ khÃ´ng Ä‘Æ°á»£c phÃ©p lÃ m máº¥t, sai lá»‡ch thÃ´ng tin nÆ¡i nháº­n. Bá»™ giáº£i mÃ£ nÆ¡i nháº­n pháº£i cÃ³ kháº£ nÄƒng khÃ´i phá»¥c láº¡i thÃ´ng tin giá»‘ng nhÆ° thÃ´ng tin gá»‘c. VÃ  nhÆ° váº­y ta cáº§n má»™t cÆ¡ cháº¿ hay Ä‘á»™ Ä‘o Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ hiá»‡u quáº£ cá»§a viá»‡c mÃ£ hoÃ¡ nÃ y.</p>

<div class="imgcap">
<div>
   <img src="/assets/3-entropy/entropy-1.png" width="500" />
</div>
</div>

<p>Shannon Ä‘Ã£ Ä‘á»‹nh nghÄ©a Entropy lÃ  kÃ­ch cá»¡ trung bÃ¬nh nhá» nháº¥t cÃ³ thá»ƒ cá»§a cÃ¡c message Ä‘Æ°á»£c gá»­i Ä‘i tá»« nguá»“n tá»›i Ä‘Ã­ch khi mÃ£ hoÃ¡, viá»‡c mÃ£ hoÃ¡ nÃ y pháº£i Ä‘áº£m báº£o khÃ´ng máº¥t mÃ¡t thÃ´ng tin. Ã”ng Ä‘Ã£ mÃ´ táº£ cÃ¡ch Ä‘á»ƒ tÃ­nh entropy - ráº¥t há»¯u Ã­ch trong viá»‡c tÃ­nh toÃ¡n hiá»‡u quáº£ cá»§a kÃªnh truyá»n dá»¯ liá»‡u. Äá»‹nh nghÄ©a trÃªn Ä‘Ã¢y cÃ³ láº½ váº«n chÆ°a rÃµ rÃ ng vá»›i báº¡n nhá»‰. TÃ´i sáº½ mÃ´ táº£ vÃ i vÃ­ dá»¥ cho báº¡n hiá»ƒu.</p>

<!-- Entropy detail -->
<p><a name="-entropy-detail"></a></p>

<h2 id="2-Ä‘i-vÃ o-chi-tiáº¿t-entropy">2. Äi vÃ o chi tiáº¿t entropy</h2>

<p><a name="-encoding"></a></p>

<h3 id="21-mÃ£-hÃ³a-thÃ´ng-tin">2.1 MÃ£ hÃ³a thÃ´ng tin.</h3>

<div class="imgcap">
    <div>
    <img src="/assets/3-entropy/entropy-2.png" width="500" />
    </div>
</div>
<hr />

<p>Giáº£ sá»­ báº¡n muá»‘n truyá»n 1 message tá»« Tokyo tá»›i New York vá»›i ná»™i dung vá» thá»i tiáº¿t cá»§a Tokyo. CÃ¡ch á»Ÿ trÃªn cÃ³ tá»‘t khÃ´ng ? Giáº£ sá»­ cáº£ ngÆ°á»i gá»­i vÃ  nháº­n Ä‘á»u biáº¿t ráº±ng ná»™i dung message Ä‘á»u nÃ³i vá» thá»i tiáº¿t cá»§a Tokyo. Váº­y há» sáº½ khÃ´ng cáº§n pháº£i gá»­i nhá»¯ng tá»« nhÆ° â€œThá»i tiáº¿tâ€, â€œTokyoâ€, â€œcá»§aâ€ â€¦ Há» Ä‘Æ¡n giáº£n chá»‰ cáº§n nÃ³i â€œFineâ€, â€œNot fineâ€, nhÆ° váº­y lÃ  Ä‘á»§.</p>

<p>MÃ£ hÃ³a vá»›i 2 loáº¡i message â€œfineâ€, â€œnot fineâ€nhÆ° váº­y tá»‘t chÆ°a ? ChÆ°a, bá»Ÿi vÃ¬ cÃ¢u há»i dáº¡ng â€œyesâ€, â€œnoâ€, ta chá»‰ cáº§n 1 bit Ä‘á»ƒ mÃ£ hÃ³a: giÃ¡ trá»‹ 0 cho â€œFineâ€, 1 cho â€œNot fineâ€.</p>

<div class="imgcap">
    <div>
    <img src="/assets/3-entropy/entropy-3.png" width="500" />
    </div>
</div>
<hr />

<p>NhÆ° váº­y cÃ¡ch mÃ£ hÃ³a nÃ y Ä‘Ã£ Ä‘áº£m báº£o khÃ´ng lÃ m máº¥t mÃ¡t thÃ´ng tin. Tháº¿ bÃ¢y giá» ta muá»‘n nÃ³i thÃªm vá» mÆ°a, tuyáº¿t (Rainy, Cloudy) thÃ¬ sao ? 1 bit sáº½ khÃ´ng Ä‘á»§ Ä‘á»ƒ mÃ£ hÃ³a sá»‘ trÆ°á»ng há»£p Ä‘Ã³. Tháº¿ ta thá»­ 2 bit:</p>

<div class="imgcap">
    <div>
    <img src="/assets/3-entropy/entropy-4.png" width="500" />
    </div>
</div>
<hr />

<p>â€œFineâ€ lÃ  00, tuy nhiÃªn ta cÃ³ thá»ƒ rÃºt gá»n nÃ³ thÃ nh 0 vÃ¬ 2 trÆ°á»ng há»£p cÃ²n láº¡i Ä‘á»u báº¯t Ä‘áº§u báº±ng sá»‘ 1. Do â€œNot fineâ€ Ä‘Ã£ bao trÃ¹m cÃ¡c trÆ°á»ng há»£p cÃ²n láº¡i (khÃ¡c â€œfineâ€) nÃªn ta cÃ³ thá»ƒ bá». Náº¿u thÃªm â€œSnowâ€ vÃ o, ta cÃ³ thá»ƒ mÃ£ hÃ³a nhÆ° sau:</p>

<div class="imgcap">
    <div>
    <img src="/assets/3-entropy/entropy-5.png" width="500" />
    </div>
</div>
<hr />

<p>NhÆ° váº­y, chÃºng ta Ä‘Ã£ hiá»ƒu viá»‡c mÃ£ hÃ³a báº±ng cÃ¡c bit 0-1 lÃ  nhÆ° nÃ o rá»“i nhÃ©. Tiáº¿p Ä‘áº¿n, chÃºng ta cáº§n biáº¿t liá»‡u cÃ¡ch mÃ£ hÃ³a nhÆ° trÃªn Ä‘Ã£ tá»‘i Æ°u chÆ°a.</p>
<hr />

<!-- 2.2 TÃ­nh kÃ­ch cá»¡ mÃ£ hÃ³a trung bÃ¬nh.-->
<p><a name="-encode-average-size"></a></p>

<h3 id="22-kÃ­ch-cá»¡-mÃ£-hÃ³a-trung-bÃ¬nh">2.2 KÃ­ch cá»¡ mÃ£ hÃ³a trung bÃ¬nh.</h3>
<p>Giáº£ sá»­ Tokyo liÃªn tá»¥c gá»­i thÃ´ng tin vá» thá»i tiáº¿t tá»›i New York háº±ng giá» vá»›i cÃ¡ch mÃ£ hÃ³a nhÆ° Ä‘Ã£ nÃ³i trÃªn. Sau khi thu tháº­p, tá»•ng há»£p thÃ´ng tin, ta cÃ³ 1 phÃ¢n phá»‘i xÃ¡c suáº¥t vá» táº§n suáº¥t gá»­i vá»›i tá»«ng loáº¡i thÃ´ng tin nhÆ° sau:</p>

<div class="imgcap">
    <div>
    <img src="/assets/3-entropy/entropy-6.png" width="500" />
    </div>
</div>
<hr />

<p>Dá»±a vÃ o táº§n suáº¥t vÃ  báº£ng mÃ£ hoÃ¡, ta tÃ­nh sá»‘ bit trung bÃ¬nh cá»§a má»™t tin nháº¯n lÃ :</p>

\[(0.6 * 1 )+(0.38*2)+(0.01 * 3)+(0.01 * 3)=1.42\]

<p>NhÆ° váº­y, vá»›i cÃ¡ch báº£ng mÃ£ hoÃ¡ trÃªn, trung bÃ¬nh má»—i tin nháº¯n gá»­i Ä‘i dÃ i 1.42 bit. Báº¡n cÃ³ thá»ƒ tháº¥y, giÃ¡ trá»‹ trung bÃ¬nh nÃ y phá»¥ thuá»™c vÃ o cáº£ phÃ¢n phá»‘i xÃ¡c suáº¥t vÃ  cÃ¡ch mÃ£ hÃ³a má»—i 1 loáº¡i tin. VÃ­ dá»¥, trong trÆ°á»ng há»£p phÃ¢n phá»‘i xÃ¡c suáº¥t thay Ä‘á»•i: Fine = 10%, Cloudy = 10%, Rainy = 40%, Snow = 40%, Ä‘á»™ dÃ i trung bÃ¬nh má»—i tin nháº¯n lÃ  :</p>

\[(0.1*1) + ( 0.1 * 2)+(0.4 * 3 )+(0.4 * 3) = 2.7\]

<p>NhÆ° váº­y, chÃºng ta Ä‘Ã£ biáº¿t cÃ¡ch tÃ­nh Ä‘á»™ dÃ i mÃ£ hÃ³a trung bÃ¬nh rá»“i, nhÆ°ng lÃ m cÃ¡ch nÃ o Ä‘á»ƒ biáº¿t giÃ¡ trá»‹ nÃ y Ä‘Ã£ lÃ  nhá» nháº¥t hay chÆ°a, nÃ³i cÃ¡ch khÃ¡c liá»‡u cÃ¡ch mÃ£ hÃ³a cá»§a chÃºng ta Ä‘Ã£ tá»‘i Æ°u hay chÆ°a.</p>
<hr />

<!--  -->
<p><a name="-encode-min-average-size"></a></p>

<h3 id="23-kÃ­ch-cá»¡-mÃ£-hÃ³a-trung-bÃ¬nh-nhá»-nháº¥t">2.3 KÃ­ch cá»¡ mÃ£ hÃ³a trung bÃ¬nh NHá» NHáº¤T.</h3>
<p>Ta cÃ³ thá»ƒ thá»­ nhiá»u báº£ng mÃ£ hoÃ¡ khÃ¡c nhau vÃ  ngá»“i tÃ­nh tay ra Ä‘á»™ dÃ i mÃ£ hÃ³a trung bÃ¬nh cá»§a tá»«ng trÆ°á»ng há»£p rá»“i chá»n xem cÃ¡ch nÃ o cho Ä‘á»™ dÃ i nhá» nháº¥tâ€¦ Tuy nhiÃªn cÃ¡ch nÃ y cháº¯c cháº¯n khÃ´ng á»•n. Váº­y náº¿u ta cÃ³ 1 phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a dá»¯ liá»‡u, cÃ³ cÃ¡ch nÃ o Ä‘á»ƒ tÃ­nh luÃ´n ra Ä‘á»™ dÃ i trung bÃ¬nh nhá» nháº¥t hay khÃ´ng?</p>

<p>Ta cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh sá»‘ bit mÃ£ hoÃ¡ cho 1 loáº¡i message dá»±a vÃ o táº§n suáº¥t xuáº¥t hiá»‡n cá»§a nÃ³. Cá»¥ thá»ƒ: loáº¡i message nÃ o xuáº¥t hiá»‡n vá»›i táº§n suáº¥t cÃ ng lá»›n, sá»‘ bit mÃ£ hoÃ¡ cho nÃ³ trong báº£ng mÃ£ hoÃ¡ cÃ ng pháº£i nhá». Tá»©c sá»‘ bit mÃ£ hoÃ¡ cho 1 message tá»· lá»‡ nghá»‹ch vá»›i táº§n suáº¥t xuáº¥t hiá»‡n cá»§a nÃ³.</p>

<p>Giáº£ sá»­ ta cÃ³ 8 kiá»ƒu tin nháº¯n khÃ¡c nhau cÃ¹ng xuáº¥t hiá»‡n vá»›i xÃ¡c suáº¥t \(p=1/8 = 12.5%\). 1 bit thÃ¬ mÃ£ hÃ³a Ä‘Æ°á»£c 2 giÃ¡ trá»‹. tháº¿ 2 bit mÃ£ hÃ³a  Ä‘Æ°á»£c 4, 3 bit Ä‘Æ°á»£c 8 giÃ¡ trá»‹. NhÆ° váº­y ta chá»‰ cáº§n 3 bit lÃ  mÃ£ hÃ³a Ä‘Æ°á»£c, thÃªm 1 bit vÃ o cÅ©ng khÃ´ng cÃ³ thÃªm tÃ¡c dá»¥ng gÃ¬. ThÃ´ng thÆ°á»ng, vá»›i N giÃ¡ trá»‹, ta cáº§n   \(\log_2^N\) bit Ä‘á»ƒ mÃ£ hÃ³a. Vá»›i 1 loáº¡i tin nháº¯n xuáº¥t hiá»‡n 1 trong N láº§n gá»­i tin, cÃ´ng thá»©c bÃªn trÃªn mÃ´ táº£ kÃ­ch cá»¡ tá»‘i thiá»ƒu cáº§n thiáº¿t Ä‘á»ƒ mÃ£ hÃ³a.
VÃ  vÃ¬ \(P = \frac{1}{N}\) lÃ  xÃ¡c suáº¥t cá»§a loáº¡i tin nháº¯n Ä‘Ã³, phÆ°Æ¡ng trÃ¬nh cÃ³ thá»ƒ viáº¿t láº¡i thÃ nh</p>

\[\log_2^N = -\log_2^{1/N} = -\log_2^{p}\]

<p>NhÆ° váº­y \(-\log_2^{P}\) lÃ  sá»‘ bit tá»‘i thiá»ƒu Ä‘á»ƒ mÃ£ hoÃ¡ 1 message vá»›i xÃ¡c suáº¥t xuáº¥t hiá»‡n \(p\)</p>

<p>Káº¿t há»£p vá»›i cÃ´ng thá»©c tÃ­nh kÃ­ch cá»¡ trung bÃ¬nh Ä‘Ã£ biáº¿t á»Ÿ pháº§n trÆ°á»›c, ta Ä‘Æ°á»£c kÃ­ch thÆ°á»›c trung bÃ¬nh tá»‘i thiá»ƒu cá»§a 1 message Ä‘Æ°á»£c gá»­i Ä‘i khi biáº¿t táº§n suáº¥t xuáº¥t hiá»‡n tá»«ng loáº¡i message - Ä‘Ã¢y chÃ­nh lÃ  entropy:</p>

\[Entropy = -\sum_{i}^{n} p_i * \log_2^{p_i}\]

<p>Trong Ä‘Ã³ \(p_i\) lÃ  xÃ¡c suáº¥t cá»§a loáº¡i tin nháº¯n thá»© i. HÃ£y ngá»“i vÃ  suy ngáº«m vá» cÃ´ng thá»©c nÃ y, khÃ´ng cÃ³ gÃ¬ magic á»Ÿ Ä‘Ã¢y cáº£, cÃ´ng thá»©c nÃ y Ä‘Æ¡n giáº£n lÃ  sá»± káº¿t há»£p giá»¯a tÃ­nh kÃ­ch cá»¡ mÃ£ hÃ³a trung bÃ¬nh vÃ  kÃ­ch cá»¡ mÃ£ hÃ³a nhá» nháº¥t cá»§a tá»«ng loáº¡i tin. Thá»­ 1 vÃ i vÃ­ dá»¥:</p>

<div class="imgcap">
    <div>
    <img src="/assets/3-entropy/entropy-7.png" width="500" />
    </div>
</div>

<div class="imgcap">
    <div>
    <img src="/assets/3-entropy/entropy-8.png" width="500" />
    </div>
</div>

<p>Váº­y, entropy lÃ : \((0.5 * 1)+(0.25 * 2)+(0.125 * 3)+(0.125 * 3)=1.75\) bits</p>
<hr />

<!-- TÃ­nh cháº¥t entropy ########### -->
<p><a name="-entropy-property"></a></p>

<h3 id="24-tÃ­nh-cháº¥t-cá»§a-entropy">2.4 TÃ­nh cháº¥t cá»§a Entropy</h3>

<p>Vá»›i cÃ´ng thá»©c tÃ­nh entropy á»Ÿ trÃªn, entropy Ä‘Æ°á»£c tÃ­nh hoÃ n toÃ n dá»±a vÃ o xÃ¡c suáº¥t. Giáº£ sá»­ vá»›i phÃ¢n phá»‘i xÃ¡c suáº¥t vá» thá»i tiáº¿t (mÆ°a, náº¯ng) lÃ  P = {p1, p2}. NgÆ°á»i ta nháº­n ra Entropy(P) Ä‘áº¡t max khi p1=p2. Khi p1 vÃ  p2 cÃ ng lá»‡ch nhau thÃ¬ Entropy(P) cÃ ng giáº£m. Náº¿u p1=p2=0.5, entropy Ä‘áº¡t max, ta ráº¥t khÃ³ Ä‘á»ƒ Ä‘oÃ¡n thá»i tiáº¿t ngÃ y mai mÆ°a hay náº¯ng. Náº¿u p1=0.1 vÃ  p2=0.9, ta sáº½ tá»± tin Ä‘oÃ¡n ráº±ng ngÃ y mai trá»i sáº½ náº¯ng, lÃºc nÃ y entropy cÃ³ giÃ¡ trá»‹ tháº¥p hÆ¡n nhiá»u.</p>

<p>Entropy cao Ä‘á»“ng nghÄ©a vá»›i viá»‡c ta khÃ³ Ä‘oÃ¡n trÆ°á»›c Ä‘Æ°á»£c sá»± kiá»‡n sáº¯p xáº£y ra. Ta cÃ³ thá»ƒ gá»i Ä‘Ã³ lÃ  sá»± báº¥t Ä‘á»‹nh, sá»± báº¥t á»•n hay entropy lÃ  1 thÆ°á»›c Ä‘o sá»± â€œkhÃ³ Ä‘oÃ¡nâ€ cá»§a thÃ´ng tin sáº¯p xáº£y ra trong tÆ°Æ¡ng lai theo 1 phÃ¢n phá»‘i xÃ¡c suáº¥t.</p>

<p>Entropy Ä‘Æ°á»£c sá»­ dá»¥ng nhiá»u trong lÄ©nh vá»±c Machine learning, Deep learning. TiÃªu biá»ƒu lÃ  <strong>cross entropy</strong> trong Loss function vÃ  thuáº­t toÃ¡n <strong>Infomation Gain</strong> trong thuáº­t toÃ¡n CÃ¢y quyáº¿t Ä‘á»‹nh.</p>

<p><a name="-refer"></a></p>

<h3 id="3-tÃ i-liá»‡u-tham-kháº£o">3. TÃ i liá»‡u tham kháº£o</h3>
<ol>
  <li><a href="https://towardsdatascience.com/demystifying-entropy-f2c3221e2550">Demystifying entropy</a></li>
</ol>
:ET