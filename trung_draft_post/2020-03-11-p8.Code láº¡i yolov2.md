---
layout: post
title: "Bài 8: Cùng code YOLOv2 - YOLO tutorial P3"
title2: "8. Cùng code YOLOv2 - YOLO tutorial P3"
tag: [ALL POST, yolo tutorial]
img: assets/6-yolo/yologo_2.png
category: [computer vision]
summary: Trải qua 2 bài lý thuyết, bây giờ là lúc bắt tay vào code.
---

Tổng quan:
-[](#)
- [1. Tiền xử lí dữ liệu ](#-1)
    - [1.1 Đọc hiểu annotated data](#-1.1)
    - [1.2 Tiền xử lí trong utils.py](#-1.2)
- [2. Kiến trúc model](#)
- [3. Loss function](#)
- [4. Hậu xử lí output](#)


<div class="imgcap">
    <div >
        <img src="https://i.imgur.com/yBpsMgh.png" width="400">
    </div>
    <div class="thecap">H1: Kết qủa thu được </div>
</div>

Trong hai bài trước, mình đã viết về thuật toán YOLO và YOLO_v2. Nhưng đọc thuật toán suông thì khó hiểu. Để hiểu sâu hơn, cách tốt nhất là cùng bắt tay vào code. Mình sẽ code yolo cho Face Detect.

Github: [github trungthanhnguyen0502](https://github.com/trungthanhnguyen0502/yolo-2)

<!-- ############ -->
<a name="-1"></a>

## 1.Tiền xử lí dữ liệu:

<!-- ############ -->
<a name="-1.1"></a>

### 1.1 Đọc hiểu annotated data

Dataset: [Wider Face](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/). 
Cấu trúc thư mục của WiderFace gồm 2 phần: WIDER_ANNS chứa annotated data và WIDER_TRAIN chứa ảnh.

<!-- image code -->
<div class="imgcap">
    <div >
        <img src="https://i.imgur.com/6cjzCYK.png" width="200">
    </div>
    <div class="thecap">H2: Cấu trúc thư mục của WiderFace dataset </div>
</div>
<hr>

Trong WIDER_ANNS, ta chỉ quan tâm tới wider_train.txt - chứa toàn bộ thông tin về các ảnh, thông tin về 1 ảnh bao gồm: 
+ Dòng 1: image path
+ Dòng 2: Số face trong ảnh đó (đặt là n)
+ n dòng tiếp: tọa độ từng face trên ảnh, mỗi dòng là 1 dãy 10 số nhưng ta chỉ quan tâm tới 4 số đầu: là tọa độ left, top, width, height. Dưới đây là VD về dữ liệu 1 ảnh.

~~~~
    0--Parade/0_Parade_marchingband_1_849.jpg
    1
    449 330 122 149 0 0 0 0 0 0 
~~~~
<hr>

Mình đã code class WiderPreprocess trong **wider_preprocess.py** để tiền xử lí dữ liệu, các thao tác xử lí anns, ảnh, chuyển format dữ liệu đều được code trong này, bao gồm:
+ Class khởi tạo với 2 tham số : annotation file path, image folder path
+ get_img_data: đọc thông tin trong anns.txt và trả về 1 list, mỗi phần tử của list là 1 dict gồm 3 trường: [ img_path, face_number, face_coordinate]
+ Tọa độ đã được chuyển từ format : (left, top, w, h) -> (xmin, ymin, xmax, ymax)
+ Sau lần đầu chạy get_img_data(), nên lưu vào 1 file (preprocessed_data.txt), lần sau sử dụng lại cho nhanh

Code dưới được mình chạy trog file **main.ipynb**:

~~~~python
from wider_preprocess import WiderPreprocess

ANNS_DIR = "./WiderDataset/WIDER_ANNS/wider_train.txt"
IMAGE_DIR = "./WiderDataset/WIDER_TRAIN/images/"
wpreprocess = WiderPreprocess(ANNS_DIR, IMAGE_DIR)

# dòng này chạy lần đầu, sau đó comment code lại
img_data = wpreprocess.get_img_data()
with open("preprocessed_data.txt", "wb") as fp:
    pickle.dump(img_data, fp)

# Sau lần 1, chỉ cần run 3 dòng dưới:
img_data = []
with open("preprocessed_data.txt", "rb") as fp:
  img_data = pickle.load(fp)
~~~~
<hr>



<!-- ############ -->
<a name="-1.2"></a>

### 1.2 Tiền xử lí trong utils.py
Sau khi có img_data chứa dữ liệu, ta cần viết 1 data_generation. Công việc chính của data_generation là chuẩn hoá $$(x,y,w,h)$$ trước khi fit vào model. Công việc này được thực hiện trong hàm **data_gen** . Code này được mình đặt trong **utils.py** . Do code khá phức tạp và dài nên mình không đi sâu vào giải thích từng dòng. Trước tiên bạn cần hiểu được luồng xử lí của hàm data_gen, sau đó có thời gian hãy vọc code để hiểu sâu hơn.

Luồng xử lí của hàm data_gen:
+ Đọc ảnh
+ Augmentation: xoay ảnh, kéo dãn ảnh, resize ...
+ Chuẩn hoá $$(x,y,w,h)$$ thành dạng <strong>y_predict</strong> tương ứng với $$shape = (batchsize, S, S, B, 4+1+CLASS)$$

Trong main.ipynb chỉ cần import data_gen từ utils và code như sau:
~~~~python
from utils import data_gen
x_train, y_train = data_gen(img_data, 16)
~~~~
<hr>

<!-- ############ -->
<a name="-2"></a>

## 2 Kiến trúc model

Kiến trúc CNN (backbone) bạn có thể tuỳ biến. Trong bài này, mình sử dụng kiến trúc đơn giản dạng **tiny-yolo** - 1 loại yolo gọn, nhẹ thích hợp cho real-time và ít class. Trong trường hợp này CLASS = 1 vì chỉ có duy nhất 1 loại là khuôn mặt, đây là bài toán single object detect. Nếu bạn muốn nhận dạng nhiều loại cùng lúc (multi-object) như ô tô, xe máy, chó, mèo ... thì thay đổi giá trị CLASS.

Mình code bằng keras cho dễ hiểu. Kiến trúc CNN đóng gói trong **create_yolo_model**. Model có tính lặp lại cụm: Conv $$\longrightarrow$$ BatchNorm $$\longrightarrow$$ Relu $$\longrightarrow$$ Polling

Với 5 BBox và S=13, Class=1, output là 1 tensor với $$shape = (batchsize, S, S, B, 4+1+CLASS) = (13,13,5,6)$$.
<hr>

~~~~python
from keras.models import Sequential, Model, load_model
from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense
from keras.layers.advanced_activations import LeakyReLU
import numpy as np
import scipy.io
import os
import tensorflow as tf

def create_yolo_model():
    GRID_H, GRID_W = 13 , 13
    BOX = 5
    CLASS = 1

    model = Sequential()
    model.add(Conv2D(16, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(416,416,3)))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.1))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    for i in range(0,4):
        model.add(Conv2D(32*(2**i), (3,3), strides=(1,1), padding='same', use_bias=False))
        model.add(BatchNormalization())
        model.add(LeakyReLU(alpha=0.1))
        model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.1))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1), padding='same'))

    model.add(Conv2D(1024, (3,3), strides=(1,1), padding='same', use_bias=False))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.1))

    model.add(Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.1))

    model.add(Conv2D(BOX * (4 + 1 + CLASS), (1, 1), strides=(1, 1), kernel_initializer='he_normal'))
    model.add(Activation('linear'))
    model.add(Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS)))
    return model
~~~~

# 3.Loss Function.
Khác với các thuật toán thông thường **y_pred** và **y_true** có **format giống nhau**, output của mạng yolo này chỉ là **dạng tiền dữ liệu** của tọa độ, hình dạng hay phân bố xác suất ...khác dạng với y_true -> cần phải **xử lí** riêng từng thành phần của y_pred trong loss function với các hàm sigmoid, sofmax, ... để đưa y_pred và y_true về cùng dạng...

Nhắc lại công thức: (bạn nên mở lại post 2) 
<div style="margin-top:50px" align="center">
    <img width="500px" src="https://i.imgur.com/HyTTFNU.png"/>
    <img width="500px" src="https://i.imgur.com/vXIJMiW.jpg"/>
</div>

Đây là phần khó, đòi hỏi sự cẩn thận về logic. Thuật toán sai, train mất cả ngày mới phát hiện ra model không hội tụ mới khổ. Đoạn code này mình có tham khảo 1 repo trên github, tuy nhiên về sau mới nhận ra code đó cũng bị sai nhiều chỗ. Chắc repo đó cũng được tham khảo từ 1 chỗ khác về rồi sửa nhưng bị sai.

Trong code này, mình sử dụng Anchor box tham khảo từ nguồn khác (không nhớ link):

$$[[1.08,1.19],  [3.42,4.41],  [6.63,11.38],  [9.42,5.11],  [16.62,10.52]]$$

**y_true**: shape $$(batchsize, 13, 13, 5, 4 + 1+ 1)$$ (giống y_pred) trong đó: 
+ 4 số đầu: x_min, y_min, x_max, y_max (tính theo pixel - chưa chuẩn hóa theo kích thước image) => cần chuẩn hoá theo Anchor box
+ 1 Box confidence, value = 1 nếu Cell đó tồn tại object, ngược lại = 0. 
+ 1 CLASS: Do là bài toán single-object -> giá trị bằng 1 nếu Cell có object, ngược lại = 0
<hr>

~~~~python
# ANCHORS là 5 anchor box được định nghĩa từ trước (copy ở paper nào đó về)
def custom_loss(y_true, y_pred):
    # CONSTANT 
    ANCHORS = '1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52'
    ANCHORS = [float(ANCHORS.strip()) for ANCHORS in ANCHORS.split(',')]
    ANCHORS = np.reshape(ANCHORS, [1,1,1,5,2])
    NORM_H, NORM_W = 416, 416
    GRID_H, GRID_W = 13 , 13
    BOX = 5
    CLASS = 1

    ## trọng số cho từng loss thành phần
    SCALE_NOOB, SCALE_CONF, SCALE_COOR, SCALE_PROB = 0.5, 5.0, 5.0, 1.0

    ### x and y predict    
    pred_box_xy = tf.sigmoid(y_pred[:,:,:,:,:2])
    
    ### chuẩn hoá w, h về khoảng [0,1] so với kích thước ảnh rồi lấy căn bậc 2 
    ### loss function sử dụng căn bậc 2 của x,y,w,h - nên xem lại công thức
    grid_size = [float(GRID_W), float(GRID_H)]
    grid_size = np.reshape(grid_size, [1,1,1,1,2])
    pred_box_wh = tf.exp(y_pred[:,:,:,:,2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])
    pred_box_wh = tf.sqrt(pred_box_wh / grid_size)
    
    ### box confidence
    pred_box_conf = tf.expand_dims(tf.sigmoid(y_pred[:, :, :, :, 4]), -1)
    pred_box_prob = tf.expand_dims(tf.sigmoid(y_pred[:, :, :, :, 5]), -1)
    # pred_box_prob = tf.nn.softmax(y_pred[:, :, :, :, 5:])
    # trong trường hợp multi object detect, dùng softmax thay sigmoid, 

    ### concate các x,y,w,h,confidence, class_probability vừa được chuẩn hoá lại thành y_pred mới
    y_pred = tf.concat([pred_box_xy, pred_box_wh, pred_box_conf, pred_box_prob], 4)
    print("Y_pred shape: {}".format(y_pred.shape))
    
    ### ADJUST GROUND TRUTH
    # chuẩn hoá (x,y) theo khoảng [0,1] nằm trong 1 cell
    cell_wh = [(float(NORM_W)/GRID_W), (float(NORM_H)/GRID_H)]
    cell_wh = np.reshape(cell_wh, [1,1,1,1,2])
    center_xy = .5*(y_true[:,:,:,:,0:2] + y_true[:,:,:,:,2:4])
    center_xy = center_xy / cell_wh
    true_box_xy = center_xy - tf.floor(center_xy)
    
    #chuẩn hoá (w,h) về khoảng [0,1] theo kích thước ảnh rồi lấy căn bậc 2
    true_box_wh = (y_true[:,:,:,:,2:4] - y_true[:,:,:,:,0:2])
    true_box_wh = tf.sqrt(true_box_wh / np.reshape([float(NORM_W), float(NORM_H)], [1,1,1,1,2]))
    
    ### ADJUST CONFIDENCE
    # diện tích predict bounding box
    pred_tem_wh = tf.pow(pred_box_wh, 2) * np.reshape([GRID_W, GRID_H], [1,1,1,1,2])
    pred_box_area = pred_tem_wh[:,:,:,:,0] * pred_tem_wh[:,:,:,:,1]
    pred_box_ul = pred_box_xy - 0.5 * pred_tem_wh
    pred_box_bd = pred_box_xy + 0.5 * pred_tem_wh
    
    # diện tích ground truth bounding box
    true_tem_wh = tf.pow(true_box_wh, 2) * np.reshape([GRID_W, GRID_H], [1,1,1,1,2])
    true_box_area = true_tem_wh[:,:,:,:,0] * true_tem_wh[:,:,:,:,1]
    true_box_ul = true_box_xy - 0.5 * true_tem_wh
    true_box_bd = true_box_xy + 0.5 * true_tem_wh
    
    # tính iou giữa predict BBox và ground truth BBox
    intersect_ul = tf.maximum(pred_box_ul, true_box_ul) 
    intersect_br = tf.minimum(pred_box_bd, true_box_bd)
    intersect_wh = intersect_br - intersect_ul
    intersect_wh = tf.maximum(intersect_wh, 0.0)
    intersect_area = intersect_wh[:,:,:,:,0] * intersect_wh[:,:,:,:,1]

    iou = tf.truediv(intersect_area, true_box_area + pred_box_area - intersect_area)
    reduce_max = tf.reduce_max(iou, [3], True)

    # filter giữ lại các confidence của predict BBox có iou lớn nhất
    best_box = tf.equal(iou, reduce_max)
    best_box = tf.to_float(best_box)
    true_box_conf = tf.expand_dims(best_box * y_true[:,:,:,:,4], -1)
    true_box_prob = y_true[:,:,:,:,5:]
    
    ### concat, tổng hợp lại y_true sau khi đã chuẩn hoá
    y_true = tf.concat([true_box_xy, true_box_wh, true_box_conf, true_box_prob], 4)
    
    ### Tính các loss thành phần
    weight_coor = tf.concat(4 * [true_box_conf], 4)
    weight_coor = SCALE_COOR * weight_coor
    weight_conf = SCALE_NOOB * (1. - true_box_conf) + SCALE_CONF * true_box_conf
    weight_prob = tf.concat(CLASS * [true_box_conf], 4) 
    weight_prob = SCALE_PROB * weight_prob 
    weight = tf.concat([weight_coor, weight_conf, weight_prob], 4)
    print("Weight shape: {}".format(weight.shape))
    
    ### Finalize the loss
    loss = tf.pow(y_pred - y_true, 2)
    loss = loss * weight
    loss = tf.reshape(loss, [-1, GRID_W * GRID_H * BOX*(4 + 1 + CLASS)])
    loss = tf.reduce_sum(loss, 1)
    loss = .5 * tf.reduce_mean(loss)
    return loss
~~~~

Code này khá lằng nhằng, mình đã cố comment giải thích hết sức có thể. Bạn cần đọc từng dòng một và nghĩ xem dòng đó chạy như thế nào. Cách tốt nhất là print shape của các tensor ra.

Với đoạn code tính iou, trước tiên hãy đọc về thuật toán tính iou (google phát là ra, dễ hiểu nên mình không nói nhiều ở đây, vì bài dài rồi, viết vào lại thành lạc đề).


**Tạo và train model**: Mình train model kiểu đặt trong vòng lắp này vì thói quen và lười viết generator thôi, bạn có thể tuỳ biến.

~~~python

model = create_yolo_model()
adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss = custom_loss, optimizer= adam, metrics=['mse'])

x_batch = None
y_batch = None

train_data = data[:int(0.98*len(data))]
test_data = data[int(0.98*len(data)):]
x_test, y_test = data_gen(test_data, 100)

max_iter = 500

for i in range(max_iter):
    x_batch, y_batch = data_gen(train_data, 16)

    model.fit(x_batch, y_batch, epochs = 10, verbose = 0)

    if i% 10 == 0:
        print(f"iter {i}")
        model.save(f"./trained_model/{i}.h5")
~~~

## 4. Hậu xử lí
Như mình viết trên rằng y_prd là dạng tiền dữ liệu, từ y_pre, cần tính toán thêm nữa mới ra được output thật sự. Ta cần viết 1 hàm **interpret_netout** để hậu xử lí y_pred

Thêm nữa, sẽ xảy ra 2 trường hợp mạng đoán ra 2 bounding_box của cùng 1 face. Để giải quyết vấn đề này, ta làm theo các bước sau: 
+ b1: y_pred -> 1 list các box (box được lưu trong BoundBox class), tính xác suất mỗi box: <strong>prob = objectness * faceness</strong>
+ b2: Lọc: box có prob > THRESHOLD thì giữ nguyên, ngược lại gán bằng 0 
+ b2: Sort các box theo prob giảm dần
+ b3: Duyệt qua từng box, với 1 box, xét các box sau nó xem có box nào có iou với nó > iou_threshold thì loại những box đó.
+ b4: Cuối cùng, tính lại tọa độ theo xmin, ymin, xmax, ymax và tính lại theo size của image.

~~~python
def interpret_netout(image, netout, CLASS, GRID_H = 13, GRID_W = 13):
    BOX = 5
    boxes = []
    THRESHOLD = 0.3
    ANCHORS = '1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52'
    ANCHORS = [float(ANCHORS.strip()) for ANCHORS in ANCHORS.split(',')]
    
    for row in range(GRID_H):
        for col in range(GRID_W):
            for b in range(BOX):
                box = BoundBox(CLASS)
                box.x, box.y, box.w, box.h, box.c = netout[row,col,b,:5]
                box.col, box.row = col, row
                box.x = (col + sigmoid(box.x)) / GRID_W
                box.y = (row + sigmoid(box.y)) / GRID_H
                box.w = ANCHORS[2 * b + 0] * np.exp(box.w) / GRID_W
                box.h = ANCHORS[2 * b + 1] * np.exp(box.h) / GRID_H
                box.c = sigmoid(box.c)

                classes = netout[row,col,b,5:]
                box.probs = softmax(classes) * box.c
                box.probs *= box.probs > THRESHOLD
                    
    sorted_indices = list(reversed(np.argsort([box.probs for box in boxes])))
    for i in range(len(sorted_indices)):
        index_i = sorted_indices[i]
        if boxes[index_i].probs == 0:
            continue
        else:
            for j in range(i+1, len(sorted_indices)):
                index_j = sorted_indices[j]
                iou = boxes[index_i].iou(boxes[index_j])
                if iou >= 0.4:
                    boxes[index_j].probs = 0
                elif iou == -1:
                    boxes[index_i].probs = 0
    true_boxs = []
    for box in boxes:
        if box.probs > THRESHOLD:
            try:
                xmin  = int((box.x - box.w/2) * image.shape[1])
                xmax  = int((box.x + box.w/2) * image.shape[1])
                ymin  = int((box.y - box.h/2) * image.shape[0])
                ymax  = int((box.y + box.h/2) * image.shape[0])
                true_boxs.append([xmin, ymin, xmax, ymax, box.probs])
            except Exception as e:
                print("some error")
    return true_boxs

def sigmoid(x):
    return 1. / (1.  + np.exp(-x))

def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)
~~~

Trong quá trình train hoặc sau khi train, muốn test lại trên 1 ảnh, ta chỉ cần vài dòng như này:

~~~python
def test(model, x_test):
    j = randint(0,len(x_test) - 2)
    y = model.predict(x_test)
    true_boxs = interpret_netout(x_test[j]*255, y[j])
    print(len(true_boxs))
    img = draw_face_with_box(x_test[j]*255, true_boxs)
    plot_image(img)
~~~

Và đây là kết quả demo:

<div class="imgcap">
    <div >
        <img src="https://i.imgur.com/yBpsMgh.png" width="400">
    </div>
    <div class="thecap">H: Kết qủa demo </div>
</div>

Như vậy, trong bài viết này mình đã viết những phần quan trọng nhất của project. Những phần còn lại đều nằm theo link github: [github trungthanhnguyen0502](https://github.com/trungthanhnguyen0502/yolo-2). Có đoạn nào khó hiểu, hãy comment dưới hoặc liên lạc qua facebook

