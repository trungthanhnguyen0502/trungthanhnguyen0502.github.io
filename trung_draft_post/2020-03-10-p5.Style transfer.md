---
layout: post
title: "Bài 4: Thuật toán style transfer"
title2: "4. Thuật toán style transfer"
tag: [style transfer]
category: [deep learning, computer vision]
mathjax: true
summary: Sinh ra 1 bức ảnh có nội dung của ảnh chụp đường phố Hà Nội nhưng có phong cách vẽ của tranh Bùi Xuân Phái :D ...
img: assets/4-style-transfer/result.png
---

- [1 Thuật toán](#-1)
    - [1.1 Content feature](#-1-1)
    - [1.2 Style feature](#-1-2)
- [2 Thực hành code](#-2)
    - [2.1 Chuẩn bị](#-2-1)
    - [2.2 Gõ code nào ](#-2-2)
    - [2.3 Result ](#-2-3)



Bạn nghĩ sao về một bức ảnh chụp (máy ảnh) Hà Nội nhưng lại mang phong cách tranh của Bùi Xuân Phái :D. Với sự ra đời của thuật toán Style Transfer, chuyện đó là hoàn toàn có thể. Dưới đây là sản phẩm của mình:

<!-- image code -->
<div class="imgcap">
    <div >
        <img src="/assets/4-style-transfer/h1.png" width="500">
    </div>
    <div class="thecap">H1: Kết qủa của mình thu được </div>
</div>
<hr>

<a name="-1"></a>

## 1. Thuật toán.

Dưới đây là hình minh họa cho thuật toán. Chúng ta có 3 ảnh input gồm:
+ generate_image: Được khởi tạo random, lúc đầu là ảnh nhiễu bất kì, sau quá trình run thuật toán, generate_image dần dần được update trở thành kết quả chúng ta mong muốn.
+ content_image: chứa nội dung mà generate_image sẽ chứa.
+ style_image: chứa style (phong cách) mà generate_image sẽ chứa sau quá trình update sẽ thành ảnh mong muốn.

*Note: đường update gradient mình vẽ thiếu mũi tên, chiều mũi tên từ combine_loss hướng tới generate_image*

<div class="imgcap">
    <div >
        <img src="/assets/4-style-transfer/h2.png" width="600">
    </div>
    <div class="thecap">H2: Minh hoạ thuật toán </div>
</div>
<hr>

Ý tưởng thuật toán rất đơn giản: cả 3 ảnh cùng đưa vào 1 <strong>pretrain-CNN</strong> để extract ra các thông tin sau:
+ Với content_image thì extract ra <strong>content_feature</strong> (dữ liệu liên quan tới nội dung ảnh), ta gọi là <strong>content_A</strong>
+ Với style_image thì extract ra <strong>style_feature</strong>, gọi là <strong>style_B</strong>
+ Với generate_image thì extract ra cả 2 loại trên, gọi là <strong>content_C</strong> và <strong>style_C</strong>
+ Dựa vào 4 feature trên, ta tính hay "sự sai khác" theo dạng:  <strong>loss = F(content_A - content_C) + G(style_B - style_C)</strong>
+ Tính <strong>gradient</strong> của loss theo biến generate_image (Hay tính đạo hàm Loss theo generate_image)
+ Update generate_image với thuật toán gradient descent
+ Quy trình sẽ lặp đi lặp lại nhằm tối ưu giá trị loss, dừng thuật toán khi generate_image đủ tốt


<!-- ########## -->
<a name="-1-1"></a>

### 1.1 Content_feature
Câu hỏi đặt ra là làm thế nào để extract ra content_feature và style_feature từ 1 model pretrain, và làm thế nào để phân biệt giữa hai loại này?

Với content_feature: output của các layer trong CNN chính là content_feature, nó chứa thông tin về đường nét, bố cục, hình dạng của mọi thứ trong ảnh. Giả sử tại 1 layer thứ i, ta rút ra được $$output_i$$ với shape là $$(1 , w_i, h_i, c_i)$$. Có thể gọi $$output_i$$ bao gồm $$c_i$$ feature_map, mỗi feature_map có shape: $$(1 , w_i, h_i)$$ . Mỗi feature_map này "phản ảnh" thông tin ảnh theo 1 "góc nhìn" khác nhau

<!-- ########## -->
<a name="-1-2"></a>

### 1.2 Style_feature
Thế còn Style_feature? cái này phức tạp hơn một chút. Cũng từ $$output_i$$ trên, ta cần biến đổi một chút ta mới nhận được style_feature:
+ Giả sử tại layer thứ i, ta có $$output$$ là A với shape: $$(1 , 5, 5, 9)$$, tức A gồm 9 feature_map (5 x 5)
+ Ta cần reshape A thành matrix B với shape: $$(25, 9)$$.
+ Tính gram_matrix <strong>$$C = B^T * B$$</strong>, với $$B^T$$ là ma trận chuyển vị của B.
+ C có $$shape = (9,9)$$, C chính là ma trận "tương quan" giữa 9 feature_map với nhau, nó phản ảnh style/phong cách của ảnh/tranh vẽ

Thuật toán về cơ bản là thế, bắt tay vào code bạn sẽ dễ hiểu hơn.

<!-- ########## -->
<a name="-2"></a>

## 2. Thực hành code

<!-- ########## -->
<a name="-2-1"></a>

### 2.1 Chuẩn bị
+ Bạn nên vào github của mình và download code về tại: [github trungthanhnguyen0502](https://github.com/trungthanhnguyen0502/style-transfer)

+ Mình code bằng tensorflow thuần, bạn cần chút hiểu biết về "computation graph" trong tensorflow.
+ Bạn có thể dùng bất kì model pretrain CNN nào, tuy nhiên mình (và nhiều người) nhận thấy bài toán dạng này thì <strong>VGG</strong> thích hợp nhất. Mình dùng 1 phiên bản VGG16 mà mình hay sử dụng. Để có thể sử dụng VGG cùng phiên bản với mình, hãy đảm bảo trong project của bạn có 2 file : <strong>download.py</strong> và <strong>vgg16.py</strong> được tải trong github của mình

+ Mình hướng dẫn code chính, việc lặt vặt như import thư viện, viết function phụ mình không giải thích ở blog, hãy đọc code

+ 4 dòng đầu bạn có thể xóa đi nếu không sử dụng google colab.

+ Để đọc code dễ hiểu nhất, nên <strong>đọc từ hàm transfer</strong> để hiểu được kịch bản code. Trong transfer sẽ gọi đến các hàm khác.

<!-- ########## -->
<a name="-2-2"></a>

### 2.2 Gõ code nào 

Download và khởi tạo model:
~~~python
vgg16.maybe_download()
vgg = vgg16.VGG16()
~~~

Function tính mean_square_error (MSE)và tính gram_matrix (gram_matrix: $$C= B^T * B$$)
~~~python
def mean_square_error(a,b):
    return tf.reduce_mean(tf.square(a-b))

def gram_matrix(a):    
    shape = a.get_shape()
    reshape_tensor = tf.reshape(a, shape=[-1, int(shape[3])])
    gram_tensor = tf.matmul(tf.transpose(reshape_tensor), reshape_tensor)
    return gram_tensor
~~~

Content Loss function
~~~python
def content_loss(model, session, content_img, c_layer_ids):
    """
    Tính loss giữa content image và generate image
    content_img: image được dùng để trích xuất thông tin về nội dung
    c_layer_ids: index của các layer sẽ trích xuất feature_map
    """
    with model.graph.as_default():
        c_layers = model.get_layer_tensors(c_layer_ids)
        feed_dict = model.create_feed_dict(image=content_img)
        c_values = session.run(c_layers, feed_dict=feed_dict)
        c_loss = []
        for v, l in zip(c_values, c_layers):
            loss = mean_square_error(l, v)
            c_loss.append(loss)
        return tf.reduce_mean(c_loss)
~~~

Style Loss function
~~~python
def style_loss(model, session, style_img, s_layer_ids):
    """
    Tương tự content_loss, tính loss giữa style của style_img và generate image
    """
    with model.graph.as_default():
        s_layers = model.get_layer_tensors(s_layer_ids)
        gram_matrix_tensor = [gram_matrix(l) for l in s_layers]
        
        feed_dict = model.create_feed_dict(image=style_img)
        gram_matrix_values = session.run(gram_matrix_tensor, feed_dict=feed_dict)

        s_loss = []
        for v, l in zip(gram_matrix_values, gram_matrix_tensor):
            loss = mean_square_error(v,l)
            s_loss.append(loss)
        return tf.reduce_mean(s_loss)
~~~

Denoise Loss function: 1 kĩ thuật được bổ sung để generate_image giảm thiểu sự sai khác giữa 2 pixel cạnh nhau. 

~~~python
def denoise_loss(input_):
    d_loss = tf.reduce_mean(tf.abs(input_[:,1:,:,:] - input_[:,:-1,:,:])) + \
            tf.reduce_mean(tf.abs(input_[:,:,1:,:] - input_[:,:,:-1,:]))
    return d_loss
~~~

Transfer: là kịch bản chính, gọi tới các hàm trên.

+ Tensorflow cung cấp cho chúng ta hàm tf.gradient(loss, tensor_variable) để tính đạo hàm của loss theo biến tensor_variable.
+ step_size là 1 giá trị cho trước, có thể coi là giá trị learning rate khởi tạo

+ Ta áp dụng thuật toán tính learning rate trong mỗi vòng lặp, việc tính learning rate này đảm bảo giá trị thay đổi mỗi pixel trong generate_img không quá lớn, không vượt quá giá trị max trong ma trận Gradient, công thức:

$$learningRate = \frac{stepsize}{(gradient.max() + 1e-8)}$$

+ Ngoài gradient.max(), ta có thể dùng gradient.std()

+ w_content, w_style, w_denoise là trọng số biểu thị độ quan trọng của từng loại loss: content, style hay denoise


~~~python
def transfer(model, content_img, style_img,
             c_layer_ids, s_layer_ids, w_content=1.5, 
             w_style=10.0, w_denoise=0.3, iters= 60, step_size=10):

    # generate_img
    generate_img = np.random.rand(*content_img.shape) + 128
    session = tf.Session(graph = model.graph)

    c_loss = content_loss(model, session, content_img, c_layer_ids)
    s_loss = style_loss( model, session, style_img, s_layer_ids)
    d_loss = denoise_loss(model.input)
    with model.graph.as_default():
        adj_content = tf.Variable(1e-10, name='adj_content')
        adj_style = tf.Variable(1e-10, name='adj_style')
        adj_denoise = tf.Variable(1e-10, name='adj_denoise')

        session.run([adj_content.initializer,
                     adj_style.initializer,
                     adj_denoise.initializer])
 
        update_adj_content = adj_content.assign(1/(c_loss + 1e-10))
        update_adj_style = adj_style.assign(1/(s_loss + 1e-10))
        update_adj_denoise = adj_denoise.assign(1/(d_loss + 1e-10))

        total_loss = w_content * adj_content * c_loss +  w_denoise * adj_denoise * d_loss + w_style * adj_style * s_loss
        grad = tf.gradients(total_loss, model.input)
        run_list = [grad, update_adj_content, update_adj_style, update_adj_denoise]
        
        for i in range(iters):
            print(i)
            feed_dict = model.create_feed_dict(image=generate_img)
            grad_value, _, _, _ = session.run(run_list, feed_dict=feed_dict)
            grad_value = np.squeeze(grad_value)
            grad_value = grad_value.reshape(generate_img.shape)
            
            learning_rate = step_size/(np.std(grad_value) + 1e-8)
            generate_img -= grad_value * learning_rate
            generate_img = np.clip(generate_img, 0.0, 255.0)
            if( i % 10 == 0):
                plot_image(generate_img)
    return generate_img
~~~


<!-- ########## -->
<a name="-2-3"></a>

### 2.3 Kết quả thu được 
~~~python
content_dir = 'data/content.jpg'
content_img = load_image(content_dir)
content_img = cv2.resize(content_img, (224,224))

style_dir = 'data/style.jpeg'
style_img = load_image(style_dir)
style_img = cv2.resize(style_img, (224,224))

c_layer_ids = [4]
s_layer_ids = range(13)
result_img = transfer(model, content_img, style_img, 
                      c_layer_ids, s_layer_ids, 
                      w_content=2.0, w_style=9.0, w_denoise=0.4, 
                      iters= 60, step_size=10)
~~~

Sau khi mình run code nhiều lần thấy w_content giao động quanh giá trị 2, w_style ~ 10, w_denoise ~ 0.5 sẽ cho kết quả tốt nhất. Bạn có thể tự custom code, như điều chỉnh tham số, bỏ denoise_loss, áp dụng với ảnh khác. Ngoài ra, bạn có thể code bằng Keras, Pytorch với các pretrain model tốt hơn, gọn hơn

Kết qủa: với các tham số mình đã đặt mặc định, chỉ sau 60 vòng lặp, ta đã có thể thu được kết quả mong muốn:

<div class="imgcap">
    <div >
        <img src="/assets/4-style-transfer/result.png" width="500">
    </div>
    <div class="thecap">H3: Kết qủa của mình thu được </div>
</div>


