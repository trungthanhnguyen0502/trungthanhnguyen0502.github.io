{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorflowDataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKYXFIyBfA2v",
        "colab_type": "text"
      },
      "source": [
        "# 1. Vai trò của tensorflow Dataset\n",
        "\n",
        "Chắc hẳn các bạn từng thắc mắc vì sao trong deep learning các bộ dữ liệu bigdata có kích thước rất lớn mà các máy tính có RAM nhỏ hơn vẫn có thể huấn luyện được?\n",
        "\n",
        "Xuất phát từ lý do đó, bài này mình sẽ lý giải các cách thức dữ liệu có thể được truyền vào mô hình để huấn luyện theo cách tiếp cận dễ hiểu nhất. Các bạn chuyên gia và giàu kinh nghiệm huấn luyện mô hình có thể bỏ qua bài viết này vì nó khá cơ bản.\n",
        "\n",
        "**Vì sao có thể truyền các bộ dữ liệu lớn vào mô hình huấn luyện?**\n",
        "\n",
        "Các bộ dữ liệu deep learning thường có kích thước rất lớn. Trong quá trình huấn luyện các model deep learning chúng ta không thể truyền toàn bộ dữ liệu vào mô hình cùng một lúc bởi dữ liệu thường có kích thước lớn hơn RAM máy tính. Xuất phát từ lý do này, các framework deep learning đều hỗ trợ các hàm huấn luyện mô hình theo generator. Dữ liệu sẽ không được khởi tạo ngay toàn bộ từ đầu mà sẽ huấn luyện đến đâu sẽ được khởi tạo đến đó theo từng phần nhỏ gọi là batch.\n",
        "\n",
        "Tùy theo định dạng dữ liệu là text, image, data frame, numpy array,... mà chúng ta sẽ sử dụng những module tạo dữ liệu huấn luyện khác nhau.\n",
        "\n",
        "Vậy thì với từng kiểu dữ liệu khác nhau sẽ có phương pháp xử lý như thế nào để đưa vào huấn luyện mô hình? Có những kĩ thuật khởi tạo dataset trong tensorflow nào? Bài viết này mình sẽ giới thiệu tới các bạn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSiMq9S0aZuM",
        "colab_type": "text"
      },
      "source": [
        "# 2. Định nghĩa generator\n",
        "\n",
        "generator có thể coi là một người vay nợ, được quyền sử dụng tiền của người khác mà không trả ngay. Nếu chúng ta coi tiền là dữ liệu thì ta có thể hình dung generator sẽ sử dụng và biến đổi dữ liệu như cách người vay nợ sử dụng tiền vào các mục đích của mình. Tuy nhiên dữ liệu sau biến đổi không được trả về như các hàm thông thường.\n",
        "\n",
        "Để đơn giản hóa mình lấy ví dụ một hàm tính lãi suất phải trả theo năm như sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZO8aP5mcZPQ",
        "colab_type": "text"
      },
      "source": [
        "Giả sử một người vay $n$ món nợ với cùng lãi suất là 1%/tháng. Để tính lãi suất phải trả của các khoản vay chúng ta có thể sử dụng vòng for và tính để tính kết quả trong 1 lần."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FjVCXGQpTeG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e8d3d5e3-9576-4fa6-da72-4ccd6d3e0375"
      },
      "source": [
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "def _interest_rate(month):\n",
        "  return (1+0.01)**month - 1\n",
        "\n",
        "\n",
        "periods = [1, 3, 6, 9, 12]\n",
        "scales = [_interest_rate(month) for month in periods]\n",
        "print('scales of origin balance: ', scales)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scales of origin balance:  [0.010000000000000009, 0.030301000000000133, 0.061520150601000134, 0.09368527268436089, 0.12682503013196977]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Nx_escpWMC",
        "colab_type": "text"
      },
      "source": [
        "Tuy nhiên nếu sử dụng generator thì chúng ta chỉ việc thay `return` bằng `yield`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcwJsNIbcYWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2b9344d0-9ab0-4096-8d13-bdb72f711e06"
      },
      "source": [
        "def _gen_interest_rate(month):\n",
        "  yield (1+0.01)**month - 1\n",
        "\n",
        "\n",
        "periods = [1, 3, 6, 9, 12]\n",
        "scales = [_gen_interest_rate(month) for month in periods]\n",
        "print('scales of origin balance: ', scales)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scales of origin balance:  [<generator object _gen_interest_rate at 0x7efebc147d58>, <generator object _gen_interest_rate at 0x7efebc141150>, <generator object _gen_interest_rate at 0x7efebc118518>, <generator object _gen_interest_rate at 0x7efebc118570>, <generator object _gen_interest_rate at 0x7efebc118938>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5r6jeWDdmue",
        "colab_type": "text"
      },
      "source": [
        "Ta thấy generator sẽ không trả về kết quả ngay mà chỉ tạo sẵn các ô nhớ lưu hàm generator mô tả cách tính lãi suất. Do đó chúng ta sẽ không tốn chi phí thời gian để thực hiện các phép tính. Thực tế là chúng ta đang nợ máy tính kết quả trả về. Chỉ khi nào được chủ nợ gọi tên bằng cách kích hoạt trong hàm `next()` thì mới tính kết quả."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1-LAL61ePPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "c11e4a1a-797c-4992-9ca7-7b061f09cb61"
      },
      "source": [
        "[next(_gen_interest_rate(0.01, n)) for n in periods]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.01,\n",
              " 1.0303010000000001,\n",
              " 1.0615201506010001,\n",
              " 1.0936852726843609,\n",
              " 1.1268250301319698]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9eoKt0Sq8_b",
        "colab_type": "text"
      },
      "source": [
        "Do đó generator có lợi thế là:\n",
        "\n",
        "* Không sinh toàn bộ dữ liệu cùng một lúc, do đó sẽ nâng cao hiệu suất vì sử dụng ít bộ nhớ hơn.\n",
        "\n",
        "* Không phải chờ toàn bộ các vòng lặp được xử lý xong thì mới xử lý tiếp nên tiết kiệm thời gian tính toán.\n",
        "\n",
        "Đó chính là lý do generator chính là giải pháp được lựa chọn cho huấn luyện mô hình deep learning với dữ liệu lớn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxwj6SnRfE5e",
        "colab_type": "text"
      },
      "source": [
        "# 3. Các cách khởi tạo một Dataset\n",
        "\n",
        "Dataset là một class của tensorflow được sử dụng để wrap dữ liệu trước khi truyền vào mô hình để huấn luyện. Bạn hình dung dữ liệu của bạn có input là ma trận X và output là Y. Ban đầu X và Y chỉ là các dữ liệu thô định dạng numpy. Tất nhiên chúng ta có thể truyền trực tiếp chúng vào hàm `fit()` của mô hình nhưng để kiểm soát được X và Y chẳng hạn như fit vào với batch size bằng bao nhiêu? có shuffle dữ liệu hay không thì chúng ta nên wrap chúng trong `tf.Dataset`.\n",
        "\n",
        "Có 2 phương pháp chính để khởi tạo một tf.Dataset trong tensorflow:\n",
        "\n",
        "* In memory Dataset: Khởi tạo các dataset ngay từ đầu và dữ liệu được lưu trữ trên memory.\n",
        "* Generator Dataset: Dữ liệu được sinh ra theo từng batch và xen kẽ với quá trình huấn luyện từ các hàm khởi tạo generator.\n",
        "\n",
        "Phương pháp `In memory Dataset` sẽ phù hợp với các bộ dữ liệu kích thước nhỏ mà RAM có thể load được. Quá trình huấn luyện theo cách này thì nhanh hơn so với phương pháp `Generator Dataset` vì dữ liệu đã được chuẩn bị sẵn mà không tốn thời gian chờ khởi tạo batch. Tuy nhiên dễ xảy ra `out of memory` trong quá trình huấn luyện.\n",
        "\n",
        "Theo cách `Generator Dataset` chúng ta sẽ qui định cách mà dữ liệu được tạo ra như thế nào thông qua một hàm `generator`. Quá trình huấn luyện đến đâu sẽ tạo batch đến đó. Do đó các bộ dữ liệu big data có thể được load theo từng batch sao cho kích thước vừa được dung lượng RAM. Theo cách huấn luyện này chúng ta có thể huấn luyện được các bộ dữ liệu có kích thước lớn hơn nhiều so với RAM bằng cách chia nhỏ chúng theo batch. Đồng thời có thể áp dụng thêm các step preprocessing data trước khi dữ liệu được đưa vào huấn luyện. Do đó đây thường là phương pháp được ưa chuộng khi huấn luyện các model deep learning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEGEoWnSFr_V",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. In Memory Dataset\n",
        "\n",
        "Bên dưới chúng ta sẽ thử nghiệm khởi tạo một dtaset trên tensorflow theo phuwong pháp `In Memory Dataset`. Bộ dữ liệu được lựa chọn là chữ số viết tay mnist với kích thước của tập train và validation lần lượt là `60000` và `10000`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy523ZJhjBc1",
        "colab_type": "code",
        "outputId": "3be6a0f2-0675-413a-ad89-5dfc92329108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "path = 'gdrive/My Drive/Colab Notebooks/TensorflowData'\n",
        "os.chdir(path)\n",
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dog-Cat-Classifier', 'TensorflowDataset.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ7B0tmHjrxa",
        "colab_type": "code",
        "outputId": "dae0bbf1-f7ee-4947-f468-3f0d51e65f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "(60000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mATlIPNslB4o",
        "colab_type": "text"
      },
      "source": [
        "Như vậy các dữ liệu train và test của bộ dữ liệu mnist đã được load vào bộ nhớ. Tiếp theo chúng ta sẽ khởi tạo Dataset cho những dữ liệu in memory này bằng hàm `tf.data.Dataset.from_tensor_slices()`. Hàm này sẽ khai báo dữ liệu đầu vào cho mô hình huấn luyện."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T-A1CSolS9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhgOX4mRmMht",
        "colab_type": "text"
      },
      "source": [
        "Khi đó chúng ta đã có thể fit vào mô hình huấn luyện các dữ liệu được truyền vào `tf.Dataset` là `(X_train, y_train)`.\n",
        "\n",
        "Chúng ta cũng có thể áp dụng các phép biến đổi bằng các hàm như `Dataset.map()` hoặc `Dataset.batch()` để biến đổi dữ liệu trước khi fit vào model. Các bạn xem thêm tại [tf.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Chẳng hạn trước khi truyền batch vào huấn luyện tôi sẽ thực hiện chuẩn hóa batch theo phân phối chuẩn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxGtoL9RmgFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.backend import std, mean\n",
        "from tensorflow.math import reduce_std, reduce_mean\n",
        "\n",
        "def _normalize(X_batch, y_batch):\n",
        "  '''\n",
        "  X_batch: matrix digit images, shape batch_size x 28 x 28\n",
        "  y_batch: labels of digit.\n",
        "  '''\n",
        "  X_batch = tf.cast(X_batch, dtype = tf.float32)\n",
        "  # Padding về 2 chiều các giá trị 0 để được shape là 32 x 32\n",
        "  pad = tf.constant([[0, 0], [2, 2], [2, 2]])\n",
        "  X_batch = tf.pad(X_batch, paddings=pad, mode='CONSTANT', constant_values=0)\n",
        "  X_batch = tf.expand_dims(X_batch, axis=-1)\n",
        "  mean = reduce_mean(X_batch)\n",
        "  std = reduce_std(X_batch)\n",
        "  X_norm = (X_batch-mean)/std\n",
        "  return X_norm, y_batch\n",
        "\n",
        "train_dataset = train_dataset.batch(32).map(_normalize)\n",
        "valid_dataset = valid_dataset.batch(32).map(_normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL46S9Skwe4_",
        "colab_type": "text"
      },
      "source": [
        "`train_dataset` và `valid_dataset` lần lượt thực hiện các bước xử lý dữ liệu sau:\n",
        "\n",
        "* Hàm `.batch(32)`: Trích xuất ra từ list `(X_train, y_train)` các batch_size có kích thước là 32.\n",
        "\n",
        "* Hàm `.map(_normalize)`: Mapping đầu vào là các batch `(X_batch, y_batch)` kích thước 32 vào hàm số `_normalize()`. Kết quả trả về là giá trị đã chuẩn hóa theo batch của `X_batch` và `y_batch`. Dữ liệu này sẽ được sử dụng để huấn luyện model.\n",
        "\n",
        "\n",
        "Huấn luyện và kiểm định model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM5lIys2wd1_",
        "colab_type": "code",
        "outputId": "0a358f8c-bd98-4c98-e58d-e2de7c5ade9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "base_extractor = MobileNet(input_shape = (32, 32, 1), include_top = False, weights = None)\n",
        "flat = Flatten()\n",
        "den = Dense(10, activation='softmax')\n",
        "model = Sequential([base_extractor, \n",
        "                   flat,\n",
        "                   den])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenet_1.00_32 (Model)    (None, 1, 1, 1024)        3228288   \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,238,538\n",
            "Trainable params: 3,216,650\n",
            "Non-trainable params: 21,888\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZl_jpiL35ro",
        "colab_type": "code",
        "outputId": "f89206e8-8023-4a50-fc70-095e467836e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "model.compile(Adam(), loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(train_dataset,\n",
        "          validation_dataset = valid_dataset,\n",
        "          epochs = 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 687s 366ms/step - loss: 0.4430 - accuracy: 0.8630\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 686s 366ms/step - loss: 0.1505 - accuracy: 0.9586\n",
            "Epoch 3/5\n",
            "1505/1875 [=======================>......] - ETA: 2:15 - loss: 0.1432 - accuracy: 0.9635"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-329713397abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(train_dataset,\n\u001b[1;32m      3\u001b[0m           \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           epochs = 5)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    784\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aACv_AMmm0sD",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Generator Dataset\n",
        "\n",
        "Theo cách khởi tạo từ generator chúng ta sẽ không phải ghi nhớ toàn bộ dữ liệu vào RAM. Thay vào đó có thể tạo dữ liệu trong quá trình huấn luyện ở mỗi lượt fit từng batch.\n",
        "\n",
        "Giả sử bên dưới chúng ta có tên các món ăn được chia thành hai nhóm thuộc các địa phương 'hà nội' và 'hồ chí minh'. Chúng ta sẽ khởi tạo data generator để sinh dữ liệu cho mô hình phân loại món ăn theo địa phương."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhRg1ruL0LIM",
        "colab_type": "code",
        "outputId": "3c4d957a-de1d-40a2-f1cd-bb6ad18f7ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "hanoi = ['bún chả hà nội', 'chả cá lã vọng hà nội', 'cháo lòng hà nội', 'ô mai sấu hà nội', 'ô mai', 'chả cá', 'cháo lòng']\n",
        "hochiminh = ['bánh canh sài gòn', 'hủ tiếu nam vang sài gòn', 'hủ tiếu bò sài gòn', 'banh phở sài gòn', 'bánh phở', 'hủ tiếu']\n",
        "city = ['hanoi'] * len(hanoi) + ['hochiminh'] * len(hochiminh)\n",
        "corpus = hanoi+hochiminh\n",
        "\n",
        "data = pd.DataFrame({'city': city, 'food': corpus})\n",
        "data.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>city</th>\n",
              "      <th>food</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>hochiminh</td>\n",
              "      <td>hủ tiếu nam vang sài gòn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hanoi</td>\n",
              "      <td>chả cá lã vọng hà nội</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>hochiminh</td>\n",
              "      <td>bánh canh sài gòn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hanoi</td>\n",
              "      <td>bún chả hà nội</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>hochiminh</td>\n",
              "      <td>bánh phở</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         city                      food\n",
              "8   hochiminh  hủ tiếu nam vang sài gòn\n",
              "1       hanoi     chả cá lã vọng hà nội\n",
              "7   hochiminh         bánh canh sài gòn\n",
              "0       hanoi            bún chả hà nội\n",
              "11  hochiminh                  bánh phở"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ6k0YvCq1vj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Voc(object):\n",
        "  def __init__(self, corpus):\n",
        "    self.corpus = corpus\n",
        "    self.dictionary = {'unk': 0}\n",
        "    self._initialize_dict(corpus)\n",
        "  \n",
        "  def _add_dict_sentence(self, sentence):\n",
        "    words = sentence.split(' ')\n",
        "    for word in words:\n",
        "      if word not in self.dictionary.keys():\n",
        "        max_indice = max(self.dictionary.values())\n",
        "        self.dictionary[word] = (max_indice + 1)\n",
        "\n",
        "  def _initialize_dict(self, sentences):\n",
        "    for sentence in sentences:\n",
        "      self._add_dict_sentence(sentence)\n",
        "    \n",
        "  def _tokenize(self, sentence):\n",
        "    words = sentence.split(' ')\n",
        "    token_seq = [self.dictionary[word] for word in words]\n",
        "    return np.array(token_seq)\n",
        "\n",
        "voc = Voc(corpus = corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIEs3dijynJM",
        "colab_type": "text"
      },
      "source": [
        "corpus là list toàn bộ tên các món ăn. Class Voc có tác dụng khởi tạo index từ điển cho toàn bộ corpus (bộ văn bản)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRw6_1FvvzAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc.dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHPG1wRuzHiM",
        "colab_type": "text"
      },
      "source": [
        "Tiếp theo chúng ta sẽ khởi tạo một `random_generator` có tác dụng lựa chọn ngẫu nhiên một tên món ăn trong corpus và tokenize chúng."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH0PiHMHm5rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "cat_indices = {\n",
        "    'hanoi': 0,\n",
        "    'hochiminh': 1\n",
        "}\n",
        "\n",
        "def generators():\n",
        "  i = 0\n",
        "  while True:\n",
        "    i = np.random.choice(data.shape[0])\n",
        "    sentence = data.iloc[i, 1]\n",
        "    x_indice = voc._tokenize(sentence)\n",
        "    label = data.iloc[i, 0]\n",
        "    y_indice = cat_indices[label]\n",
        "    yield x_indice, y_indice\n",
        "    i += 1\n",
        "\n",
        "random_generator = tf.data.Dataset.from_generator(\n",
        "    generators,\n",
        "    output_types = (tf.float16, tf.float16),\n",
        "    output_shapes = ((None,), ())\n",
        ")\n",
        "\n",
        "random_generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3BrC4KHo1Ur",
        "colab_type": "code",
        "outputId": "c53b8253-446b-453d-cc0d-49e15e90e090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "random_generator_batch = random_generator.shuffle(20).padded_batch(20, padded_shapes=([None], []))\n",
        "sequence_batch, label = next(iter(random_generator_batch))\n",
        "\n",
        "print(sequence_batch)\n",
        "print(label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 8.  9.  3.  4.  0.  0.]\n",
            " [10. 11.  0.  0.  0.  0.]\n",
            " [17. 18. 19. 20. 15. 16.]\n",
            " [22. 23. 15. 16.  0.  0.]\n",
            " [13. 14. 15. 16.  0.  0.]\n",
            " [13. 23.  0.  0.  0.  0.]\n",
            " [17. 18. 19. 20. 15. 16.]\n",
            " [ 1.  2.  3.  4.  0.  0.]\n",
            " [10. 11.  0.  0.  0.  0.]\n",
            " [17. 18. 21. 15. 16.  0.]\n",
            " [ 8.  9.  0.  0.  0.  0.]\n",
            " [22. 23. 15. 16.  0.  0.]\n",
            " [ 2.  5.  6.  7.  3.  4.]\n",
            " [13. 14. 15. 16.  0.  0.]\n",
            " [ 1.  2.  3.  4.  0.  0.]\n",
            " [13. 14. 15. 16.  0.  0.]\n",
            " [ 8.  9.  0.  0.  0.  0.]\n",
            " [13. 14. 15. 16.  0.  0.]\n",
            " [13. 23.  0.  0.  0.  0.]\n",
            " [10. 11. 12.  3.  4.  0.]], shape=(20, 6), dtype=float16)\n",
            "tf.Tensor([0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.], shape=(20,), dtype=float16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTs0X7ae1-qP",
        "colab_type": "text"
      },
      "source": [
        "hàm `shuffle(20)` có tác dụng trộn lẫn ngẫu nhiên dữ liệu. Sau đó dữ liệu được chia thành những batch có kích thước là 10 và padding giá trị 0 sao cho bằng với độ dài của câu dài nhất bằng hàm `padded_batch()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFGSvYUQ7CQi",
        "colab_type": "text"
      },
      "source": [
        "## 3.2.1. Sử dụng ImageGenerator\n",
        "\n",
        "ImageGenerator cũng là một dạng data generator được xây dựng trên framework keras và dành riêng cho dữ liệu ảnh.\n",
        "\n",
        "Đây là một high level function nên cú pháp đơn giản, rất dễ sử dụng nhưng khả năng tùy biến và can thiệp sâu vào dữ liệu kém."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B1VK14H7Uv_",
        "colab_type": "text"
      },
      "source": [
        "Khi khởi tạo ImageGenerator chúng ta sẽ khai báo các thủ tục preprocessing image trước khi đưa vào huấn luyện. Mình sẽ không quá đi sâu vào các kĩ thuật preprocessing data này. Bạn đọc quan tâm có thể xem thêm tại [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dtaitAr7SAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rescale = 1./255, \n",
        "    rotation_range = 20,\n",
        "    horizontal_flip = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_E7CBo39RPA",
        "colab_type": "code",
        "outputId": "5a886122-6d31-4970-cef4-a6a497414504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import glob2\n",
        "\n",
        "root_folder = 'Dog-Cat-Classifier/Data/Train_Data/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dog-Cat-Classifier/Data/Train_Data/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs3azBjA8_fe",
        "colab_type": "text"
      },
      "source": [
        "Tiếp theo chúng ta sẽ truyền dữ liệu vào mô hình thông qua một hàm là `flow_from_directory()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOcA46Wi9Lz_",
        "colab_type": "code",
        "outputId": "b4e6466e-7642-4780-b278-efe3de114823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "images, labels = next(image_gen.flow_from_directory(root_folder))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1399 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyHHSnFc9azF",
        "colab_type": "text"
      },
      "source": [
        "Hàm `flow_from_directory()` sẽ có tác dụng đọc các ảnh từ `root_folder` và lấy ra những thông tin bao gồm ma trận ảnh sau biến đổi và nhãn tương ứng. Cấu trúc cây thư mục của `root_folder` có dạng như sau:\n",
        "\n",
        "`root-folder`\n",
        "  \n",
        "  `sub-folder-class-1`\n",
        "  \n",
        "  `sub-folder-class-2`\n",
        "\n",
        "  `...`\n",
        "  \n",
        "  `sub-folder-class-C`\n",
        "\n",
        "Trong đó bên trong các `sub-folder-class-i` là list toàn bộ các ảnh thuộc về một class. Hàm `flow_from_directory()` sẽ tự động xác định các file dữ liệu nào là ảnh để load vào quá trình huấn luyện mô hình.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHU3VU1g9-v1",
        "colab_type": "code",
        "outputId": "d089dbf1-d2a9-46f4-bff0-e5ffe9a5fea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "!ls {root_folder}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat  dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd2BwG4F-u_-",
        "colab_type": "text"
      },
      "source": [
        "Ở đây trong root_folder chúng ta có 2 `sub-folders` tương ứng với 2 classes là `dog, cat`.\n",
        "\n",
        "Tiếp theo ta sẽ khởi tạo một `tf.Dataset` từ generator thông qua hàm `from_generator()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUSvnell-qII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_gen_dataset = tf.data.Dataset.from_generator(\n",
        "    image_gen.flow_from_directory, \n",
        "    args = ([root_folder]),\n",
        "    output_types=(tf.float32, tf.float32), \n",
        "    output_shapes=([32,256,256,3], [32, 1])\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRdhMxuo5lLO",
        "colab_type": "text"
      },
      "source": [
        "Trong hàm `from_generator()` chúng ta phải khai báo bắt buộc định dạng dữ liệu input và output thông qua tham số `output_types` và output shape thông qua tham số `output_shapes`.\n",
        "\n",
        "Như vậy kết quả trả ra sẽ là những batch có kích thước 32 và ảnh có kích thước `256 x 256` và nhãn tương ứng của ảnh."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMgmOZo1tUcR",
        "colab_type": "text"
      },
      "source": [
        "## 3.2.2. Customize ImageGenerator\n",
        "\n",
        "Giả sử bạn có một bộ dữ liệu ảnh mà kích thước các ảnh là khác biệt nhau. Đồng thời bạn cũng muốn can thiệp sâu hơn vào bức ảnh trước khi đưa vào huấn luyện như giảm nhiễu bằng bộ lọc [Gausianblur](https://phamdinhkhanh.github.io/2020/01/06/ImagePreprocessing.html#222-l%C3%A0m-m%E1%BB%9D-%E1%BA%A3nh-image-blurring), rotate ảnh, crop, zoom ảnh, .... Nếu sử dụng các hàm mặc định của image preprocessing trong ImageGenerator thì sẽ gặp hạn chế đó là bị giới hạn bởi một số phép biến đổi mà hàm này hỗ trợ. Sử dụng high level framework tiện thì rất tiện nhưng khi muốn can thiệp sâu thì rất khó. Muốn can thiệp được sâu vào bên trong các biến đổi chúng ta phải customize lại một chút ImageGenerator.\n",
        "\n",
        "Cách thức customize như thế nào. Mình sẽ giới thiệu với các bạn qua chương này.\n",
        "\n",
        "Đầu tiên chúng ta sẽ download bộ dữ liệu `Dog & Cat` đã được thu nhỏ số lượng ảnh về. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx7PzLGbNc1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/ardamavi/Dog-Cat-Classifier.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJBQi2-7NrnS",
        "colab_type": "text"
      },
      "source": [
        "```  Cloning into 'Dog-Cat-Classifier'...\n",
        "  remote: Enumerating objects: 1654, done.\n",
        "  remote: Total 1654 (delta 0), reused 0 (delta 0), pack-reused 1654\n",
        "  Receiving objects: 100% (1654/1654), 34.83 MiB | 16.60 MiB/s, done.\n",
        "  Resolving deltas: 100% (147/147), done.\n",
        "  Checking out files: 100% (1672/1672), done.```\n",
        "\n",
        "\n",
        "\n",
        "Tiếp theo ta sẽ khởi tạo một DataGenerator cho bộ dữ liệu ảnh kế thừa class Sequence của keras. Mình sẽ giải thích các phương thức trong DataGenerator này bên dưới."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ico-SjLGa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "import cv2\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,\n",
        "                 all_filenames, \n",
        "                 labels, \n",
        "                 batch_size, \n",
        "                 index2class,\n",
        "                 input_dim,\n",
        "                 n_channels,\n",
        "                 n_classes=2, \n",
        "                 shuffle=True):\n",
        "        '''\n",
        "        all_filenames: list toàn bộ các filename\n",
        "        labels: nhãn của toàn bộ các file\n",
        "        batch_size: kích thước của 1 batch\n",
        "        index2class: index của các class\n",
        "        input_dim: (width, height) đầu vào của ảnh\n",
        "        n_channels: số lượng channels của ảnh\n",
        "        n_classes: số lượng các class \n",
        "        shuffle: có shuffle dữ liệu sau mỗi epoch hay không?\n",
        "        '''\n",
        "        self.all_filenames = all_filenames\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.index2class = index2class\n",
        "        self.input_dim = input_dim\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        return:\n",
        "          Trả về số lượng batch/1 epoch\n",
        "        '''\n",
        "        return int(np.floor(len(self.all_filenames) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        params:\n",
        "          index: index của batch\n",
        "        return:\n",
        "          X, y cho batch thứ index\n",
        "        '''\n",
        "        # Lấy ra indexes của batch thứ index\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # List all_filenames trong một batch\n",
        "        all_filenames_temp = [self.all_filenames[k] for k in indexes]\n",
        "\n",
        "        # Khởi tạo data\n",
        "        X, y = self.__data_generation(all_filenames_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        '''\n",
        "        Shuffle dữ liệu khi epochs end hoặc start.\n",
        "        '''\n",
        "        self.indexes = np.arange(len(self.all_filenames))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, all_filenames_temp):\n",
        "        '''\n",
        "        params:\n",
        "          all_filenames_temp: list các filenames trong 1 batch\n",
        "        return:\n",
        "          Trả về giá trị cho một batch.\n",
        "        '''\n",
        "        X = np.empty((self.batch_size, *self.input_dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Khởi tạo dữ liệu\n",
        "        for i, fn in enumerate(all_filenames_temp):\n",
        "            # Đọc file từ folder name\n",
        "            img = cv2.imread(fn)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, self.input_dim)\n",
        "            label = fn.split('/')[3]\n",
        "            label = self.index2class[label]\n",
        "    \n",
        "            X[i,] = img\n",
        "\n",
        "            # Lưu class\n",
        "            y[i] = label\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atgMqK7_OTgA",
        "colab_type": "text"
      },
      "source": [
        "Một DataGenerator sẽ cần xác định kích thước của một batch, số lượt steps huấn luyện.\n",
        "\n",
        "* Hàm __len__(): Như chúng ta đã biết, `__len__()` là một built in function trong python. Bất kì một object nào của python cũng sẽ có hàm `__len__()`. Đối với Datagenerator thì chúng ta sẽ qui định \n",
        "\n",
        "$$\\text{len} = \\frac{\\text{# Obs}}{\\text{batch size}}$$\n",
        "\n",
        "Đây chính là số lượng step trong một epoch.\n",
        "\n",
        "* Hàm `__getitem__()`: Trong quá trình huấn luyện chúng ta cần phải access vào từng batch trong bộ dữ liệu. Hàm `__getitem__()` sẽ khởi tạo batch theo thứ tự của batch được truyền vào hàm.\n",
        "\n",
        "* Hàm `on_epoch_end()`: Đây là hàm được tự động run mỗi khi một epoch huấn luyện bắt đầu và kết thúc. Tại hàm này chúng ta sẽ xác định các hành động khi bắt đầu hoặc kết thúc một epoch như có shuffle dữ liệu hay không. Điều chỉnh lại tỷ lệ các class tước khi fit vào model,....\n",
        "\n",
        "* Hàm `__data_generation()`: Hàm này sẽ được gọi trong `__getitem__()`. `__data_generation()` sẽ trực tiếp biến đổi dữ liệu và quyết định các kết quả dữ liệu trả về cho người dùng. Tại hàm này ta có thể thực hiện các phép preprocessing image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaNogXm5jcjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import glob2\n",
        "\n",
        "dict_labels = {\n",
        "    'dog': 0,\n",
        "    'cat': 1\n",
        "}\n",
        "\n",
        "root_folder = 'Dog-Cat-Classifier/Data/Train_Data/*/*'\n",
        "fns = glob2.glob(root_folder)\n",
        "print(len(fns))\n",
        "\n",
        "image_generator = DataGenerator(\n",
        "    all_filenames = fns,\n",
        "    labels = None,\n",
        "    batch_size = 32,\n",
        "    index2class = dict_labels,\n",
        "    input_dim = (224, 224),\n",
        "    n_channels = 3,\n",
        "    n_classes = 2,\n",
        "    shuffle = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1vrexQWj9Ez",
        "colab_type": "code",
        "outputId": "70a813ba-8870-430a-97fe-e917e3cc4b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X, y = image_generator.__getitem__(1)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 224, 224, 3)\n",
            "(32,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g67vTr-1yzhC",
        "colab_type": "text"
      },
      "source": [
        "Như vậy ta có thể thấy, tại mỗi lượt huấn luyện model lấy ra một batch có kích thước là 32. Mặc dù ảnh của chúng ta có kích thước khác nhau nhưng đã được resize về chung một kích thước là `width x height = 224 x 224`.\n",
        "\n",
        "Chúng ta sẽ thử nghiệm huấn luyện model với generator. Đầu tiên là khởi tạo model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc252SuR-YjC",
        "colab_type": "code",
        "outputId": "47c4db40-5321-4c2d-8507-b2d134a034e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "base_extractor = MobileNet(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')\n",
        "flat = Flatten()\n",
        "den = Dense(1, activation='sigmoid')\n",
        "model = Sequential([base_extractor, \n",
        "                   flat,\n",
        "                   den])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenet_1.00_224 (Model)   (None, 7, 7, 1024)        3228864   \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 50177     \n",
            "=================================================================\n",
            "Total params: 3,279,041\n",
            "Trainable params: 3,257,153\n",
            "Non-trainable params: 21,888\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yibTY-8ut4AJ",
        "colab_type": "text"
      },
      "source": [
        "Tiếp theo để huấn luyện model chúng ta chỉ cần thay generator vào vị trí của train data trong hàm `fit()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoYD-W6g-Xdu",
        "colab_type": "code",
        "outputId": "140bce5c-54b8-4368-8e6c-e24ddb8895b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "model.compile(Adam(), loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(image_generator,\n",
        "          epochs = 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "43/43 [==============================] - 247s 6s/step - loss: 1.1607 - accuracy: 0.8903\n",
            "Epoch 2/5\n",
            "43/43 [==============================] - 248s 6s/step - loss: 0.5528 - accuracy: 0.9295\n",
            "Epoch 3/5\n",
            "43/43 [==============================] - 244s 6s/step - loss: 0.2020 - accuracy: 0.9542\n",
            "Epoch 4/5\n",
            "43/43 [==============================] - 248s 6s/step - loss: 0.2046 - accuracy: 0.9615\n",
            "Epoch 5/5\n",
            "43/43 [==============================] - 245s 6s/step - loss: 0.0494 - accuracy: 0.9840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd031252fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbdSEXbF3nv3",
        "colab_type": "text"
      },
      "source": [
        "Chỉ với khoảng 5 epochs nhưng kết quả đã đạt 98.4% độ chính xác. Đây là một kết quả khá ấn tượng."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgV-qN-dti8Z",
        "colab_type": "text"
      },
      "source": [
        "# 4. Tổng kết\n",
        "\n",
        "Như vậy qua bài viết này tôi đã giới thiệu tới các bạn các phương pháp chính để khởi tạo một Dataset trong tensorflow, ưu nhược điểm và trường hợp sử dụng của từng phương pháp.\n",
        "\n",
        "Khi nắm vững được kiến thức này, các bạn sẽ không còn phải lo lắng nếu phải đối mặt với những bộ dữ liệu rất lớn mà không biết cách truyền vào mô hình huấn luyện.\n",
        "\n",
        "Chúc các bạn thành công với những mô hình sắp tới. Cuối cùng không thể thiếu là các tài liệu mà tôi đã tham khảo để viết bài này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jx1El0-uFkF",
        "colab_type": "text"
      },
      "source": [
        "# 5. Tài liệu tham khảo\n",
        "\n",
        "1. [tensorflow data - tensorflow](https://www.tensorflow.org/guide/data)\n",
        "2. [tensorflow ImageDataGenerator - tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n",
        "3. [how to generate data on the fly - standford.edu](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly)\n",
        "4. [generator python - wiki](https://wiki.python.org/moin/Generators)"
      ]
    }
  ]
}