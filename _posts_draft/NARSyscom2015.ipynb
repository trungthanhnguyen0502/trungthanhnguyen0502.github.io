{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NARSyscom2015.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFTBH2jBT8KP",
        "colab_type": "text"
      },
      "source": [
        "## 1. Các dạng mô hình Recommendation\n",
        "\n",
        "Ngày nay, các thuật toán recommendation ngày càng phát triển đa dạng bởi tính ứng dụng cao trong việc khuyến nghị và matching sản phẩm tới khách hàng trên các nền tảng website/app kinh doanh online. Recommendation có thể coi là vũ khí trong cuộc chơi của các ông lớn trong những ngành có tính đốt tiền cao như ecommerce, airbnb, logistic,.... Do đó đây là bài toán thu hút được rất nhiều sự đầu tư và nghiên cứu từ những trường đại học và từ các tập đoàn công nghệ. Facebook, Amazon, Google, Alibaba,... có thể coi là những nền tảng có hệ thống recommendation phát triển nhất. Recommendation từ những nền tảng này không những đạt độ chính xác cao mà còn đáp ứng nhu cầu đồng thời của một lượng lớn người dùng online. Các kết quả khuyến nghị từ google, facebook rất tốt là điều không quá ngạc nhiên (chỉ khi nó không tốt mới đáng ngạc nhiêu) bởi đây là những tập đoàn có dữ liệu lớn và có đội ngũ nghiên cứu hàng đầu thế giới.\n",
        "\n",
        "Tuy nhiên các doanh nghiệp vừa và nhỏ khác cũng hoàn toàn có thể tự xây dựng và phát triển một hệ thống recommendation từ chính dữ liệu về hành vi người dùng và hệ thống cây sản phẩm. Hãy tin vào điều đó vì lý thuyết thuật toán recommendation là không quá phức tạp và tài liệu các thuật toán recommendation là sẵn có.\n",
        "\n",
        "Ở [Bài 15 - collaborative và content-based filtering](https://phamdinhkhanh.github.io/2019/11/04/Recommendation_Compound_Part1.html) và [Bài 20 - Recommendation Neural Network](https://phamdinhkhanh.github.io/2019/12/26/Sorfmax_Recommendation_Neural_Network.html) tôi đã giới thiệu tới các bạn các phương pháp recommendation nhằm tìm kiếm biểu diễn của một item cụ thể. Những bài toán này phù hợp với dữ liệu không có yếu tố thời gian. Tuy nhiên theo xu hướng nghiên cứu ở thời điểm hiện tại, những lớp mô hình state-of-art được phát triển để phù hợp với dữ liệu dạng session có yếu tố thời gian, rất phổ biến trong các nền tảng website/app hiện đại. Chúng ta cùng tìm hiểu một lớp thuật toán mới là `Session-Based Recommendation` mà tôi sẽ giới thiệu qua bài viết này.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3JL7UMyggsu",
        "colab_type": "text"
      },
      "source": [
        "## 2. Khác biệt giữa Collaborative-Filtering và Session-Based System\n",
        "\n",
        "Cả 2 thuật toán Collaborative-Filtering và Session-Based System đều dựa trên các lượt tương tác giữa user và item. Tuy nhiên thực tiễn cho thấy Collaborative-Filtering tồn tại những hạn chế đó là hành vi rating của người dùng không được đặt trong ngữ cảnh (context). Tức là mối liên hệ của một lượt rating với các lượt rating liền kề về mặt thời gian không được quan tâm. \n",
        "\n",
        "Nhưng bối cảnh lại rất có ảnh hưởng tới sở thích của khách hàng. Chẳng hạn như khách hàng trước đó đã từng xem một bộ phim bom tấn. Do bộ phim đó quá hay nên mặc dù bộ phim hiện tại rất xuất sắc nhưng vì ảnh hưởng so sánh làm cho khách hàng vẫn có xu hướng đánh giá thấp bộ phim hiện tại. Mọi người thường nói không nên so sánh nhưng có vẻ đây là một phản chứng :D. Bối cảnh trong trường hợp này đóng vai trò rất lớn nhưng không được đưa vào mô hình Collabrative Filtering. Chính vì hạn chế đó, các mô hình Session-Based System đã khắc phục bằng cách đưa thêm yếu tố bối cảnh vào mô hình nhằm mang lại độ chính xác cao hơn.\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/fxVTVXR.png)\n",
        "> **Hình 1:** So sánh giữa mô hình Collaborative-Filtering và mô hình Session- Based System. Dữ liệu đầu vào là các session của khách hàng 1 (dòng 1 và 3) và khách hàng 2 (dòng 2). Ở nửa phía trên của hình minh họa là mô hình Collaborative Filtering, các lượt rating được sắp xếp từ các session theo đúng điểm rating của user với item. Cuối cùng ta thu được một ma trận tiện ích (utility) của user-item như vị trí thứ 3 từ trái sang. Dựa trên các lượt rating đã biết ta cần dự báo các vị trí chưa biết (dấu ?). Tiếp theo bên dưới hình minh họa là mô hình Session-Based Recommendation System. Theo mô hình này các đầu vào là những items được giữ nguyên vị trí như trong session gốc. Chúng ta cần dự đoán sản phẩm mà tiếp theo khách hàng có khả năng tương tác.\n",
        "\n",
        "Khái niệm về session có thể được định nghĩa linh hoạt. Đó có thể là toàn bộ chuỗi các sản phẩm mà khách hàng đã view. Trường hợp khác sẽ giới hạn session chỉ bao gồm danh sách các sản phẩm có trong giỏ hàng hoặc cũng có thể là danh sách sản phẩm khách hàng đã thanh toán trong một lần. Thậm chí session cũng có thể mở rộng thành list các event trong một khoảng thời gian ngắn của một khách hàng (có thể thuộc các sessions khác nhau).\n",
        "\n",
        "Mô hình Collaborative-Filtering gặp hạn chế ở một số tình huống user không đăng nhập, hệ thống sẽ bị giới hạn trong việc thu thập thông tin người dùng và do đó chúng ta không thể biết khách hàng là ai để recommend chính xác các sản phẩm họ có nhu cầu. Trong khi đó mô hình Session Based System lợi thế hơn khi tìm kiếm kiểu (pattern) của chuỗi hành vi mua sắm từ các session và khuyến nghị theo kiểu mà không quan tâm đến định danh user.\n",
        "\n",
        "Ngoài ra Session-Based System không chia nhỏ các session của một user thành những cặp đánh giá (user, item) không theo thứ tự như Collaborative-Filtering nên bảo toàn được mối liên hệ về bối cảnh của session, một yếu tố có ảnh hưởng lớn đến hành vi của user.\n",
        "\n",
        "## Các dạng của Session-Based System\n",
        "\n",
        "Trong chương này chúng ta sẽ tìm cách phân loại Session-Based System theo 2 nhánh chính: Tiếp cận theo khía cạnh mô hình (model-based) và tiếp cận phi mô hình (model-free). Trong mỗi cách tiếp cận sẽ có một vài lớp mô hình khác nhau được trình bày cụ thể bên dưới:\n",
        "\n",
        "### Tiếp cận phi mô hình (model-free)\n",
        "\n",
        "Các lượt khuyến nghị của mô hình tự do chủ yếu được xây dựng dựa trên các kĩ thuật khai phá dữ liệu như thống kê tần suất, tính độ tương quan và không đòi hỏi các thuật toán phức tạp. Có 2 nhánh tiếp cận chủ yếu là Pattern/rule-based RS và Sequential Pattern-based RS cho các session thứ bậc.\n",
        "\n",
        "1. **Pattern/Rule-based**: Theo cách tiếp cận Pattern/Rule-based, mô hình sẽ dựa trên giải định hầu hết các lượt mua sắm của khách hàng sẽ có chung những kiểu (pattern) mua sắm. chúng ta sẽ phân tích tần suất kiểu hoặc các qui luật kết hợp và sử dụng các kiểu và qui luật kết hợp này để đưa ra khuyến nghị cho các lượt mua sắm tiếp theo. Chẳng hạn như khách hàng khi mua bánh mì sẽ thường mua kèm sữa nên ta sẽ coi `{bánh mì, sữa}` là một kiểu và áp dụng vào khuyến nghị. Dữ liệu đầu vào của mô hình là dữ liệu không có thứ tự.\n",
        "\n",
        "2. **Sequential Pattern-based**: Cách tiếp cận chuỗi Pattern-based nhằm giải quyết những dữ liệu mà có tính thứ tự item (dữ liệu dạng chuỗi thời gian). Cũng tương tự như phương pháp `Pattern/Rule-based`, chúng ta cũng phân tích một tập hợp các kiểu mua sắm nhưng dưới dạng chuỗi và sau đó khuyến nghị những items còn lại diễn ra sau đó.\n",
        "\n",
        "### Tiếp cận mô hình (model_based)\n",
        "\n",
        "Là phương pháp phức tạp hơn, tiếp cận theo mô hình sẽ yêu cầu xây dựng thuật toán dựa trên giải thuyết ngặt đó là dữ liệu có tính thời gian và tính thứ tự chẳng hạn như mô hình Markov Chain. Các cách tiếp cận của phương pháp mô hình cơ sở chủ yếu được phân thành 3 loại chính gồm: mô hình Markov Chain, mô hình phân tích nhân tố (factorization model) và mô hình mạng neural.\n",
        "\n",
        "1. **Mô hình Markov Chain**: Mô hình hồi qui sự phụ thuộc của bậc 1 qua toàn bộ chuỗi các items bằng việc sử dụng sự dịch chuyển xác suất và sau đó tạo ra các khuyến nghị của các items theo sau đó bằng việc tối ưu hóa các sự phụ thuộc này. Sự khác biệt so với phương pháp chuỗi Pattern-based đó là mô hình dễ dàng lọc ra những pattern và items không thường xuyên và dẫn tới sự mất mát về thông tin. Markov Chain sẽ lấy toàn bộ các đầu vào và do đó làm giảm mất mát thông tin đáng kể.\n",
        "\n",
        "2. **Mô hình phân tích nhân tố**: Phương pháp này gần giống với [Bài 3 - Mô hình Word2Vec](https://phamdinhkhanh.github.io/2019/04/29/ModelWord2Vec.html). Những cách tiếp cận này sẽ phân tích suy biến ma trận đồng xuất hiện hoặc ma trận dịch chuyển item-to-item thành những véc tơ nhân tố ẩn đại diện cho mỗi item và sau đó dự báo các items theo sau đó bằng cách sử dụng những biểu diễn nhân tố ẩn. Cần phân biệt các cách tiếp cận này với các phương pháp phân tích suy biến ma trận trong collaborative-filtering khi chúng sử dụng đầu vào là ma trận tương tác user-item thay vì item-to-item. Các véc tơ nhân tố ẩn của users và items đồng thời cũng được xác định một cách riêng biệt.\n",
        "\n",
        "3. **Mô hình mạng neural**: Mô hình này tận dụng những lợi thế của mô hình mạng neural để học những mối liên hệ phức tạp giữa những items trong cùng một session. Mô hình có thể là những mô hình học nông (shallow network) chỉ với vài layer như trong bài [Bài 20 - Recommendation Neural Network](https://phamdinhkhanh.github.io/2019/12/26/Sorfmax_Recommendation_Neural_Network.html) nhằm tìm ra biểu diễn nhúng của các item hoặc mô hình học sâu (deep network) như các lớp mô hình RNN sẽ được giới thiệu ở phần thực hành.\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/n2VSMD0.png)\n",
        "\n",
        "**Hình 1:** Sơ đồ mô hình Recommendation System theo session based.\n",
        "\n",
        "### So sánh các phương pháp tiếp cận.\n",
        "\n",
        "Nói chung, phương pháp phi mô mô hình rất đơn giản, dễ hiểu và dễ thực hiện. Bởi vì cả hai cách tiếp cận Pattern/Rule-based and Sequential Pattern-based chỉ dựa trên tần suất, chúng rất dễ lọc ra các items hoặc patterns không xuất hiện thường xuyên nhưng quan trọng. Do đó, các phương pháp phi mô hình phù hợp với khám phá mối quan hệ giữa các item thường xuyên trong bộ dữ liệu đơn giản. Tuy nhiên, chúng có thể dễ dàng thất bại trong việc mô hình hóa sự phụ thuộc phức tạp trong các bộ dữ liệu phức tạp cho các khuyến nghị dựa trên session. \n",
        "\n",
        "Ngược lại, cách tiếp cận dựa trên mô hình phức tạp hơn nhiều nhưng có sức mạnh đối với các bộ dữ liệu phức tạp. Một mặt, chúng không lọc rõ ràng các item hoặc patterns từ các bộ dữ liệu, nhưng giữ lại thông tin ở mức tối đa. Mặt khác, nhờ sự phức tạp của các cơ chế như mạng học sâu neural networks, các phương pháp dựa trên lớp mô hình này có thể mô hình hóa quan hệ phức tạp và tiềm ẩn trong dữ liệu, dẫn đến các khuyến nghị dựa trên session tốt hơn.\n",
        "\n",
        "Trong phần thực hành bài này tôi sẽ tiếp cận Session-Based System theo phương pháp mô hình và sử dụng một kiến trúc học sâu để dự báo item tiếp theo trong một chuỗi session. Thuật toán được sử dụng có tên là `Neural Attentive Session-based Recommendation`. Một thuận toán kết hợp giữa kiến trúc mạng GRU trong RNN với cơ chế attention. Chúng ta sẽ tìm hiểu về thuật toán này bên dưới.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB_hmv4DeuH6",
        "colab_type": "text"
      },
      "source": [
        "## 3. Lý thuyết mô hình GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzcWf9lQHsSh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Như chúng ta đã biết, một trong những ưu điểm của mô hình RNN là có khả năng ghi nhớ được sự phụ thuộc chuỗi, do đó rất phù hợp để áp dụng vào mô hình session-based recommendation system. Tuy nhiên hạn chế của RNN đó là sự phụ thuộc dài hạn yếu và thường xảy ra hiện tượng triệt tiêu đạo hàm. GRU là một kiến trúc của RNN cho phép giải quyết được vấn đề triệt tiêu đạo hàm nhờ cơ chế thêm hoặc bớt thông tin thông qua các cổng cập nhật, tái thiết lập. Một lớp mô hình khác là LSTM cũng giải quyết được vấn đề triệt tiêu đạo hàm. Tuy nhiên các thử nghiệm giữa 2 mô hình LSTM và GRU áp dụng trong kiến trúc mô hình recommendation này đều cho thấy kết quả của GRU là vượt trội hơn.\n",
        "\n",
        "**Kiến trúc mô hình GRU**\n",
        "\n",
        "Có rất nhiều kiến trúc khác nhau của mạng RNN như LSTM, GRU. Nhìn chung kiến trúc của chúng đều là một chu kì tuần hoàn gồm các layers nối tiếp nhau. Sự khác biệt giữa các mạng được thể hiện qua kiến trúc chi tiết trong từng layer, bạn đọc có thể xem thêm [Bài 2 - Lý thuyết về mạng LSTM](https://phamdinhkhanh.github.io/2019/04/22/Ly_thuyet_ve_mang_LSTM.html). Ở bài này tôi sẽ áp dụng mô hình GRU vào xây dựng một hệ thống recommendation system. GRU là một trong những kiến trúc giải quyết được tốt vấn đề triệt tiêu gradient (vanishing gradient problem) nhờ các `cổng cập nhật` (update gate) và `cổng tái thiết lập` (reset gate). Hai cổng này giúp điều chỉnh thông tin để quyết định xem thông tin nào được truyền qua chúng. Chính vì thế chúng có thể giữ được thông tin qua một số lượng quãng thời gian rất dài mà không mất đi thông tin quan trọng và xóa đi những thông tin không liên quan. Cụ thể về những cổng này sẽ được diễn giải như sau:\n",
        "\n",
        "![](https://miro.medium.com/max/3146/1*7oE-4Wg6bZ7u8yDf5cjJPA.png)\n",
        "\n",
        "> **Hình 2:** Sơ đồ một GRU layer\n",
        "\n",
        "Trong hình vẽ trên:\n",
        "\n",
        "* Dấu $+$ kí hiệu cho phép cộng véc tơ.\n",
        "* Dấu $\\sigma$ kí hiệu cho hàm sigmoid có giá trị như sau:\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
        "* Dấu $\\odot$ kí hiệu cho tích hadamard của các véc tơ.\n",
        "$$\\mathbf{u} \\odot \\mathbf{v} = (u_1 v_1, u_2 v_2, \\dots, u_n v_n)$$\n",
        "\n",
        "Tại mỗi time step sẽ nhận đầu vào là các hidden véc tơ và chính là đầu ra của time step trước kết hợp với véc tơ nhúng của input tại time step đó. Sau khi đi qua một loạt các cổng bên trong layer GRU sẽ trả về kết quả là một hidden véc tơ ở đầu ra. Cụ thể trong layer GRU bao gồm những bước nào sẽ diễn giải bên dưới.\n",
        "\n",
        "**Các bước của GRU layer:**\n",
        "\n",
        "* **Bước 1**: Cổng cập nhật\n",
        "\n",
        "Đầu tiên GRU layer sẽ quyết định thông tin nào sẽ bị loại bỏ khỏi bộ nhớ bằng `cổng cập nhật`. Hàm $\\sigma$ có giá trị càng lớn tiệm cận 1 thì lượng thông tin cũ được lưu giữ càng lớn và ngược lại. Đầu vào của `cổng cập nhật` là các véc tơ hidden $\\mathbf{h}_{t-1}$ và véc tơ nhúng $\\mathbf{x}_t$.\n",
        "\n",
        "$$z_t = \\sigma(\\mathbf{W}_z[\\mathbf{h}_{t-1}, \\mathbf{x}_{t}])$$\n",
        "\n",
        "$\\mathbf{h}_{t-1}$ là kết quả ouput của layer GRU thứ $t-1$ nên lưu giữ được các thông tin của toàn bộ $t-1$ bước thời gian trước. Thông tin này được kết nối với thông tin ở bước thời gian hiện tại $\\mathbf{x}_t$. `Cổng cập nhật` sẽ giúp mô hình xác định được bao nhiêu thông tin trong quá khứ sẽ được chuyển tiếp tới tương lai. Cơ chế cổng cập nhật thực sự rất mạnh, chỉ cần copy toàn bộ lại thông tin trong quá khứ chúng ta sẽ loại bỏ được rủi ro triệt tiêu gradient.\n",
        "\n",
        "* **Bước 2**: Cổng tái thiết lập\n",
        "\n",
        "Trái ngược với cổng cập nhật, cổng tái thiết lập sẽ quyết định bao nhiêu thông tin trong quá khứ sẽ bị lãng quên. Chúng ta cũng sử dụng hàm $\\sigma$ để tính toán thông tin bị lãng quên:\n",
        "\n",
        "$$r_t = \\sigma(\\mathbf{W}_r[\\mathbf{h}_{t-1}, \\mathbf{x}_t])$$\n",
        "\n",
        "* **Bước 3**: Nội dung nhớ hiện tại\n",
        "\n",
        "Sau khi tính toán xong các `cổng cập nhật` và `cổng tái thiết lập`, thông tin mới sẽ được tính toán lại bằng cách kết hợp thông tin từ cổng tái thiết lập và đầu vào tại time step $t$ như sau:\n",
        "\n",
        "$$h'_t = \\tanh(\\mathbf{W}[r_t \\odot \\mathbf{h}_{t-1}, \\mathbf{x}_t])$$\n",
        "\n",
        "Gía trị tổng hợp quá khứ $\\mathbf{h}_{t-1}$ đã được loại bỏ những thông tin không quan trọng bằng cách nhân với véc tơ tái thiết lập $r_t$. Việc cập nhật này là cực kì quan trọng, giả sử trong một tác vụ phân loại cảm xúc bình luận có một câu bình luận như sau:\n",
        "\n",
        "'Đây là một bộ phim rất hay.'\n",
        "\n",
        "Để biết được nội dung của bình luận trên là tích cực hay tiêu cực ta chỉ cần quan tâm đến từ cuối cùng. Như vậy tại bước thời gian $t$, ta chỉ cần điều chỉnh véc tơ $r_t$ về gần 0 để cho các giá trị trước đó bị lãng quên thì giá trị mới sau cập nhật trọng tâm vào giá trị hiện tại $\\mathbf{x}_t$.\n",
        "\n",
        "* **Bước 4**: Trí nhớ cuối cùng tại time step hiện tại\n",
        "\n",
        "Đây là bước cuối cùng để tính toán ra output chính là véc tơ hidden $\\mathbf{h}_t$. Véc tơ hiện tại sẽ tổng hợp thông tin giữa nội dung nhớ hiện tại $h'_t$ với các thông tin được lưu giữ tại các bước trước đó $h_{t-1}$ thông qua trọng số hiệu chỉnh là véc tơ cổng cập nhật $z_t$:\n",
        "\n",
        "$$\\mathbf{h}_t = z_t \\odot h_{t-1}+(1-z_t) \\odot h'_{t}$$\n",
        "\n",
        "Có thể coi các hệ số $z_t, (1-z_t)$ như là các hệ số đánh đổi giữa giá trị quá khứ và giá trị hiện tại. Như vậy muốn lưu giữ nhiều thông tin quá khứ hơn thì thiết lập $z_t$ gần với 1 hơn và muốn giữ lại nhiều thông tin hiện tại hơn thì thiết lập $z_t$ gần với 0 hơn. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v7wgzwZexT7",
        "colab_type": "text"
      },
      "source": [
        "### 3.1. Global encoder \n",
        "\n",
        "Global encoder là quá trình encode các thông tin đầu vào chính là các hidden véc tơ tại mỗi time step. Kết quả sau cùng thu được là một global véc tơ được thiết lập bằng chính hidden véc tơ tại time step cuối cùng như sau:\n",
        "\n",
        "$$c_{t}^{g} = \\mathbf{h}_t$$\n",
        "\n",
        "global véc tơ sẽ có tác dụng tổng hợp lại toàn bộ các thông tin tại các time step trước đó của mô hình. Do đó nó đại diện cho nội dung của toàn bộ input ở pharse encoder. Tuy nhiên $\\mathbf{h}_t$ không thể hiện được mối liên hệ cục bộ giữa các inputs tại những time step khác nhau. Mối liên hệ này sẽ được thể hiện thông qua một cơ chế attention được giải thích bên dưới."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2yrai3de4zH",
        "colab_type": "text"
      },
      "source": [
        "### 3.2. Local encoder\n",
        "\n",
        "Để ghi nhận được hành vi của người dùng tại các session hiện tại, chúng ta dựa vào một cơ chế attention cho phép lựa chọn các kết hợp tùy ý từ đầu vào tại các time step thông qua tổ hợp tuyến tính của chúng. Về kĩ thuật attention các bạn có thể xem thêm tại [Bài 4 - Attention is all you need](https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html).\n",
        "\n",
        "\n",
        "![Global_vs_Local_Encoder](https://i.imgur.com/3AVpLGx.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41JG1SIpPPH1",
        "colab_type": "text"
      },
      "source": [
        "(a) Sơ đồ graphic của global encoder trong NARM, hidden state cuối cùng được mô tả như feature của chuỗi hành vi của user $c_{t}^{g} = \\mathbf{h}_{t}$ | (b) Sơ đồ graphic của local encoder trong NARM, ở đây tổng có trọng số của toàn bộ các hidden véc tơ được mô tả như feature ghi nhận hành vi của user $c_{t}^{l} = \\sum_{j=1}^{t} \\alpha_{tj}\\mathbf{h}_t$\n",
        "\n",
        "> **Hình 3:** Cơ chế global encoder và local encoder trong mô hình NARM.\n",
        "\n",
        "Để tính toán được local véc tơ ta cần tính các hệ số attention $\\alpha_{tj}$ đại diện cho mức độ phân phối attention của đầu vào tại time step $j$ tới đầu ra tại time step $t$.\n",
        "\n",
        "Khác với cơ chế tính trọng số $\\alpha_{tj}$ ở attention layer trong [Bài 4](https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html) một chút. Chúng ta sẽ không tính $\\alpha_{tj}$ trực tiếp từ $\\text{score}(\\mathbf{h}_t, \\mathbf{h}_j)$. Ở đây ta sử dụng một cơ chế phức tạp hơn, bao gồm các bước:\n",
        "\n",
        "* **Bước 1:** Giảm chiều dữ liệu của $\\mathbf{h}_t, \\mathbf{h}_j$ bằng một phép chiếu linear projection lên không gian latent bằng cách nhân với các ma trận $\\mathbf{A}_1$ và $\\mathbf{A}_2$. Khi đó thu được véc tơ latent tổng hợp như sau:\n",
        "\n",
        "$$\\mathbf{l}_{tj} = \\mathbf{A}_1 \\mathbf{h}_t + \\mathbf{A}_2 \\mathbf{h}_j$$\n",
        "\n",
        "* **Bước 2:** Tính véc tơ phân phối xác suất cho véc tơ latent $\\mathbf{l}_{tj}$ bằng cách truyền vào hàm sigmoid. Hệ số $\\alpha_{tj}$ bằng tổng tổ hợp tuyến tính của sigmoid với các hệ số của véc tơ $\\mathbf{v}$.\n",
        "\n",
        "$$\\alpha_{tj} = \\mathbf{v}^{\\intercal}\\sigma(\\mathbf{l}_{tj})$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKbecrHtaqOH",
        "colab_type": "text"
      },
      "source": [
        "### 3.3. Kiến trúc mô hình NARM\n",
        "\n",
        "\n",
        "![NARMEncoderDecoder](https://i.imgur.com/9T55U5X.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2utZwcsta7Vd",
        "colab_type": "text"
      },
      "source": [
        "**Hình 4:** Sơ đồ mô hình NARM, ở đây session của feature $c_t$ được biểu diễn bởi véc tơ concatenate $c_t = [c_t^{g}; c_t^{l}]$.\n",
        "\n",
        "Đối với nhiệm vụ của session-based recommedation, global encoder sẽ thống kê toàn bộ chuỗi hành vi, trong khi local encoder có thể lựa chọn phù hợp một số items quan trọng trong session hiện tại để nắm bắt mục đích chính của user. Chúng ta phỏng đoán rằng biểu diễn của một chuỗi hành vi có thể cung cấp những thông tin hữu ích trong session hiện tại. Do đó, chúng ta sử dụng biểu diễn của chuỗi hành vi và của hidden states liền trước để tính toán attention weight cho mỗi item click. Sau đó một mở rộng tự nhiên kết hợp các chuỗi đặc trưng hành vi và đặc trưng mục đích của user bằng cách concatenate chúng để tạo thành một biểu diễn mở rộng cho mỗi time step.\n",
        "\n",
        "Như thể hiện trong hình 4, chúng ta có thể nhìn thấy véc tơ tổng hợp $\\mathbf{h}_{t}^{g}$ được kết hợp vào $c_t$ để cung cấp một biểu diễn chuỗi hành vi cho mô hình NARM. Do đó $\\mathbf{h}_{t}^{g}$ được coi là global encoder. Sau đó chúng được sử dụng để tính toán attention weight với những hidden states trước đó. Bằng cơ chế encoding hỗi hợp, cả chuỗi hành vi và mục đích chính của user trong session hiện tại có thể được mô hình vào một biểu diễn thống nhất $c_t$, nó là kết hợp của các véc tơ $c_t^g$ và $c_t^l$ như sau:\n",
        "\n",
        "$$c_t = [c_t^{g}; c_t^{l}] = [\\mathbf{h}_t^{g};\\sum_{j=1}^{t} \\alpha_{tj}\\mathbf{h}_t^{l}]$$\n",
        "\n",
        "Hình 4 cũng đưa ra một sơ đồ mô phỏng cơ chế decoding trong mô hình NARM. Tổng quát hóa, một RNN chuẩn tối ưu hóa fully-connected layer để giải mã. Nhưng sử dụng fully-connected layer có nghĩa rằng số lượng các tham số cần phải được học trong layer này là $|H|*|N|$ ở đây $|H|$ là số chiều của biểu diễn session và $|N|$ là số item được đề xuất cho dự báo. \n",
        "\n",
        "Chính vì thế, chúng ta phải dự trữ một không gian lớn để lưu trữ các tham số này. Mặc dù có một vài cách tiếp cận để giảm tham số như sử dụng hierachical softmax layer và negative sampling ngẫu nhiên, chúng không phải là lựa chọn phù hợp nhất cho mô hình của chúng ta.\n",
        "\n",
        "Một cách thay thế được đề xuất là bi-linear decoding scheme giúp không chỉ giảm số lượng các tham số mà còn cải thiện kết quả cho mô hình NARM. Đặc biệt, một hàm số đo độ tương đương bi-linear giữa các biểu diễn của session hiện tại và mỗi đề cử item được sử dụng để tính toán điểm tương đương $Si$,\n",
        "\n",
        "$$S_i = \\text{emb}_{i}^{\\top}\\mathbf{B}\\mathbf{c}_t$$\n",
        "\n",
        "Trong đó $\\mathbf{B} \\subset \\mathbb{R}^{D \\times H}$, $D$ là chiều của mỗi item embedding. Sau đó điểm tương đương của mỗi item được đưa vào một softmax layer để đạt được xác suất mà item sẽ xảy ra tiếp theo. Bằng sử dụng bi-linear decoder, chúng ta sẽ giảm số lượng các tham số từ $N \\times H$ xuống còn $D \\times H$, ở đây $D$ nhỏ hơn đáng kể so với $N$. Ngoài ra, kết quả kiểm nghiệm cho thấy sử dụng bi-linear decoder có thể cải thiện biểu diễn của mô hình NARM.\n",
        "\n",
        "Để học tham số biểu diễn của mô hình, chúng ta không tối ưu hóa thủ tục huấn luyện được đề xuất, ở đây mô hình được huấn luyện trong một session-parallel, sequence-to-sequence manner. Thay vào đó, nhằm mục đích làm phù hợp attention vào local encoder, NARM tiến hành mỗi chuỗi $[x_1, x_2, \\dots, x_{t-1}, x_t]$ một cách tách biệt. Mô hình của chúng ta có thể được huấn luyện sử dụng một mini-batch gradient chuẩn trên hàm mất mát cross-entropy:\n",
        "\n",
        "$$L(p, q) = -\\sum_{i=1}^{m}p_i\\log(q_i)$$\n",
        "\n",
        "\n",
        "Với $p$ là phân phối xác suất ground truth và $q$ phân phối xác suất được dự báo. Cuối cùng chúng ta sử dụng phương pháp lan truyền ngược (Back propagation through time) để cập nhật các biểu diễn cho các hidden state ứng với mỗi item.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPLUrYnBe6u-",
        "colab_type": "text"
      },
      "source": [
        "## 4. Huấn luyện mô hình"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syL49_JLUBos",
        "colab_type": "text"
      },
      "source": [
        "### 4.1. Dataset\n",
        "\n",
        "[yoochoose](https://2015.recsyschallenge.com/) là tập dữ liệu được cung cấp từ cuộc thi RecSys Challange 2015. Dữ liệu bao gồm tập hợp các thông tin về các click event trong từng session tương ứng. Trong các events sẽ bao gồm buy event giúp xác định các sự kiện mua hàng. Mục tiêu là dự báo khách hàng có khả năng mua hàng sắp tới không và nếu mua hàng thì mua sản phẩm nào?\n",
        "\n",
        "Bộ dữ liệu bao gồm 3 files chính:\n",
        "\n",
        "**Các files huấn luyện:**\n",
        "\n",
        "1 . **yoochoose-clicks.dat**: Dữ liệu về các click events. Mỗi một dòng bao gồm các trường sau đây:\n",
        "\n",
        "* Session ID: Id của session. Trong một session có thể có 1 hoặc nhiều lượt clicks.\n",
        "* Timestamp: Thời điểm diễn ra click.\n",
        "* Item ID: Id dùng để xác định item.\n",
        "* Category: Nhóm category của item.\n",
        "\n",
        "2 . **yoochoose-buys.dat**: Dữ liệu về buy events. Mỗi một dòng bao gồm các trường sau đây:\n",
        "\n",
        "* Session ID: Id của session. Trong một session có thể có 1 hoặc nhiều lượt mua sắm.\n",
        "* Timestamp: Thời điểm diễn ra hành vi mua sắm.\n",
        "* Item ID: Id của item.\n",
        "* Price: Gía của sản phẩm.\n",
        "* Quantity: Số lượng sản phẩm được mua.\n",
        "\n",
        "Do các trong các session sẽ một số session có các event buying nên số lượng các session trong file yoochoose-buys.dat nhiều hơn so với yoochoose-clicks.dat và mọi session trong file yoochoose-buys.dat sẽ chứa trong file yoochoose-clicks.dat. Một session có thể rất ngắn (vài phút) hoặc rất dài (vài giờ) và theo đó số lượng event cũng tương ứng ít hoặc nhiều tùy vào hoạt động của user.\n",
        "\n",
        "**File kiểm tra:**\n",
        "\n",
        "3 . **yoochoose-test.dat**: Cấu trúc tương tự như file yoochoose-clicks.dat của huấn luyện. Bao gồm các trường: Session ID, Timestamp, Item ID, Category.\n",
        "Mục tiêu của chúng ta là cần dự báo xem trong các session của tập test có event buying hay không và nếu có thì list các Item ID được mua là gì?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41TUsDixfBxa",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1.1. Khởi tạo dữ liệu\n",
        "\n",
        "Bên dưới chúng ta sẽ tiến hành đọc dữ liệu từ các file trong tập dataset `yoochoose` và thực hiện một số lọc bỏ các session có độ dài ngắn và các item có tần suất xuất hiện thấp.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qX5dyW0fqCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdriver')\n",
        "path = '/content/gdriver/My Drive/Colab Notebooks/Recommendation'\n",
        "os.chdir(path)\n",
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbNnDDx_gkQK",
        "colab_type": "text"
      },
      "source": [
        "Unzip file `yoochoose-data.7z`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uod7EeNBksRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install pyunpack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N5I1U7Zk9wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install patool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00fVBh69gxoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pyunpack import Archive\n",
        "\n",
        "# os.mkdir('yoochoose-data')\n",
        "# Archive('yoochoose-data.7z').extractall('yoochoose-data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_0xFeTLmJLr",
        "colab_type": "text"
      },
      "source": [
        "Đọc dữ liệu yoochoo-clicks.dat dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrvhwgUxnZmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.listdir('yoochoose-data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6xUzjj4k1pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# import pandas as pd\n",
        "# from datetime import datetime\n",
        "# import time\n",
        "\n",
        "# filepath = 'yoochoose-data/yoochoose-clicks.dat'\n",
        "# # os.getcwd('input')\n",
        "# dataset = pd.read_csv(filepath, names = ['SessionId', 'DateTime', 'ItemId', 'Category'])\n",
        "# dataset['DateTime'] = dataset['DateTime'].apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ'))\n",
        "# dataset['Timestamp'] = dataset['DateTime'].apply(lambda x: x.strftime('%s.%f'))\n",
        "# dataset = dataset.sort_values(by = ['SessionId', 'DateTime'], ascending = True)\n",
        "# print(dataset.shape)\n",
        "# dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIjn2d640O97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List các sessionId."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCk6aU6qnswi",
        "colab_type": "text"
      },
      "source": [
        "Bên dưới ta sẽ lọc bỏ các itemId có ít hơn 5 lượt xuất hiện và lọc bỏ các session có ít hơn 2 itemId."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYVkX9ihns87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Thống kê số lượt xuất hiện và lọc ra các ItemId có trên 5 lượt xuất hiện\n",
        "# df_item_count = dataset[['ItemId', 'SessionId']].groupby('ItemId').count().sort_values(by = 'SessionId', ascending = False)\n",
        "# df_item_count.columns = ['CountItemId']\n",
        "# df_item_count_5 = df_item_count[df_item_count['CountItemId'] < 5]\n",
        "# # Lọc khỏi dataset những ItemId có ít hơn 5 lượt xuất hiện\n",
        "# dataset = dataset[~dataset['ItemId'].isin(list(df_item_count_5.index))]\n",
        "# print(dataset.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYEUGyfAfFFI",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1.2. Phân chia dữ liệu train/test\n",
        "\n",
        "Từ tập dữ liệu train ta sẽ tách ra 7 ngày cuối cùng làm dữ liệu test. Đối với dữ liệu còn lại để việc huấn luyện được nhanh hơn thì chúng ta sẽ chỉ dữ lại 1/4 số lượng các session cho huấn luyện.\n",
        "\n",
        "Khi đó dữ liệu test sẽ có cấu trúc tương tự như train, mỗi session bao gồm các itemIds được sắp xếp theo thứ tự thời gian. Từ list các itemId liền trước ta cần dự báo itemId tiếp theo có khả năng được click. Các dữ liệu trên test được xem như là session mới hoàn toàn và được sử dụng để kiểm tra mức độ dự báo chuẩn xác của mô hình được huấn luyện từ tập dữ liệu train.\n",
        "\n",
        "Bên dưới ta sẽ thực hành phân chia train/test cho mô hình."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNepGzg9rF15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from datetime import timedelta\n",
        "# # Phân chia tập train/test sao cho tập test là 7 ngày gần đây nhất và train là dữ liệu còn lại\n",
        "# maxdate = dataset['DateTime'].max()\n",
        "# mindate7 = maxdate - timedelta(days = 7)\n",
        "# test = dataset[dataset['DateTime'] >= mindate7]\n",
        "# dataset = dataset[dataset['DateTime'] <= mindate7]\n",
        "# print(dataset.shape)\n",
        "# print(test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAIDQE3pwNSI",
        "colab_type": "text"
      },
      "source": [
        "Lấy ra ngẫu nhiên 1/4 số lượng các session cho huấn luyện."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdIPuxU0wMt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # list các sessionIds\n",
        "# sessIds = list(dataset['SessionId'].unique())\n",
        "# # Lấy ngẫu nhiên 1/4 số lượng các session\n",
        "# n_filter = int(len(sessIds)/4)\n",
        "# np.random.shuffle(sessIds)\n",
        "# sessIdsFilter = sessIds[:n_filter]\n",
        "# # Lựa chọn các 1/4 session làm dataset train (dữ liệu này bao gồm cả train và validation)\n",
        "# # Set index là sessionId để filter nhanh hơn\n",
        "# dataset.set_index('SessionId', inplace=True)\n",
        "# dataset_filter = dataset[dataset.index.isin(sessIdsFilter)]\n",
        "# dataset_filter = dataset_filter.reset_index()\n",
        "# print(dataset_filter.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScPVda69s0_2",
        "colab_type": "text"
      },
      "source": [
        "Khởi tạo chuỗi các itemId sắp xếp theo thời gian trên mỗi session. Mỗi một itemId sẽ tương ứng với giá trị thời gian của nó. Cấu trúc của các `train_sess, test_sess` có dạng:\n",
        "\n",
        "`['sessionId': {'itemId1':'Timestamp1', ..., 'itemId_n':'Timestamp_n'}]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7IdxxqMsXSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Lấy ra dictionary có dạng {SessionId:{ItemId1:Timestamp1, ItemId2:Timestamp2, ...}}\n",
        "# train_sess = dataset_filter[['SessionId', 'ItemId', 'Timestamp']].groupby('SessionId').apply(lambda x: dict(zip(x['ItemId'], x['Timestamp'])))\n",
        "# test_sess = test[['SessionId', 'ItemId', 'Timestamp']].groupby('SessionId').apply(lambda x: dict(zip(x['ItemId'], x['Timestamp'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM7xyGRzfHub",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1.3. Preprocessing data\n",
        "\n",
        "Ở bước này ta sẽ khởi tạo input và output cho mô hình.\n",
        "\n",
        "Input của mô hình là list các itemIds trong quá khứ và Output là itemId ở vị trí hiện tại. Qúa trình khởi tạo các Input và Output trên một session được thực hiện tịnh tiến từ vị trí itemID đầu tiên cho đến cuối cùng.\n",
        "\n",
        "Ví dụ: Chúng ta có `sessionId = [itemId_1, itemId_2, ..., itemId_n]`\n",
        "Như vậy sau các cặp (input, output) sẽ là:\n",
        "\n",
        "* cặp thứ 1: `([itemId_1], [itemId_2])`\n",
        "* cặp thứ 2: `([itemId_1, itemId_2], [itemId_3])`\n",
        "\n",
        "  ...\n",
        "\n",
        "* cặp thứ n-1: `([itemId_1, itemId_2,..., itemId_(n-1)], [itemId_n])`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHC4gaZI18Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sessDict = {214834865: '1396808691.295000', 214706441: '1396808691.426000', 214820225: '1396808691.422000'}\n",
        "\n",
        "# def _preprocess_sess_dict(sessDict):\n",
        "#     sessDictTime = dict([(v, k) for (k, v) in sessDict.items()])\n",
        "#     sessSort = sorted(sessDictTime.items(), reverse = False)\n",
        "#     times = [item[0] for item in sessSort]\n",
        "#     itemIds = [item[1] for item in sessSort]\n",
        "#     inp_seq = []\n",
        "#     labels = []\n",
        "#     inp_time = []\n",
        "\n",
        "#     for i in range(len(sessSort)):\n",
        "#         if i >= 1:\n",
        "#             inp_seq += [itemIds[:i]]\n",
        "#             labels += [itemIds[i]]\n",
        "#             inp_time += [times[i]]\n",
        "#     return inp_seq, inp_time, labels, itemIds\n",
        "\n",
        "# inp_seq, inp_time, labels, itemIds = _preprocess_sess_dict(sessDict)\n",
        "# print('input sequences: ', inp_seq)\n",
        "# print('input times: ', inp_time)\n",
        "# print('targets: ', labels)\n",
        "# print('sequence: ', itemIds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcn5TMYN4c8-",
        "colab_type": "text"
      },
      "source": [
        "Khởi tạo chuỗi input và output cho toàn bộ các session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTwTRQgQ4iDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _preprocess_data(data_sess):\n",
        "#     inp_seqs = []\n",
        "#     inp_times = []\n",
        "#     labels = []\n",
        "#     sequences = []\n",
        "#     sessIds = list(data_sess.index)\n",
        "#     for sessId in sessIds:\n",
        "#         sessDict = data_sess.loc[sessId]\n",
        "#         inp_seq, inp_time, label, sequence = _preprocess_sess_dict(sessDict)\n",
        "#         inp_seqs += inp_seq\n",
        "#         inp_times += inp_time\n",
        "#         labels += label\n",
        "#         sequences += sequence\n",
        "#     return inp_seqs, inp_times, labels, sequences\n",
        "\n",
        "# train_inp_seqs, train_inp_dates, train_labs, train_sequences = _preprocess_data(train_sess)\n",
        "# test_inp_seqs, test_inp_dates, test_labs, test_sequences = _preprocess_data(test_sess)\n",
        "\n",
        "# train = (train_inp_seqs, train_labs)\n",
        "# test = (test_inp_seqs, test_labs)\n",
        "\n",
        "# print('Done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10vf9Z5c5fn1",
        "colab_type": "text"
      },
      "source": [
        "Do kích thước dữ liệu là khá lớn nên để thuận lợi cho những lượt train sau ta nên lưu dữ liệu của train và test vào một folder là `yoochoose-data-4`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhKKJ2ry7j_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# import os\n",
        "\n",
        "# def _save_file(filename, obj):\n",
        "#   with open(filename, 'wb') as fn:\n",
        "#     pickle.dump(obj, fn)\n",
        "\n",
        "# # Tạo folder yoochoose-data-4 để lưu dữ liệu train/test nếu chưa tồn tại\n",
        "# if not os.path.exists('yoochoose-data/yoochoose-data-4'):\n",
        "#   os.mkdir('yoochoose-data/yoochoose-data-4')\n",
        "\n",
        "# # Lưu train/test\n",
        "# _save_file('yoochoose-data/yoochoose-data-4/train.pkl', train)\n",
        "# _save_file('yoochoose-data/yoochoose-data-4/test.pkl', test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp6eYuHE8Rad",
        "colab_type": "text"
      },
      "source": [
        "Ở những lượt huấn luyện sau ta chỉ cần load lại những dữ liệu train/test đã lưu tại folder `yoochoose-data-4`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2w7CVyl8jTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def _load_file(filename):\n",
        "  with open(filename, 'rb') as fn:\n",
        "    data = pickle.load(fn)\n",
        "  return data\n",
        "\n",
        "# Load dữ liệu train/test từ folder\n",
        "train = _load_file('yoochoose-data/yoochoose-data-4/train.pkl')\n",
        "test = _load_file('yoochoose-data/yoochoose-data-4/test.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk3QShYBvrqG",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1.4. Khởi tạo dictionary\n",
        "\n",
        "Chúng ta cần sử dụng dictionary để nhúng itemId của mỗi một sản phẩm thành một index sao cho mỗi chỉ số này là duy nhất đối với mỗi itemId. Từ index ta có thể tạo ra được các véc tơ one-hot dễ dàng làm đầu vào cho huấn luyện mô hình ở bước embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5NCBvQ2vsf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Các token default\n",
        "PAD_token = 0  # token padding cho câu ngắn\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.item2index = {}\n",
        "        self.item2count = {}\n",
        "        self.index2item = {PAD_token: \"PAD\"}\n",
        "        self.num_items = 1  # số lượng mặc định ban đầu là 1 ứng với PAD_token\n",
        "\n",
        "    def addSenquence(self, data):\n",
        "        for sequence in data:\n",
        "          for item in sequence:\n",
        "              self.addItem(item)\n",
        "\n",
        "    # Thêm một item vào hệ thống\n",
        "    def addItem(self, item):\n",
        "        if item not in self.item2index:\n",
        "            self.item2index[item] = self.num_items\n",
        "            self.item2count[item] = 1\n",
        "            self.index2item[self.num_items] = item\n",
        "            self.num_items += 1\n",
        "        else:\n",
        "            self.item2count[item] += 1\n",
        "\n",
        "    # Loại các item dưới ngưỡng xuất hiện min_count\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "        \n",
        "        keep_items = []\n",
        "\n",
        "        for k, v in self.item2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_items.append(k)\n",
        "\n",
        "        print('keep_items {} / {} = {:.4f}'.format(\n",
        "            len(keep_items), len(self.item2index), len(keep_items) / len(self.item2index)\n",
        "        ))\n",
        "\n",
        "        # Khởi tạo lại từ điển\n",
        "        self.item2index = {}\n",
        "        self.item2count = {}\n",
        "        self.index2item = {PAD_token: \"PAD\"}\n",
        "        self.num_items = 1\n",
        "\n",
        "        # Thêm các items vào từ điển\n",
        "        for item in keep_items:\n",
        "            self.addItem(item)\n",
        "\n",
        "    # Hàm convert sequence về chuỗi các indices\n",
        "    def _seqItem2seqIndex(self, x):\n",
        "        return [voc.item2index[item] if item in voc.item2index else 0 for item in x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viZkOCMkNpuq",
        "colab_type": "text"
      },
      "source": [
        "Lấy toàn bộ list các itemIds trong các session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhYF9RKPNoyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import chain\n",
        "seq_targets = [train[1]] + [test[1]]\n",
        "sessionIds = list(chain.from_iterable(seq_targets))\n",
        "sessionIds = set(sessionIds)\n",
        "print('Number of sessionIds: ', len(sessionIds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVuHIqoQN1qG",
        "colab_type": "text"
      },
      "source": [
        "Khởi tạo vocabullary cho bộ dữ liệu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8_LU4aHzEhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = Voc('DictItemId')\n",
        "voc.addSenquence(seq_targets)\n",
        "\n",
        "# Convert thử nghiệm một sequence itemIds\n",
        "print('sequence of itemIds: ', train[0][7])\n",
        "print('converted indices: ', voc._seqItem2seqIndex(train[0][7]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ77PTEpN8EZ",
        "colab_type": "text"
      },
      "source": [
        "Tiếp theo ta sẽ chuyển dữ liệu train,  test từ item sang indices của item"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH6YBA8gONjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x_index = [voc._seqItem2seqIndex(seq) for seq in train[0]]\n",
        "test_x_index = [voc._seqItem2seqIndex(seq) for seq in test[0]]\n",
        "train_y_index = voc._seqItem2seqIndex(train[1])\n",
        "test_y_index = voc._seqItem2seqIndex(test[1])\n",
        "train_index = (train_x_index, train_y_index)\n",
        "test_index = (test_x_index, test_y_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLwQkclw4kv6",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1.5. Phân chia tập train/test/validation\n",
        "\n",
        "Từ 2 tập dữ liệu `train_index` và `test_index` ban đầu ta sẽ phân chia thành 3 tập train/test và validation như sau:\n",
        "\n",
        "* Mỗi tập dữ liệu bao gồm 2 phần: input bao gồm chuỗi các itemId liên tiếp, output là itemId tiếp theo trong được khách hàng click.\n",
        "\n",
        "* Dữ liệu validation được rút ra từ dữ liệu train set theo tỷ lệ được qui định tại valid_portion. Phần còn lại của train set được dữ làm tập train set mới.\n",
        "\n",
        "* Dữ liệu test được tính toàn trực tiếp từ tập test set.\n",
        "\n",
        "* Đối với dữ liệu input, chuỗi dữ liệu sẽ được truncate về độ dài maxlen nếu nó vượt qua maxlen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cMipwjA5BAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(root='', valid_portion=0.1, maxlen=19, sort_by_len=False, train_set=None, test_set=None):\n",
        "    \"\"\"Load dataset từ root\n",
        "    root: folder dữ liệu train, trong trường hợp train_set, test_set tồn tại thì không sử dụng train_set và test_set\n",
        "    valid_portion: tỷ lệ phân chia dữ liệu validation/train\n",
        "    maxlen: độ dài lớn nhất của sequence\n",
        "    sort_by_len: có sort theo chiều dài các session trước khi chia hay không?\n",
        "    train_set: training dataset\n",
        "    test_set:  test dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load the dataset\n",
        "    if train_set is None and test_set is None:\n",
        "        path_train_data = os.path.join(root, 'train.pkl')\n",
        "        path_test_data = os.path.join(root, 'test.pkl')\n",
        "        with open(path_train_data, 'rb') as f1:\n",
        "            train_set = pickle.load(f1)\n",
        "\n",
        "        with open(path_test_data, 'rb') as f2:\n",
        "            test_set = pickle.load(f2)\n",
        "\n",
        "    if maxlen:\n",
        "        new_train_set_x = []\n",
        "        new_train_set_y = []\n",
        "        # Lọc dữ liệu sequence đến maxlen\n",
        "        for x, y in zip(train_set[0], train_set[1]):\n",
        "            if len(x) < maxlen:\n",
        "                new_train_set_x.append(x)\n",
        "                new_train_set_y.append(y)\n",
        "            else:\n",
        "                new_train_set_x.append(x[:maxlen])\n",
        "                new_train_set_y.append(y)\n",
        "        train_set = (new_train_set_x, new_train_set_y)\n",
        "        del new_train_set_x, new_train_set_y\n",
        "\n",
        "        new_test_set_x = []\n",
        "        new_test_set_y = []\n",
        "        for xx, yy in zip(test_set[0], test_set[1]):\n",
        "            if len(xx) < maxlen:\n",
        "                new_test_set_x.append(xx)\n",
        "                new_test_set_y.append(yy)\n",
        "            else:\n",
        "                new_test_set_x.append(xx[:maxlen])\n",
        "                new_test_set_y.append(yy)\n",
        "        test_set = (new_test_set_x, new_test_set_y)\n",
        "        del new_test_set_x, new_test_set_y\n",
        "\n",
        "    # phân chia tập train thành train và validation\n",
        "    train_set_x, train_set_y = train_set\n",
        "    n_samples = len(train_set_x)\n",
        "    sidx = np.arange(n_samples, dtype='int32')\n",
        "    np.random.shuffle(sidx)\n",
        "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
        "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
        "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
        "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
        "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
        "\n",
        "    (test_set_x, test_set_y) = test_set\n",
        "\n",
        "    # Trả về indices thứ tự độ dài của mỗi phần tử trong seq\n",
        "    def len_argsort(seq):\n",
        "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
        "\n",
        "    # Sắp xếp session theo độ dài tăng dần\n",
        "    if sort_by_len:\n",
        "        sorted_index = len_argsort(test_set_x)\n",
        "        test_set_x = [test_set_x[i] for i in sorted_index]\n",
        "        test_set_y = [test_set_y[i] for i in sorted_index]\n",
        "\n",
        "        sorted_index = len_argsort(valid_set_x)\n",
        "        valid_set_x = [valid_set_x[i] for i in sorted_index]\n",
        "        valid_set_y = [valid_set_y[i] for i in sorted_index]\n",
        "\n",
        "    train = (train_set_x, train_set_y)\n",
        "    valid = (valid_set_x, valid_set_y)\n",
        "    test = (test_set_x, test_set_y)\n",
        "    return train, valid, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHLdqJYs8r9B",
        "colab_type": "text"
      },
      "source": [
        "### 4.2. Data Loader\n",
        "\n",
        "Để streaming dữ liệu theo batch thì ta cần sử dụng class `DataLoader` của pytorch. Class này có chức năng tương tự như ImageDataGenerator trong tensorflow và keras. Nó sẽ cho phép ta sử dụng generator để sinh dữ liệu cho từng batch huấn luyện. Do đó ta sẽ có thể huấn luyện được những mô hình từ dữ liệu có kích thước lớn hơn RAM gấp rất nhiều lần. Data Loader trong pytorch sẽ sử dụng dữ liệu là các class `Dataset` của pytorch như bên dưới."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uatVj0Q1zjSR",
        "colab_type": "text"
      },
      "source": [
        "#### 4.2.1. RecSysDataset\n",
        "\n",
        "`Dataset` sẽ có dữ liệu hoàn toàn giống với tập train/test và validation mà ta đã phân chia ở bước trên. Chúng được tạo ra đơn thuần là để thích hợp với định dạng data được sử dụng trong quá trình khởi tạo `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTOPWkLfzmZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class RecSysDataset(Dataset):\n",
        "    \"\"\"define the pytorch Dataset class for yoochoose and diginetica datasets.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        print('-'*50)\n",
        "        print('Dataset info:')\n",
        "        print('Number of sessions: {}'.format(len(data[0])))\n",
        "        print('-'*50)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        session_items = self.data[0][index]\n",
        "        target_item = self.data[1][index]\n",
        "        return session_items, target_item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azqh1ygH2uDs",
        "colab_type": "text"
      },
      "source": [
        "#### 4.2.2. Hàm phụ trợ\n",
        "\n",
        "Ngoài ra để tùy biến các dữ liệu từ `Dataset`, ta có thể truyền vào `DataLoader` thông qua tham số `collate_fn` một hàm số có tác dụng biến đổi dữ liệu. \n",
        "\n",
        "Đối với mô hình này ta cũng sẽ sử dụng hàm `collate_fn()` như bên dưới để biến đổi data (chính là các batch) theo các bước như sau:\n",
        "\n",
        "* Sắp xếp độ dài input sequence theo độ dài list từ cao xuống thấp. Việc này sẽ giúp cho quá trình huấn luyện nhanh hơn.\n",
        "\n",
        "* Padding thêm 0 vào dữ liệu để độ dài list bằng nhau và bằng với độ dài của input sequence lớn nhất trong batch.\n",
        "\n",
        "* transpose dữ liệu từ `batch_size x maxlen --> maxlen x batch_size`\n",
        "\n",
        "* Cuối cùng trả về kết quả ngoài list các chuỗi sessions, list các labels còn trả thêm list các lens ghi nhận độ dài của các sesssions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm6Tz941201n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "    Hàm số này sẽ được sử dụng để pad session về max length\n",
        "    Args: \n",
        "      data: batch truyền vào\n",
        "    return: \n",
        "      batch data đã được pad length có shape maxlen x batch_size\n",
        "    \"\"\"\n",
        "    # Sort batch theo độ dài của input_sequence từ cao xuống thấp\n",
        "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    lens = [len(sess) for sess, label in data]\n",
        "    labels = []\n",
        "    # Padding batch size\n",
        "    padded_sesss = torch.zeros(len(data), max(lens)).long()\n",
        "    for i, (sess, label) in enumerate(data):\n",
        "        padded_sesss[i,:lens[i]] = torch.LongTensor(sess)\n",
        "        labels.append(label)\n",
        "    \n",
        "    # Transpose dữ liệu từ batch_size x maxlen --> maxlen x batch_size\n",
        "    padded_sesss = padded_sesss.transpose(0,1)\n",
        "    return padded_sesss, torch.tensor(labels).long(), lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqYCTNKIfaS0",
        "colab_type": "text"
      },
      "source": [
        "### 4.3. Metric\n",
        "\n",
        "Có 2 metrics chính để đo lường hiệu quả của mô hình recommendation đó là:\n",
        "\n",
        "* `Recall@k` : Metric của dữ liệu bao gồm tỷ lệ xuất hiện ground truth của itemId trong top $k$ sản phẩm được suggest có xác suất lớn nhất. Tỷ lệ này cho biết khả năng khách hàng sẽ click vào một sản phẩm được suggest từ mô hình với xác suất là bao nhiêu. Do đó giá trị của nó càng lớn thì mô hình sẽ có độ chuẩn xác càng cao. Nếu `recall@20 = 10%` có nghĩa là nếu áp dụng mô hình để suggest ra 20 sản phẩm cho khách hàng thì khả năng họ có click vào sản phẩm là 10%.\n",
        "\n",
        "\n",
        "* `mrr@k`: Chúng ta sẽ quan tâm trong trường hợp ground truth của itemId nằm trong top $k$ sản phẩm được suggest thì thứ tự là bao nhiêu? Nếu vị trí của nó càng nhỏ thì độ chính xác của mô hình càng cao. `mrr@k` sẽ là nghịch đảo vị trí của ground truth trong trường hợp nó nằm trong top $k$ sản phẩm được suggest. Do đó `mrr@k` càng lớn thì mô hình càng chất lượng.\n",
        "\n",
        "Để tính toán các metrics trên sẽ dựa trên ground truth và top $k$ sản phẩm được suggest từ mô hình như bên dưới: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhEsR5JnE7a9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "def get_recall(indices, targets):\n",
        "    \"\"\"\n",
        "    Tính toán chỉ số recall cho một tập hợp predictions và targets\n",
        "    Args:\n",
        "        indices (Bxk): torch.LongTensor. top-k indices được dự báo từ mô hình model.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the recall score\n",
        "    \"\"\"\n",
        "    # copy targets k lần để trở thành kích thước Bxk\n",
        "    targets = targets.view(-1, 1).expand_as(indices)\n",
        "    # so sánh targets với indices để tìm ra vị trí mà khách hàng sẽ hit.\n",
        "    hits = (targets == indices).to(device)\n",
        "    hits = hits.double()\n",
        "    if targets.size(0) == 0:\n",
        "        return 0\n",
        "    # Đếm số hit\n",
        "    n_hits = torch.sum(hits)\n",
        "    recall = n_hits / targets.size(0)\n",
        "    return recall\n",
        "\n",
        "\n",
        "def get_mrr(indices, targets):\n",
        "    \"\"\"\n",
        "    Tính toán chỉ số MRR cho một tập hợp predictions và targets\n",
        "    Args:\n",
        "        indices (Bxk): torch.LongTensor. top-k indices được dự báo từ mô hình model.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the MRR score\n",
        "    \"\"\"\n",
        "    tmp = targets.view(-1, 1)\n",
        "    targets = tmp.expand_as(indices)\n",
        "    hits = (targets == indices).to(device)\n",
        "    hits = hits.double()\n",
        "    if hits.sum() == 0:\n",
        "      return 0\n",
        "    argsort = []\n",
        "    for i in np.arange(hits.shape[0]):\n",
        "      index_col = torch.where(hits[i, :] == 1)[0]+1\n",
        "      if index_col.shape[0] != 0:\n",
        "        argsort.append(index_col.double())\n",
        "    inv_argsort = [1/item for item in argsort]\n",
        "    mrr = sum(inv_argsort)/hits.shape[0]\n",
        "    return mrr\n",
        "\n",
        "\n",
        "def evaluate(logits, targets, k=20):\n",
        "    \"\"\"\n",
        "    Đánh giá model sử dụng Recall@K, MRR@K scores.\n",
        "    Args:\n",
        "        logits (B,C): torch.LongTensor. giá trị predicted logit cho itemId tiếp theo.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the recall score\n",
        "        mrr (float): the mrr score\n",
        "    \"\"\"\n",
        "    # Tìm ra indices của topk lớn nhất các giá trị dự báo.\n",
        "    _, indices = torch.topk(logits, k, -1)\n",
        "    recall = get_recall(indices, targets)\n",
        "    mrr = get_mrr(indices, targets)\n",
        "    return recall, mrr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyJH5gLKiGc1",
        "colab_type": "text"
      },
      "source": [
        "Hàm `evaluate()` sẽ tính toán đồng thời cả 2 chỉ số là `recall@k` và `mrr@k`.  Tham số của hàm `evaluate()` bao gồm:\n",
        "\n",
        "* logits: Kích thước `BxC` trong đó `B` là `batch_size` và `C` là số `class`.  Mỗi dòng của logits là phân phối xác suất dự báo cho itemId tiếp theo.\n",
        "\n",
        "* targets: Kích thước `B` trong đó `B` là `batch_size`. Đây là index của itemId mà khách hàng đã click và chính là ground truth của mô hình.\n",
        "\n",
        "Kiểm tra hàm `evaluate()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPrBf_IkXtcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "logits = torch.tensor([[0.1, 0.2, 0.7],\n",
        "                       [0.4, 0.1, 0.5],\n",
        "                       [0.1, 0.2, 0.7]]).to(device)\n",
        "\n",
        "targets = torch.tensor([1, 2, 2]).to(device)\n",
        "\n",
        "evaluate(logits = logits, targets = targets, k = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoKmRZbefbvT",
        "colab_type": "text"
      },
      "source": [
        "### 4.4. Model NARM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z150lOyC2w89",
        "colab_type": "text"
      },
      "source": [
        "#### 4.4.1. Các layer của NARM\n",
        "\n",
        "**Embedding Layer:** Là layer nhúng giúp mã hóa các item index thành những véc tơ với chiều xác định bằng `embedding_dim`.\n",
        "Embedding Layer sẽ nhận đầu vào là một sequence của các item index. Layer này sẽ thực hiện 2 biến đổi chính:\n",
        "\n",
        "* Đầu tiên mỗi một item index sẽ được mã hóa về véc tơ one-hot (là véc tơ đơn vị có giá trị tại vị trí index = 1 và các vị trí khác bằng 0). Như vậy kích thước của véc tơ one-hot sẽ bằng kích thước của từ điển. Lấy ví dụ: Nếu từ điển có 10000 từ với index nhận giá trị từ 1-10000, khi đó một từ có index = 2 sẽ được mã hóa thành véc tơ [0, 1, 0, ..., 0]. Tức vị trí số 2 của véc tơ bằng 1 và các vị trí còn lại bằng 0.\n",
        "\n",
        "* Sau khi mã hóa xong toàn bộ các từ trong câu thì input của chuỗi item index sẽ trở thành ma trận có kích thước là `max_len x vocabulary_size`. Khi đó để giảm chiều của dữ liệu ta sẽ sử dụng một phép chiếu linear projection để giảm chiều từ `vocabulary_size` về `embedding_dim`. Như vậy sau cùng thu được ma trận kích thước `max_len x embedding_dim`. Lưu ý: ta giả định là không quan tâm đến `batch_size` nên kích thước ma trận ở trên chưa bao gồm `batch_size`.\n",
        "\n",
        "**Dropout:** Layer này có tác dụng tắt đi ngẫu nhiên một số lượng kết nối liên kết giữa 2 layers để giảm thiểu overfitting cho mô hình.\n",
        "\n",
        "**GRU:** Layer GRU là trọng tâm của mô hình, thực hiện quá trình dự báo chuỗi. Mô hình bao gồm nhiều time step và mỗi một time step sẽ trả ra phân phối xác suất đại diện cho từ ở vị trí đó. Kết quả trả ra của layer GRU goòm 2 thành phần: `(output, hidden)`.\n",
        "\n",
        "* Trong đó `output` sẽ chứa toàn bộ các hidden véc tơ tại toàn bộ các time step $t$ tại layer GRU cuối cùng (chúng ta có thể chồng nhiều layers GRU nối tiếp nhau thông qua khai báo ở tham số `num_layers` và chúng đều đưa ra cùng một kết quả cuối cùng mà không phụ thuộc vào `num_layers`, kết quả trả ra sẽ được tính từ layer cuối). `output` sẽ có kích thước là `max_len x batch_size x (n_directions*hidden_size)`. Sở dĩ có thêm `n_direction` là vì mô hình được thực hiện theo một chiều từ trái qua phải hoặc 2 chiều từ trái qua phải và từ phải qua trái. Trường hợp 2 chiều thì tại mỗi time step sẽ có 2 hidden véc tơ đại diện cho 2 chiều.\n",
        "\n",
        "* Tham số thứ 2 trả ra từ layer GRU là `hidden` chính là list các véc tơ hidden tại time step cuối cùng của toàn bộ các layer GRU. Như vậy `hidden` sẽ có kích thước là `(num_layers*n_directions) x batch_size x hidden_size`. Để thu được hidden state của layer GRU cuối cùng thì ta sẽ trích suất véc tơ cuối cùng (chính là công thức `ht = hidden[-1]` bên dưới).\n",
        "\n",
        "##### Quá trình tính attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcAZf6B8-4Xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class NARM(nn.Module):\n",
        "    def __init__(self, hidden_size, n_items, embedding_dim, n_layers=1, dropout=0.25):\n",
        "        super(NARM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_items = n_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(self.n_items, self.embedding_dim, padding_idx = 0)\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        # set bidirectional = True for bidirectional\n",
        "        # https://pytorch.org/docs/stable/nn.html?highlight=gru#torch.nn.GRU to get more information\n",
        "        self.gru = nn.GRU(input_size = hidden_size, # number of expected feature of input x \n",
        "                          hidden_size = hidden_size, # number of expected feature of hidden state \n",
        "                          num_layers = n_layers, # number of GRU layers\n",
        "                          dropout=(0 if n_layers == 1 else dropout), # dropout probability apply in encoder network\n",
        "                          bidirectional=True # one or two directions.\n",
        "                         )\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.n_layers)\n",
        "        self.a_1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.a_2 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.v_t = nn.Linear(self.hidden_size, 1, bias=False)\n",
        "        self.ct_dropout = nn.Dropout(0.5)\n",
        "        self.b = nn.Linear(self.embedding_dim, 2 * self.hidden_size, bias=False)\n",
        "        self.sf = nn.Softmax()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        \"\"\"\n",
        "        input_seq: Batch input_sequence. Shape: max_len x batch_size \n",
        "        input_lengths: Batch input lengths. Shape: batch_size\n",
        "        \"\"\"\n",
        "        # Step 1: Convert sequence indexes to embeddings\n",
        "        # shape: (max_length , batch_size , hidden_size)\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module. Padding zero when length less than max_length of input_lengths.\n",
        "        # shape: (max_length , batch_size , hidden_size)\n",
        "        packed = pack_padded_sequence(embedded, input_lengths)\n",
        "\n",
        "        # Step 2: Forward packed through GRU\n",
        "        # outputs is output of final GRU layer\n",
        "        # hidden is concatenate of all hidden states corresponding with each time step.\n",
        "        # outputs shape: (max_length , batch_size , hidden_size x num_directions)\n",
        "        # hidden shape: (n_layers x num_directions , batch_size , hidden_size)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding. Revert of pack_padded_sequence\n",
        "        # outputs shape: (max_length , batch_size , hidden_size x num_directions)\n",
        "        outputs, length = pad_packed_sequence(outputs)\n",
        "\n",
        "        # Step 3: Global Encoder & Local Encoder\n",
        "        # num_directions = 1 -->\n",
        "        # outputs shape:(max_length , batch_size , hidden_size)\n",
        "        # hidden shape: (n_layers , batch_size , hidden_size)\n",
        "        # lấy hidden state tại time step cuối cùng\n",
        "        ht = hidden[-1]\n",
        "        # reshape outputs\n",
        "        outputs = outputs.permute(1, 0, 2) # [batch_size, max_length, hidden_size]     \n",
        "        c_global = ht\n",
        "        # Flatten outputs thành shape: [batch_size, max_length, hidden_size]\n",
        "        gru_output_flatten = outputs.contiguous().view(-1, self.hidden_size)\n",
        "        # Thực hiện một phép chiếu linear projection để tạo các latent variable có shape [batch_size, max_length, hidden_size]\n",
        "        q1 = self.a_1(gru_output_flatten).view(outputs.size()) \n",
        "        # Thực hiện một phép chiếu linear projection để tạo các latent variable có shape [batch_size, max_length, hidden_size]\n",
        "        q2 = self.a_2(ht)\n",
        "        # Ma trận mask đánh dấu vị trí khác 0 trên padding sequence.\n",
        "        mask = torch.where(input_seq.permute(1, 0) > 0, torch.tensor([1.], device = self.device), torch.tensor([0.], device = self.device)) # batch_size x max_len\n",
        "        # Điều chỉnh shape\n",
        "        q2_expand = q2.unsqueeze(1).expand_as(q1) # shape [batch_size, max_len, hidden_size]\n",
        "        q2_masked = mask.unsqueeze(2).expand_as(q1) * q2_expand # batch_size x max_len x hidden_size\n",
        "        # Tính trọng số alpha đo lường similarity giữa các hidden state \n",
        "        alpha = self.v_t(torch.sigmoid(q1 + q2_masked).view(-1, self.hidden_size)).view(mask.size()) # batch_size x max_len\n",
        "        alpha_exp = alpha.unsqueeze(2).expand_as(outputs) # batch_size x max_len x hidden_size\n",
        "        # Tính linear combinition của các hidden state\n",
        "        c_local = torch.sum(alpha_exp * outputs, 1) # (batch_size x hidden_size)\n",
        "\n",
        "        # Véc tơ combinition tổng hợp\n",
        "        c_t = torch.cat([c_local, c_global], 1) # batch_size x (2*hidden_size) \n",
        "        c_t = self.ct_dropout(c_t)\n",
        "        # Tính scores\n",
        "\n",
        "        # Step 4: Decoder\n",
        "        # embedding cho toàn bộ các item\n",
        "        item_indices = torch.arange(self.n_items).to(device) # 1 x n_items\n",
        "        item_embs = self.embedding(item_indices) # n_items x embedding_dim\n",
        "        # reduce dimension by bi-linear projection\n",
        "        B = self.b(item_embs).permute(1, 0) # (2*hidden_size) x n_items\n",
        "        scores = torch.matmul(c_t, B) # batch_size x n_items\n",
        "        # scores = self.sf(scores)\n",
        "        return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7d_1K_hFEF6",
        "colab_type": "text"
      },
      "source": [
        "#### 4.4.2. Kiểm tra model NARM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaONEMITGlWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thử nghiệm model bằng cách giả lập 1 input và thực hiện quá trình feed forward\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "hidden_size = 3\n",
        "n_layers = 7\n",
        "# embedding = nn.Embedding(11000, hidden_size)\n",
        "input_variable = torch.tensor([[  66,  369,   66, 1272],\n",
        "                                [ 567,  183,   28,  616],\n",
        "                                [ 392, 1558, 1143,  175],\n",
        "                                [ 394,   31,   31, 5558],\n",
        "                                [   0,    0,    0,    0]]).to(device)\n",
        "\n",
        "lengths =  torch.tensor([5, 5, 5, 5]).to(device)\n",
        "print('input_seq: \\n', input_variable)\n",
        "print('input_lengths: \\n', lengths)\n",
        "model_test = NARM(hidden_size = hidden_size, n_items  = 100000, embedding_dim = 100, n_layers=1, dropout=0.25).to(device)\n",
        "print('model phrase: \\n', model_test)\n",
        "scores = model_test.forward(input_seq = input_variable, input_lengths = lengths)\n",
        "print('probability distribution: ', scores.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRU759N4XEaB",
        "colab_type": "text"
      },
      "source": [
        "### 4.5. Validation\n",
        "\n",
        "Hàm validation sẽ sử dụng để tính toán `recall@k` và `mrr@k` trên tập dữ liệu validation.\n",
        "\n",
        "Đầu tiên ta sẽ sử dụng `model` để dự báo xác suất output cho từng batch. `valid_loader` là một iterator được sử dụng để khởi tạo batch. Sử dụng vòng lặp để để duyệt qua toàn bộ batch của `valid_loader` và lưu các giá trị `recall@k` và `mrr@k` tính được ở mỗi batch vào list. Cuối cùng lấy trung bình của toàn bộ các chỉ số này để thu được `recall@k` và `mrr@k` đại diện trên tập validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFB2WhYwXG8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(valid_loader, model):\n",
        "    model.eval()\n",
        "    recalls = []\n",
        "    mrrs = []\n",
        "    with torch.no_grad():\n",
        "        for seq, target, lens in valid_loader:\n",
        "            seq = seq.to(device)\n",
        "            target = target.to(device)\n",
        "            outputs = model(seq, lens)\n",
        "            logits = F.softmax(outputs, dim = 1)\n",
        "            recall, mrr = evaluate(logits, target, k = args['topk'])\n",
        "            recalls.append(recall)\n",
        "            mrrs.append(mrr)\n",
        "    \n",
        "    mean_recall = torch.mean(torch.stack(recalls))\n",
        "    mean_mrr = torch.mean(torch.stack(mrrs))\n",
        "    return mean_recall, mean_mrr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIBxlzdVfhc4",
        "colab_type": "text"
      },
      "source": [
        "### 4.6. Training model\n",
        "\n",
        "\n",
        "Hàm `main()` giúp huấn luyện mô hình trên toàn bộ epochs và trả về kết quả là mô hình sau huấn luyện. Bên cạnh đó, mô hình sau huấn luyện sẽ được lưu vào file `latest_checkpoint.pth.tar` để có thể tái sử dụng về sau.\n",
        "\n",
        "Hàm `trainForEpoch()` thực hiện huấn luyện mô hình trên mỗi một epoch. Dựa vào Data Loader, chúng ta có thể dễ dàng duyệt qua toàn bộ dataset và training trên từng batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pipyd4uGUM62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn\n",
        "\n",
        "args = {\n",
        "    'dataset_path':'../input/yoochoose/yoochoose-clicks.dat',\n",
        "    'batch_size': 256,\n",
        "    'hidden_size': 100,\n",
        "    'embed_dim': 50,\n",
        "    'epoch': 2,\n",
        "    'lr':0.001,\n",
        "    'lr_dc':0.1,\n",
        "    'lr_dc_step':80,\n",
        "    'test':None,\n",
        "    'topk':20,\n",
        "    'valid_portion':0.1\n",
        "}\n",
        "\n",
        "here = os.path.dirname(os.getcwd())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def main():\n",
        "    print('Loading data...')\n",
        "    train_data, valid_data, test_data = load_data(train_set=train_index, test_set=test_index)\n",
        "    train_data = RecSysDataset(train_data)\n",
        "    valid_data = RecSysDataset(valid_data)\n",
        "    test_data = RecSysDataset(test_data)\n",
        "    train_loader = DataLoader(train_data, batch_size = args['batch_size'], shuffle = True, collate_fn = collate_fn)\n",
        "    valid_loader = DataLoader(valid_data, batch_size = args['batch_size'], shuffle = False, collate_fn = collate_fn)\n",
        "    test_loader = DataLoader(test_data, batch_size = args['batch_size'], shuffle = False, collate_fn = collate_fn)\n",
        "    print('Complete load data!')\n",
        "    n_items = voc.num_items\n",
        "    model = NARM(hidden_size = args['hidden_size'], n_items = n_items, embedding_dim = args['embed_dim'], n_layers=2, dropout=0.25).to(device)\n",
        "    print('complete load model!')\n",
        "\n",
        "    if args['test'] == 'store_true':\n",
        "        ckpt = torch.load('latest_checkpoint.pth.tar')\n",
        "        model.load_state_dict(ckpt['state_dict'])\n",
        "        recall, mrr = validate(test_loader, model)\n",
        "        print(\"Test: Recall@{}: {:.4f}, MRR@{}: {:.4f}\".format(args['topk'], recall, args['topk'], mrr))\n",
        "        return model\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), args['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = StepLR(optimizer, step_size = args['lr_dc_step'], gamma = args['lr_dc'])\n",
        "\n",
        "    print('start training!')\n",
        "    for epoch in tqdm(range(args['epoch'])):\n",
        "        # train for one epoch\n",
        "        trainForEpoch(train_loader, model, optimizer, epoch, args['epoch'], criterion, log_aggr = 1000)\n",
        "        scheduler.step(epoch = epoch)\n",
        "        recall, mrr = validate(valid_loader, model)\n",
        "        print('Epoch {} validation: Recall@{}: {:.4f}, MRR@{}: {:.4f} \\n'.format(epoch, args['topk'], recall, args['topk'], mrr))\n",
        "\n",
        "        # store best loss and save a model checkpoint\n",
        "        ckpt_dict = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(ckpt_dict, 'latest_checkpoint.pth.tar')\n",
        "    return model\n",
        "\n",
        "\n",
        "def trainForEpoch(train_loader, model, optimizer, epoch, num_epochs, criterion, log_aggr=1000):\n",
        "    model.train()\n",
        "\n",
        "    sum_epoch_loss = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for i, (seq, target, lens) in enumerate(train_loader):\n",
        "        seq = seq.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(seq, lens)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "\n",
        "        loss_val = loss.item()\n",
        "        sum_epoch_loss += loss_val\n",
        "\n",
        "        iter_num = epoch * len(train_loader) + i + 1\n",
        "\n",
        "        if i % log_aggr == 0:\n",
        "            print('[TRAIN] epoch %d/%d  observation %d/%d batch loss: %.4f (avg %.4f) (%.2f im/s)'\n",
        "                % (epoch + 1, num_epochs, i, len(train_loader), loss_val, sum_epoch_loss / (i + 1),\n",
        "                  len(seq) / (time.time() - start)))\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "model = main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1a9m71mFOJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.listdir()\n",
        "# chứa 'latest_checkpoint.pth.tar'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvJseklXsmvB",
        "colab_type": "text"
      },
      "source": [
        "Để nhanh gọn thì tôi chỉ training model trên 2 epoch có sử dụng GPU, thời gian huấn luyện hết tổng cộng 15 phút. Các bạn có thể điều chỉnh tăng epochs để thu được kết quả tốt hơn.\n",
        "\n",
        "Chỉ với 2 epochs nhưng kết quả của `Recall@20` là 43.03%, chỉ số này chứng tỏ khoảng 43.03% khả năng một khi mô hình khuyến nghị ra 20 kết quả top đầu thì khách hàng sẽ có click vào một trong các sản phẩm đó. Tuy nhiên điều này là lý tưởng dựa trên thực tế item mà khách hàng click xuất hiện trong top 20 được dự báo là 43.03%. Khi áp dụng vào recommendation kết quả có thể thấp hơn nhiều do việc click còn phụ thuộc vào vị trí của item có khả năng click có nằm ở top đầu hay không. \n",
        "\n",
        "Bên cạnh đó `MRR@20` tăng dần qua từng epochs cho thấy vị trí của itemId được dự báo ngày càng sát với top đầu.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYQyUkqHr3sL",
        "colab_type": "text"
      },
      "source": [
        "### 4.7. Dự báo\n",
        "\n",
        "Trước khi thực hiện dự báo thì ta sẽ cần phải load mô hình từ `PATH` đã lưu trước đó. Có 2 kiểu save mô hình chính trên pytorch đó là save toàn bộ mô hình và save các checkpoints. Nếu save toàn bộ mô hình sẽ buộc phải xác định các classes và cấu trúc thư mục lưu trữ mô hình nên thường dẫn tới xảy ra lỗi khi thay đổi project. Do đó save theo checkpoint thường được khuyến nghị, đối với bài toán này chúng ta cũng save mô hình theo checkpoint. Để load lại mô hình cũ ta thực hiện như sau:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbPVLPgwMjyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "PATH = 'latest_checkpoint.pth.tar'\n",
        "model = NARM(hidden_size = args['hidden_size'], n_items = 37682, embedding_dim = args['embed_dim'], n_layers=2, dropout=0.25).to(device)\n",
        "optimizer = optim.Adam(params = model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "epoch = checkpoint['epoch']\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6RQMNugRJvy",
        "colab_type": "text"
      },
      "source": [
        "Lưu ý: Sau khi load xong model ta luôn phải chạy hàm `model.eval()` để kích hoạt các dropout và batchnormalization layer. Nếu không kết quả dự báo có thể dẫn tới sai lệch. Tiếp theo chúng ta sẽ sử dụng mô hình để dự báo cho một trường hợp cụ thể."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNNjxtTqvJgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lựa chọn ngẫu nhiên một session trên test\n",
        "import numpy as np\n",
        "i = np.random.randint(0, len(test_index[0]))\n",
        "x = [test_index[0][i]]\n",
        "y = [test_index[1][i]]\n",
        "print('item indexes sequence input: ', x)\n",
        "print('item index next output: ', y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCXDtmf4fIfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1: Khởi tạo test_loader để biến đổi dữ liệu session đưa vào mô hình\n",
        "test_data = RecSysDataset([x, y])\n",
        "test_loader = DataLoader(test_data, batch_size = args['batch_size'], shuffle = False, collate_fn = collate_fn)\n",
        "\n",
        "# Step 2: Dự báo các indice tiếp theo mà khách hàng có khả năng click\n",
        "def _preddict(loader, model):\n",
        "    model.eval()\n",
        "    recalls = []\n",
        "    mrrs = []\n",
        "    j = 1\n",
        "    with torch.no_grad():\n",
        "      for seq, target, lens in loader:\n",
        "        seq = seq.to(device)\n",
        "        target = target.to(device)\n",
        "        outputs = model(seq, lens)\n",
        "        logits = F.softmax(outputs, dim = 1)\n",
        "        _, indices = torch.topk(logits, 20, -1)\n",
        "        print('Is next clicked item in top 20 suggestions: ', (target in indices))\n",
        "        print('Top 20 next item indices suggested: ')\n",
        "    return indices\n",
        "\n",
        "_preddict(test_loader, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoWLfEEeLnMQ",
        "colab_type": "text"
      },
      "source": [
        "Kết quả cho thấy mô hình dự báo là chính xác. Index của item mà thực tế khách hàng dự báo nằm trong list các sản phẩm được suggest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8aMs81lMIT-",
        "colab_type": "text"
      },
      "source": [
        "## 5. Tổng kết"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-af4u-oKcTj",
        "colab_type": "text"
      },
      "source": [
        "Như vậy ta đã hoàn thành quá trình huấn luyện và dự báo mô hình recommendation theo phương pháp `Neural Attentive Session-based Recommendation`. Đây là một mô hình thuộc lớp các mô hình deep neural network có hiệu quả dự báo tốt hơn so với một số phương pháp state-of-art khác như `Session-Based Recommendations With Reccurent Neural Networks`.\n",
        "\n",
        "Đồng thời qua bài viết tôi cũng đã giới thiệu đến các bạn phương pháp phân loại mô hình recommendation bao gồm: lớp thuật toán phi mô hình và lớp thuật toán mô hình kèm theo các thuật toán phổ biến trong mỗi lớp. Ưu nhược điểm của mỗi thuật toán và trường hợp nên áp dụng.\n",
        "\n",
        "Hi vọng rằng chúng ta có thể tự xây dựng được một mô hình recommendation cho doanh nghiệp của mình.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4m7QV4RVWLL",
        "colab_type": "text"
      },
      "source": [
        "## 6. Tài liệu\n",
        "\n",
        "1. [Neural Attentive Session-based Recommendation - Jing Li, Pengjie Ren, el. Shangdong University, China](https://arxiv.org/pdf/1711.04725.pdf)\n",
        "\n",
        "2. [A Survey on Session-based Recommender Systems - ShouJing Wang, LongBing Cao, Yan Wang, University of Technology Sydney, Australia](https://arxiv.org/pdf/1902.04864.pdf)\n",
        "\n",
        "3. [Session-Based Recommendations With Reccurent Neural Networks - Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk, Hungary](https://arxiv.org/pdf/1511.06939v4.pdf)\n",
        "\n",
        "4. [Neural Attentive Session Based Recommendation PyTorch - Wang Shuo, github](https://github.com/Wang-Shuo/Neural-Attentive-Session-Based-Recommendation-PyTorch)\n",
        "\n",
        "5. [Neural Attentive Recommendation Machine - lijingsdu, github](https://github.com/lijingsdu/sessionRec_NARM)\n",
        "\n",
        "6. [Bài 20 - Recommendation Neural Network - khanh's blog](https://phamdinhkhanh.github.io/2019/12/26/Sorfmax_Recommendation_Neural_Network.html)\n",
        "\n",
        "7. [Bài 15 - collaborative và content-based filtering, khanh's blog](https://phamdinhkhanh.github.io/2019/11/04/Recommendation_Compound_Part1.html)\n",
        "\n",
        "8. [Bài 3 - Mô hình Word2Vec, khanh's blog](https://phamdinhkhanh.github.io/2019/04/29/ModelWord2Vec.html)"
      ]
    }
  ]
}