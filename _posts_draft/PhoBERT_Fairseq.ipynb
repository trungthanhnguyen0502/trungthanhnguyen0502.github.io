{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhoBERT_Fairseq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZHZYD-sJj4c",
        "colab_type": "text"
      },
      "source": [
        "## 1. Lý do tại sao mình viết về BERT\n",
        "\n",
        "Ở bài trước chúng ta đã tìm hiểu về model BERT. Kể từ khi model BERT được launching, các giới hạn trong NLP dường như được phá vỡ. Việc học chuyển giao trở nên khả thi hơn, các tác vụ down stream task lần lượt được cải thiện.\n",
        "\n",
        "Đối với Tiếng Việt thì [PhoBERT](https://github.com/VinAIResearch/PhoBERT) có thể coi là một trong những project đầu tiên của BERT dành cho Tiếng Việt được public. Cá nhân mình sử dụng PhoBERT thì thấy đây là một pretrained model với độ chính xác rất tốt. Bạn đọc cũng có thể tự cảm nhận qua các phần thực hành ở bài hướng dẫn này. \n",
        "\n",
        "Mặc dù model BERT có rất nhiều các ứng dụng có thể fine tunning cho nhưng không thực sự nhiều bạn biết cách thực hiện. Gần đây mình cũng nhận được một vài Inbox hỏi về cách áp dụng BERT như thế nào. Đó chính là động lực để mình viết bài viết này nhằm mục đích tổng kết lại các ứng dụng của model BERT cho mọi người.\n",
        "\n",
        "Trước khi tìm hiểu bài này mình khuyến nghị các bạn nên đọc qua [Bài 36 - BERT model](https://phamdinhkhanh.github.io/2020/05/23/BERTModel.html) để hiểu về model BERT là gì và nguyên lý hoạt động của model BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eF7gcd-ESrx",
        "colab_type": "text"
      },
      "source": [
        "## 2. Kiến trúc RoBERTa\n",
        "\n",
        "RoBERTa là một project của facebook implement lại model BERT trên pytorch. Đây là một project support khá tốt việc huấn luyện lại trên những bộ dữ liệu mới cho các nguôn ngữ khác ngoài các ngôn ngữ phổ biến như Tiếng Anh, Tiếng Pháp,....\n",
        "\n",
        "RoBERTa lặp lại các thủ tục huấn luyện từ model BERT, nhưng có sự thay đổi đó là huấn luyện mô hình lâu hơn, với batch size lớn hơn và trên nhiều dữ liệu hơn. Ngoài ra để nâng cao độ chuẩn xác trong biểu diễn từ thì RoBERTa đã loại bỏ tác vụ dự đoán câu tiếp theo và huấn luyện trên các câu dài hơn. Đồng thời mô hình cũng thay đổi linh hoạt kiểu masking (tức ẩn đi một số từ ở câu output bằng token `<mask>`) áp dụng cho dữ liệu huấn luyện.\n",
        "\n",
        "Bạn đọc có thể tìm hiểu thêm về kiến trúc này qua bài báo về [RoBERTa](https://arxiv.org/abs/1907.11692).\n",
        "\n",
        "Ở các mục tiếp theo mình sẽ hướng dẫn các bạn khai thác các ứng dụng của model RoBERTa thông qua pretrain model PhoBERT cho Tiếng Việt.\n",
        "\n",
        "Để bắt đầu bài thực hành, bạn đọc có thể mở file [PhoBERT - tutorial Khanh Blog](https://colab.research.google.com/drive/16a4XFPioXYzQwyTusmzi1IiGP8kCHT9t?usp=sharing) và bắt đầu từ đây."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ0C1GuFK0Sr",
        "colab_type": "text"
      },
      "source": [
        "## 3. Load model BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Z-BizMMCjt",
        "colab_type": "code",
        "outputId": "597d2f72-423d-4d1e-dc8f-d68269d9b5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "path = \"/content/gdrive/My Drive/Colab Notebooks/BERT\"\n",
        "os.chdir(path)\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            " BERTModelF.ipynb\t\t      PhoBERT_large_fairseq\n",
            " BERTModel.ipynb\t\t      PhoBERT_large_fairseq.tar.gz\n",
            " bert_self_train\t\t      PhoBERT_large_transformers\n",
            " BERTweet_base_fairseq\t\t      PhoBERT_large_transformers.tar.gz\n",
            " BERTweet_base_fairseq.tar.gz\t      PhoBERT_large_transformers.tar.gz.1\n",
            " checkpoints\t\t\t      PhoBERTMaskLM.py\n",
            "'Copy of transformer (1).ipynb'       PhoBERT_pretrain\n",
            "'Copy of transformer.ipynb'\t      PhoBert-Sentiment-Classification\n",
            " detr_demo.ipynb\t\t      PhoBERT-Sentiment-Classification.ipynb\n",
            " fairseq\t\t\t      PositionEncoding.ipynb\n",
            " gpt2_bpe\t\t\t      RoBERTa_ScratchTrain.ipynb\n",
            " HowToTrainBERT.ipynb\t\t      SA_demo.zip\n",
            " label_test.pkl\t\t\t      Test_Full\n",
            " label_train.pkl\t\t      text_test.pkl\n",
            " latest_checkpoint.pth.tar\t      text_train.pkl\n",
            " model_ckpt\t\t\t      themdau_tv\n",
            " model_ckpt2\t\t\t      Train_Full\n",
            " output\t\t\t\t      TransformerModelForCorrectDiacritic.ipynb\n",
            " PhoBERT_base_fairseq\t\t      viwiki-20200501-pages-articles.xml\n",
            " PhoBERT_base_fairseq.tar.gz\t      viwiki-20200501-pages-articles.xml.bz2\n",
            " PhoBERT_base_transformers\t      vncorenlp\n",
            " PhoBERT_base_transformers.tar.gz     VNTC\n",
            " PhoBERT_base_transformers.tar.gz.1   wikiextractor\n",
            " PhoBERTContextualEmbedding.py\t      wikitext-103-raw\n",
            " PhoBERT.ipynb\t\t\t      wikitext-103-raw-v1.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhOPH8vdPnp_",
        "colab_type": "text"
      },
      "source": [
        "Để load model BERT chúng ta sẽ cần cài đặt các packages sau đây:\n",
        "\n",
        "* [fairseq](https://github.com/pytorch/fairseq): Là project của facebook chuyên hỗ trợ các nghiên cứu và dự án liên quan đến model seq2seq.\n",
        "\n",
        "* fastBPE: Là package hỗ trợ tokenize từ (word) thành các từ phụ (subwords) theo phương pháp mới nhất được áp dụng cho các pretrain model NLP hiện đại như BERT và các biến thể của BERT.\n",
        "\n",
        "* vncorenlp: Là một package NLP trong Tiếng Việt, hỗ trợ tokenize và các tác vụ NLP khác.\n",
        "\n",
        "* [transformers](https://github.com/huggingface/transformers): Là một project của huggingface hỗ trợ huấn luyện các model dựa trên kiến trúc transformer như BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5, CTRL,... phục vụ cho các tác vụ NLP trên cả nền tảng pytorch và tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umPZeOcCPnN_",
        "colab_type": "code",
        "outputId": "5b5bc41d-1ad5-4d61-8ce3-104e134ce6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install fairseq\n",
        "!pip3 install fastbpe\n",
        "!pip3 install vncorenlp\n",
        "!pip3 install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\r\u001b[K     |█                               | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 92kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 102kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 122kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 163kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 174kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 184kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 194kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 204kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 215kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 225kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 235kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 245kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 256kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 266kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 286kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 296kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.19)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/4b/6c7a0b26a48d88f56573d11aa5058808fe0d36ba40951287894f943556b5/sacrebleu-1.4.10-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2021169 sha256=380f55feb90d12851376acec334fbfd1c46bf9465e07ac712083f71b44edd4cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq\n",
            "Successfully installed fairseq-0.9.0 portalocker-1.7.0 sacrebleu-1.4.10\n",
            "Collecting fastbpe\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/37/f97181428a5d151501b90b2cebedf97c81b034ace753606a3cda5ad4e6e2/fastBPE-0.1.0.tar.gz\n",
            "Building wheels for collected packages: fastbpe\n",
            "  Building wheel for fastbpe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastbpe: filename=fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl size=481515 sha256=312210927b6e89cbf77c6fa8f3db953f2d3be19901e7e9608af83369bcc7597a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/0c/9c/fc62058b4d473a5602bcd3d3edfece796f123875379ea82d79\n",
            "Successfully built fastbpe\n",
            "Installing collected packages: fastbpe\n",
            "Successfully installed fastbpe-0.1.0\n",
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2.9)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp36-none-any.whl size=2645935 sha256=215e990829847672c17bec8721148970d0a02f52c7aa32c7a23f3397327400a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 54.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3f381771be1b39ca0433f2b9a4c4e321625f4dad0057cee4b4fa325b2cbe185d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGp5wTVXMiVz",
        "colab_type": "text"
      },
      "source": [
        "Tiếp theo chúng ta sẽ cần download các model pretrain từ [PhoBERT](https://github.com/VinAIResearch/PhoBERT).\n",
        "\n",
        "Trong hướng dẫn này mình chỉ sử dụng pretrain model BERT base được huấn luyện từ package fairseq."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7aEkxVcMYGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
        "!tar -xzvf PhoBERT_base_fairseq.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nf-SZWDObwt",
        "colab_type": "text"
      },
      "source": [
        "Sau khi download và giải nén pretrain file chúng ta sẽ kiểm tra thấy bên trong folder sẽ bao gồm 3 files đó là `bpe.codes, dict.txt, model.pt` có tác dụng như sau:\n",
        "\n",
        "* bpe.codes: Là BPE token mà mô hình đã áp dụng để mã hóa văn bản sang index.\n",
        "\n",
        "* dict.txt: Từ điển của bộ dữ liệu huấn luyện.\n",
        "\n",
        "* model.pt: File lưu trữ của mô hình trên pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGbj8MfePU-R",
        "colab_type": "code",
        "outputId": "0cd89822-2ee6-4d4e-d952-d9e077410995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls PhoBERT_base_fairseq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes  dict.txt  model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QOK4kBdIf7p",
        "colab_type": "text"
      },
      "source": [
        "**Load model pretrain PhoBERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbnfKBOHL2Sk",
        "colab_type": "code",
        "outputId": "d799fa35-aea1-41cd-daab-3ab8ac55b51c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "phoBERT = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "phoBERT.eval()  # disable dropout (or leave in train mode to finetune"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading archive file PhoBERT_base_fairseq\n",
            "| dictionary: 64000 types\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaHubInterface(\n",
              "  (model): RobertaModel(\n",
              "    (decoder): RobertaEncoder(\n",
              "      (sentence_encoder): TransformerSentenceEncoder(\n",
              "        (embed_tokens): Embedding(64001, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(258, 768, padding_idx=1)\n",
              "        (layers): ModuleList(\n",
              "          (0): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (6): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (7): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (8): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (9): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (10): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (11): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): RobertaLMHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (classification_heads): ModuleDict()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsZyUwjuQhds",
        "colab_type": "text"
      },
      "source": [
        "Ta có thể thấy kiến trúc RoBERTa đã giữa lại 12 block sub-layers là các multi-head attention ở phase Encoder và thêm một linear projection layer ở cuối để tạo ra một embedding cho từ hiện tại."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7OmxhDlK4fT",
        "colab_type": "text"
      },
      "source": [
        "## 4. Tìm hiểu về mã hóa BPE (Byte Pair Encoding)\n",
        "\n",
        "Toknenize là quá trình mã hóa các văn bản thành các index dạng số mang thông tin của văn bản để máy tính có thể huấn luyện được. Khi đó mỗi một từ hoặc ký tự sẽ được đại diện bởi một index. \n",
        "\n",
        "Trong NLP có một số kiểu tokenize như sau:\n",
        "\n",
        "**Tokenize theo word level**: Chúng ta phân tách câu thành các token được ngăn cách bởi khoảng trắng hoặc dấu câu. Khi đó mỗi token là một từ đơn âm tiết. Đây là phương pháp token được sử dụng trong các thuật toán nhúng từ truyền thống như GloVe, word2vec.\n",
        "\n",
        "**Tokenize theo multi-word level**: Tiếng Việt và một số ngôn ngữ khác tồn tại từ đơn âm tiết (từ đơn) và từ đa âm tiết (từ ghép). Do đó nếu token theo từ đơn âm tiết sẽ làm nghĩa của từ bị sai khác. Ví dụ cụm từ `vô xác định` nếu được chia thành `vô`, `xác` và `định` sẽ làm cho từ bị mất đi nghĩa phủ định của nó. Do đó để tạo ra được các từ với nghĩa chính xác thì chúng ta sẽ sử dụng thêm từ điển bao gồm cả từ đa âm tiết và đơn âm để tokenize câu. Trong Tiếng Việt có khá nhiều các module hỗ trợ tokenize dựa trên từ điển như VnCoreNLP, pyvivn, underthesea.\n",
        "\n",
        "**Tokenize theo character level**: Việc tokenize theo word level thường sinh ra một từ điển với kích thước rất lớn, điều này làm gia chi phí tính toán. Hơn nữa nếu tokenize theo word level thì đòi hỏi từ điển phải rất lớn thì mới hạn chế được những trường hợp từ nằm ngoài từ điển. Tuy nhiên nếu phân tích ta sẽ thấy hầu hết các từ đều có thể biểu thị dưới một nhóm các ký tự là chữ cái, con số, dấu xác định. Như vậy chỉ cần sử dụng một lượng các ký tự rất nhỏ có thể biểu diễn được mọi từ. Từ được token dựa trên level ký tự sẽ có tác dụng giảm kích thước từ điển mà vẫn biểu diễn được các trường hợp từ nằm ngoài từ điển. Đây là phương pháp được áp dụng trong mô hình fasttext.\n",
        "\n",
        "**Phương pháp mới BPE (SOTA)**: Nhược điểm của phương pháp tokenize theo character level đó là các token không có ý nghĩa nếu đứng độc lập. Do đó đối với các bài toán sentiment analysis, áp dụng tokenize theo character level sẽ mang lại kết quả kém hơn. Token theo word level cũng tồn tại hạn chế đó là không giải quyết được các trường hợp từ ngằm ngoài từ điển.\n",
        "\n",
        "Một phương pháp mới đã được đề xuất trong bài báo [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf) vào năm 2016, có khả năng tách từ theo level nhỏ hơn từ và lớn hơn ký tự được gọi là subword. Phương pháp đó chính là BPE (byte pair encoding). Theo phương pháp mới này, hẫu hết các từ đều có thể biểu diễn bởi subword và chúng ta sẽ hạn chế được một số lượng đáng kể các token `<unk>` đại diện cho từ chưa từng xuất hiện trước đó. Rất nhanh chóng, Phương pháp mới đã được áp dụng ở hầu hết các phương pháp NLP hiện đại từ các lớp model BERT cho tới các biến thể của nó như OpenAI GPT, RoBERTa, DistilBERT, XLMNet. Kết quả áp dụng tokenize theo phương pháp mới đã cải thiện được độ chính xác trên nhiều tác vụ dịch máy, phân loại văn bản, dự báo câu tiếp theo, hỏi đáp, dự báo mối quan hệ văn bản.\n",
        "\n",
        "**Thuật toán BPE:**\n",
        "\n",
        "BPE (Byte Pair Encoding) là một kỹ thuật nén từ cơ bản giúp chúng ta index được toàn bộ các từ kể cả trường hợp từ mở (không xuất hiện trong từ điển) nhờ mã hóa các từ bằng chuỗi các từ phụ (subwords). Nguyên lý hoạt động của BPE dựa trên phân tích trực quan rằng hầu hết các từ đều có thể phân tích thành các thành phần con. \n",
        "\n",
        "Chẳng hạn như từ: `low`, `lower`, `lowest` đều là hợp thành bởi `low` và những đuôi phụ `er`, `est`. Những đuôi này rất thường xuyên xuất hiện ở các từ. Như vậy khi biểu diễn từ `lower` chúng ta có thể mã hóa chúng thành hai thành phần từ phụ (subwords) tách biệt là `low` và `er`. Theo cách biểu diễn này sẽ không phát sinh thêm một index mới cho từ `lower` và đồng thời tìm được mối liên hệ giữa `lower`, `lowest` và `low` nhờ có chung thành phần từ phụ là `low`.\n",
        "\n",
        "\n",
        "Phương pháp BPE sẽ thống kê tần suất xuất hiện của các từ phụ cùng nhau và tìm cách gộp chúng lại nếu tần suất xuất hiện của chúng là lớn nhất. Cứ tiếp tục quá trình gộp từ phụ cho tới khi không tồn tại các subword để gộp nữa, ta sẽ thu được tập subwords cho toàn bộ bộ văn bản mà mọi từ đều có thể biểu diễn được thông qua subwords.\n",
        "\n",
        "Code của thuật toán BPE đã được tác giả chia sẻ tại [subword-nmt](https://github.com/rsennrich/subword-nmt).\n",
        "\n",
        "Qúa trình này gồm các bước như sau:\n",
        "\n",
        "* Bước 1: Khởi tạo từ điển (vocabulary).\n",
        "\n",
        "* Bước 2: Biểu diễn mỗi từ trong bộ văn bản bằng kết hợp của các ký tự với token `<\\w>` ở cuối cùng đánh dấu kết thúc một từ (lý do thêm token sẽ được giải thích bên dưới).\n",
        "\n",
        "* Bước 3: Thống kê tần suất xuất hiện theo cặp của toàn bộ token trong từ điển.\n",
        "\n",
        "* Bước 4: Gộp các cặp có tần suất xuất hiện lớn nhất để tạo thành một n-gram theo level character mới cho từ điển.\n",
        "\n",
        "* Bước 5: Lặp lại bước 3 và bước 4 cho tới khi số bước triển khai merge đạt đỉnh hoặc kích thước kỳ vọng của từ điển đạt được.\n",
        "\n",
        "Bạn sẽ dễ hình dung hơn qua ví dụ bên dưới:\n",
        "\n",
        "Gỉa sử từ điển của chúng ta gồm các từ với tần suất như sau: `vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}`.\n",
        "\n",
        "Coi mỗi ký tự là một token. Khi đó thống kê tần suất xuất hiện của các cặp ký tự như sau:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dtkHS7mTnzF",
        "colab_type": "code",
        "outputId": "5f4b78ba-f498-4eb3-ed16-af01aeace2dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import collections\n",
        "\n",
        "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "get_stats(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('d', 'e'): 3,\n",
              "             ('e', 'r'): 2,\n",
              "             ('e', 's'): 9,\n",
              "             ('e', 'w'): 6,\n",
              "             ('i', 'd'): 3,\n",
              "             ('l', 'o'): 7,\n",
              "             ('n', 'e'): 6,\n",
              "             ('o', 'w'): 7,\n",
              "             ('r', '</w>'): 2,\n",
              "             ('s', 't'): 9,\n",
              "             ('t', '</w>'): 9,\n",
              "             ('w', '</w>'): 5,\n",
              "             ('w', 'e'): 8,\n",
              "             ('w', 'i'): 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p5OieB3n8L4",
        "colab_type": "text"
      },
      "source": [
        "Lựa chọn cặp từ phụ có tần suất xuất hiện nhỏ nhất và merge chúng thành một từ phụ mới."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKnopGGCoN4V",
        "colab_type": "code",
        "outputId": "14669ac0-6a15-4497-913c-725da31145c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import re, collections\n",
        "\n",
        "pairs = get_stats(vocab)\n",
        "best = max(pairs, key=pairs.get)\n",
        "print('max pair frequency: ', best)\n",
        "\n",
        "# Hàm merge byte max frequency\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    # Tìm kiếm các vị trí xuất hiện pair bytes\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        # Thay thế các cặp pair bytes bằng single byte gộp\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "merge_vocab(best, vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max pair frequency:  ('e', 's')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'l o w </w>': 5,\n",
              " 'l o w e r </w>': 2,\n",
              " 'n e w es t </w>': 6,\n",
              " 'w i d es t </w>': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc57Bzo9oNPR",
        "colab_type": "text"
      },
      "source": [
        "Lặp lại quá trình thống kê tần suất cặp từ và gộp cặp từ với số lượt gộp là 1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ__IEKzocxE",
        "colab_type": "code",
        "outputId": "7f884a3a-00c1-4b90-cd4f-0bdd05485dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
        "\n",
        "def get_tokens(vocab):\n",
        "    tokens = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens[token] += freq\n",
        "    return tokens\n",
        "\n",
        "num_merges = 1000\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    # max_freq = max(pairs.values())\n",
        "    # if max_freq == 1:\n",
        "    #   break\n",
        "\n",
        "    if not pairs:\n",
        "      break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    # print('best', best)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print('Iter: {}'.format(i))\n",
        "    print('Best pair: {}'.format(best))\n",
        "    tokens = get_tokens(vocab)\n",
        "    print('Tokens: {}'.format(tokens))\n",
        "    print('Number of tokens: {}'.format(len(tokens)))\n",
        "    print('==========')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0\n",
            "Best pair: ('e', 's')\n",
            "Tokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 8, 'r': 2, 'n': 6, 'es': 9, 't': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 11\n",
            "==========\n",
            "Iter: 1\n",
            "Best pair: ('es', 't')\n",
            "Tokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 8, 'r': 2, 'n': 6, 'est': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 10\n",
            "==========\n",
            "Iter: 2\n",
            "Best pair: ('est', '</w>')\n",
            "Tokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'est</w>': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 10\n",
            "==========\n",
            "Iter: 3\n",
            "Best pair: ('l', 'o')\n",
            "Tokens: defaultdict(<class 'int'>, {'lo': 7, 'w': 16, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'est</w>': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 4\n",
            "Best pair: ('lo', 'w')\n",
            "Tokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'w': 9, 'est</w>': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 5\n",
            "Best pair: ('n', 'e')\n",
            "Tokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 2, 'r': 2, 'ne': 6, 'w': 9, 'est</w>': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 6\n",
            "Best pair: ('ne', 'w')\n",
            "Tokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 2, 'r': 2, 'new': 6, 'est</w>': 9, 'w': 3, 'i': 3, 'd': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 7\n",
            "Best pair: ('new', 'est</w>')\n",
            "Tokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 2, 'r': 2, 'newest</w>': 6, 'w': 3, 'i': 3, 'd': 3, 'est</w>': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 8\n",
            "Best pair: ('low', '</w>')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'low': 2, 'e': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'w': 3, 'i': 3, 'd': 3, 'est</w>': 3})\n",
            "Number of tokens: 10\n",
            "==========\n",
            "Iter: 9\n",
            "Best pair: ('w', 'i')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'low': 2, 'e': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'wi': 3, 'd': 3, 'est</w>': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 10\n",
            "Best pair: ('wi', 'd')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'low': 2, 'e': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'wid': 3, 'est</w>': 3})\n",
            "Number of tokens: 8\n",
            "==========\n",
            "Iter: 11\n",
            "Best pair: ('wid', 'est</w>')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'low': 2, 'e': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'widest</w>': 3})\n",
            "Number of tokens: 7\n",
            "==========\n",
            "Iter: 12\n",
            "Best pair: ('low', 'e')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'lowe': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'widest</w>': 3})\n",
            "Number of tokens: 6\n",
            "==========\n",
            "Iter: 13\n",
            "Best pair: ('lowe', 'r')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'lower': 2, '</w>': 2, 'newest</w>': 6, 'widest</w>': 3})\n",
            "Number of tokens: 5\n",
            "==========\n",
            "Iter: 14\n",
            "Best pair: ('lower', '</w>')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'lower</w>': 2, 'newest</w>': 6, 'widest</w>': 3})\n",
            "Number of tokens: 4\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4KTwM8tofxH",
        "colab_type": "text"
      },
      "source": [
        "Ta nhận thấy qua các lượt merge từ phụ, độ dài của các từ phụ trong từ điển tăng dần. Thuật toán hội tụ trước 1000 vòng lặp vì toàn bộ các từ phụ đã được merge và đạt ngưỡng của từng từ đơn.\n",
        "\n",
        "Khi giới hạn kích thước của từ điển hoặc số lượng lượt merge ta sẽ thu được một từ điển từ phụ là thành phần của các từ trong từ điển. Khi đó mọi từ mới dường như sẽ có thể biểu diễn được theo từ phụ.\n",
        "\n",
        "Ví dụ: Khi dừng số lượt merge tại bước 10 ta thu được từ điển: `{'low</w>': 5, 'low': 2, 'e': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'wid': 3, 'est</w>': 3}`.\n",
        "\n",
        "Khi đó ta có thể biểu diễn một token mới chưa từng xuất hiện trong từ điển là `wider` thành `wid e r`. Bạn đọc đã hình dung được tác dụng của từ phụ (subword) rồi chứ? \n",
        "\n",
        "**Tác dụng của token </w>**\n",
        "\n",
        "Gỉa định khi tokenize câu `the highest mountain` theo từ phụ ta thu được biểu diễn `['the</w>', 'high', 'est</w>', 'moun', 'tain</w>']`. Khi đó để khôi phục được thành câu gốc ta chỉ cần nối các token lại theo thứ tự thành `the</w>highest</w>mountain</w>`. Chỉ cần thay `</w>` bằng khoảng trắng ta sẽ khôi phục được câu gốc: `the highest mountain`.\n",
        "\n",
        "token `</w>` được thêm vào cuối mỗi từ để phân biệt các từ phụ nằm ở vị trí cuối câu với các vị trí khác để giúp cho việc giải mã token khả thi hơn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUEStXr9DVE3",
        "colab_type": "text"
      },
      "source": [
        "**Áp dụng BPE tokenize trong BERT:**\n",
        "\n",
        "Hầu hết các mô hình NLP hiện đại nhất đều đã chuyển sang tokenize theo BPE. Để sử dụng BPE tokenize từ các model pretrain của BERT ta thực hiện như sau:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM8wrme9-w3G",
        "colab_type": "code",
        "outputId": "cc907051-b5e3-434a-e30d-9b68577bab51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!pip3 install fairseq\n",
        "!pip3 install fastBPE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.19)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.4.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq) (1.7.0)\n",
            "Collecting fastBPE\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/37/f97181428a5d151501b90b2cebedf97c81b034ace753606a3cda5ad4e6e2/fastBPE-0.1.0.tar.gz\n",
            "Building wheels for collected packages: fastBPE\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl size=481526 sha256=424ceb11180c68d7fedb85148a9c7f6c0eda16dc7bca216bd4c6ae8051c84c81\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/0c/9c/fc62058b4d473a5602bcd3d3edfece796f123875379ea82d79\n",
            "Successfully built fastBPE\n",
            "Installing collected packages: fastBPE\n",
            "Successfully installed fastBPE-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjc403EWuiQ4",
        "colab_type": "text"
      },
      "source": [
        "Load model pretrain `RoBERTa`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b6VadBluaIt",
        "colab_type": "code",
        "outputId": "6b683374-7e4d-4157-e13b-99f99cab459f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "phoBERT = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "phoBERT.eval()  # disable dropout (or leave in train mode to finetune"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading archive file PhoBERT_base_fairseq\n",
            "| dictionary: 64000 types\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1042301B [00:01, 1016010.90B/s]\n",
            "456318B [00:00, 659175.02B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaHubInterface(\n",
              "  (model): RobertaModel(\n",
              "    (decoder): RobertaEncoder(\n",
              "      (sentence_encoder): TransformerSentenceEncoder(\n",
              "        (embed_tokens): Embedding(64001, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(258, 768, padding_idx=1)\n",
              "        (layers): ModuleList(\n",
              "          (0): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (6): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (7): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (8): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (9): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (10): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (11): TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): RobertaLMHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (classification_heads): ModuleDict()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBx4dbdcuqSH",
        "colab_type": "text"
      },
      "source": [
        "Khai báo bpe tokenizer và thực hiện token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEktUtHVRI-h",
        "colab_type": "code",
        "outputId": "b55e647d-460c-44ed-c231-08eae3114b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "\n",
        "# Khởi tạo Byte Pair Encoding cho PhoBERT\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "tokens = phoBERT.encode('Tôn Ngộ Không phò Đường Tăng đi Tây Trúc thỉnh kinh')\n",
        "print('tokens list : ', tokens)\n",
        "# Decode ngược lại thành câu từ chuỗi index token\n",
        "phoBERT.decode(tokens)  # 'Hello world!'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokens list :  tensor([    0, 11623, 31433,   453, 44334,  2080,  5922,    57,   934,  8181,\n",
            "        31686,  3078,     2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tôn Ngộ Không phò Đường Tăng đi Tây Trúc thỉnh kinh'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQheWVBPLIbh",
        "colab_type": "text"
      },
      "source": [
        "## 5. Extract features từ RoBERTa\n",
        "\n",
        "Huấn luyện mô hình theo RoBERTa sẽ thu được các version:\n",
        "\n",
        "* `BERT base`: 12 sub-layers, kích thước embedding 768, số lượng head attention là 12.\n",
        "* `BERT large`: 24 sub-layers, kích thước embedding 1024, số lượng head attention là 16.\n",
        "\n",
        "Chúng ta có thể trích xuất được các đặc trưng được tạo ra từ BERT tại phase Encoder tại layers cuối cùng hoặc toàn bộ các layers.\n",
        "\n",
        "![](https://phamdinhkhanh.github.io/assets/images/20190616_attention/EncoderInTransformer.png)\n",
        "\n",
        "**Hình 1:** Kiến trúc gồm nhiều layers tại encoder của model BERT. Mô hình huấn luyện từ RoBERTa cho phép ta trích suất các đặc trưng từ những layers của encoder. Có thể là layer cuối hoặc toàn bộ các layers.\n",
        "\n",
        "Kích thước output tại các layers sẽ là `batch_size x seq_len x d_model`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPR5Zie9Wb9R",
        "colab_type": "code",
        "outputId": "e0bfc6bc-9303-4a2e-8b8d-8b7917f1b681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Extract the last layer's features\n",
        "last_layer_features = phoBERT.extract_features(tokens)\n",
        "# assert last_layer_features.size() == torch.Size([1, 5, 1024])\n",
        "print('token size: ', tokens.size())\n",
        "print('size of last layer: ', last_layer_features.size())\n",
        "\n",
        "# Extract all layer's features (layer 0 is the embedding layer)\n",
        "all_layers = phoBERT.extract_features(tokens, return_all_hiddens=True)\n",
        "print('number layer in all layers: ', len(all_layers))\n",
        "\n",
        "# last_layer_features must equal to last layer in all_layers:\n",
        "print('Last layer features: ', all_layers[-1] == last_layer_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "token size:  torch.Size([13])\n",
            "size of last layer:  torch.Size([1, 13, 768])\n",
            "number layer in all layers:  13\n",
            "Last layer features:  tensor([[[True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         ...,\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQRW78FZY_eg",
        "colab_type": "text"
      },
      "source": [
        "## 6. Filling mask\n",
        "\n",
        "Trong bài toán này chúng ta sẽ điền các từ hợp lý vào các vị trí còn trống của câu. Trên thực tế có rất nhiều ứng dụng của bài toàn filling mask như xây dựng hệ thống suggestion search, gợi ý gõ văn bản, tìm từ đồng nghĩa, tagging.\n",
        "\n",
        "Mô hình BERT tạo ra các biểu diễn từ từ quá trình ẩn các vị trí token một cách ngẫu nhiên trong câu input và dự báo chính chính từ đó ở output dựa trên bối cảnh là các từ xung quanh.\n",
        "\n",
        "Như vậy khi đã biết các từ xung quanh, chúng ta hoàn toàn có thể dự báo được từ phù hợp nhất với vị trí đã được masking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGIJnIun5s63",
        "colab_type": "text"
      },
      "source": [
        "Down load package VnCoreNLP để tokenize các câu văn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q58uPybPtHiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter) \n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj42wS7W58G0",
        "colab_type": "text"
      },
      "source": [
        "Gỉa sử chúng ta có câu gốc là `Tôn Ngộ Không phò Đường Tăng đi thỉnh kinh tại Tây Trúc`. Từ được ẩn đi trong câu là `phò` sẽ được thay thế bằng token `<mask>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9FZ5iHzs-Lr",
        "colab_type": "code",
        "outputId": "6ea1b96d-a673-42ff-841a-afb75de71670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "\n",
        "text = 'Tôn Ngộ Không phò Đường Tăng đi thỉnh kinh tại Tây Trúc'\n",
        "text_masked = 'Học sinh được  <mask> do dịch covid-19'\n",
        "# Tokenize câu gốc và thay từ phò bằng <mask>\n",
        "words = rdrsegmenter.tokenize(text)[0]\n",
        "for i, token in enumerate(words):\n",
        "  if token == 'phò':\n",
        "    words[i] = ' <mask>'\n",
        "text_masked_tok = ' '.join(words)\n",
        "print('text_masked_tok: \\n', text_masked_tok)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text_masked_tok: \n",
            " Tôn_Ngộ_Không  <mask> Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sJB33t-6Dqm",
        "colab_type": "text"
      },
      "source": [
        "Tìm ra top 10 từ thích hợp nhất cho vị trí `<mask>` tại câu trên."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGJCpSZWwDwC",
        "colab_type": "code",
        "outputId": "4e15b136-5204-4e99-c0b3-420e08e7106a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE  \n",
        "from fairseq import options  \n",
        "import numpy as np\n",
        "\n",
        "# Khởi tạo Byte Pair Encoding cho PhoBERT\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "\n",
        "# Filling marks  \n",
        "topk_filled_outputs = phoBERT.fill_mask(text_masked_tok, topk=10) \n",
        "topk_probs = [item[1] for item in topk_filled_outputs]\n",
        "print('Total probability: ', np.sum(topk_probs))\n",
        "print('Input sequence: ', text_masked_tok)\n",
        "print('Top 10 in mask: ')\n",
        "for i, output in enumerate(topk_filled_outputs): \n",
        "  print(output[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total probability:  0.8735223989933729\n",
            "Input sequence:  Tôn_Ngộ_Không  <mask> Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Top 10 in mask: \n",
            "Tôn_Ngộ_Không và Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không đưa Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không cõng Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không hộ_tống Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không cùng Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không chở Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không theo Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không dẫn Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không , Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không tháp_tùng Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P9Nrq0YLh1W",
        "colab_type": "text"
      },
      "source": [
        "## 7. Extract feature cho các từ\n",
        "\n",
        "Chúng ta có thể tìm ra được các véc tơ embedding cho từng từ trong câu từ mô hình BERT như sau:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_E0SG0dYbSo",
        "colab_type": "code",
        "outputId": "2a3219ef-cacf-411f-b293-7cb1ee5a364a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "\n",
        "# Khởi tạo Byte Pair Encoding cho PhoBERT\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "doc = phoBERT.extract_features_aligned_to_words('học_sinh cấp 3 được đến trường sau nghỉ dịch covid')\n",
        "\n",
        "for tok in doc:\n",
        "    print('{:10}{} (...) {}'.format(str(tok), tok.vector[:5], tok.vector.size()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s>       tensor([ 0.0534,  0.1301, -0.0475, -0.8371,  0.3862], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "học_sinh  tensor([ 0.1764,  0.1603,  0.0792, -0.6043, -0.3138], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "cấp       tensor([ 0.0679,  0.0194,  0.3450, -0.4951, -0.6394], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "3         tensor([-0.0465, -0.3846,  0.1337, -1.1276,  0.1910], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "được      tensor([ 0.1920, -0.0146,  0.2933,  0.0086,  0.0690], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "đến       tensor([-0.0108, -0.6463, -0.2906, -0.0317,  0.0561], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "trường    tensor([-0.0270,  0.2676,  0.3856,  0.3514,  0.1169], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "sau       tensor([-0.1175,  0.4808,  0.0772, -0.2991,  0.0147], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "nghỉ      tensor([ 0.4385,  0.4162,  0.1529, -0.1419, -0.1928], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "dịch      tensor([ 0.2958, -0.0976,  0.2024, -0.9278,  0.0270], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "covid     tensor([ 0.1539,  0.2343,  0.4054, -1.6919, -0.7180], grad_fn=<SliceBackward>) (...) torch.Size([768])\n",
            "</s>      tensor([ 0.0558,  0.0341, -0.0286, -0.6476,  0.4656], grad_fn=<SliceBackward>) (...) torch.Size([768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCp713Ar8m0h",
        "colab_type": "text"
      },
      "source": [
        "Khi đó mỗi từ sẽ được biểu diễn bằng 768 chiều là số chiều của hidden véc tơ trong mô hình BERT base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2vAJoNuzOtR",
        "colab_type": "text"
      },
      "source": [
        "## 8. Bài toán classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YVxPXyiUi2p",
        "colab_type": "text"
      },
      "source": [
        "### 8.1. Kiến trúc mô hình\n",
        "\n",
        "Ý tưởng fine-tuning được lấy từ bài báo [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/abs/1905.05583).\n",
        "\n",
        "Model BERT base sẽ tạo ra một kiến trúc gồm 12 sub-layers ở encoder, 12 heads trong multi-head attention trên mỗi sub-layer. Output là tập hợp các véc tơ self-attention bằng chiều dài của input. Mỗi véc tơ có kích thước là 768.\n",
        "\n",
        "Để fine-tuning lại kiến trúc của BERT cho tác vụ phân loại văn bản (text classification). Chúng ta truncate decoder của BERT, giữa nguyên kiến trúc encoder của transformer và sau đó trích suất ra biểu diễn véc tơ của token `CLS` đánh dấu vị trí đầu tiên. Véc tơ  này sẽ được sử dụng làm đầu vào cho thuật toán classifier bằng cách thêm một linear projection layer (cũng chính là fully connected layer) ở cuối có kích thước bằng với số classes cần phân loại. Cụ thể hơn chúng ta cùng xem kiến trúc bên dưới.\n",
        "\n",
        "![](https://imgur.com/oo4s0l4.png)\n",
        "\n",
        "Hình 1: Kiến trúc fine-tuning classifier của BERT trong classification. Biểu diễn self-attention của token tại vị trí `CLS` được sử dụng làm input cho thuật toán phân loại. Chúng ta thêm một linear projection layer ở cuối cùng để tính toán phân phối xác suất.\n",
        "\n",
        "Để lấy ví dụng cho quá trình fine-tuning lại PhoBERT cho tác vụ phân loại văn bản mình sẽ huấn luyện model phân loại topics báo chí. Chúng ta sẽ tìm hiểu về bộ dữ liệu cho mô hình.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5on0Il6UnFL",
        "colab_type": "text"
      },
      "source": [
        "### 8.2. Dữ liệu\n",
        "\n",
        "Dữ liệu mà mình sử dụng là [VNTC](https://github.com/duyvuleo/VNTC.git) với các bài báo đã được sắp xếp theo 10 topics. Bộ dữ liệu bao gồm 33 nghìn bài báo trên tập train và 50 nghìn bài báo trên tập test có phân bố số lượng theo topics như sau:\n",
        "\n",
        "![](https://imgur.com/1lDTdC1.png)\n",
        "\n",
        "Dữ liệu sau xử lý được mình chia sẻ. Nếu không muốn tìm hiểu quá trình tạo dữ liệu, bạn đọc có thể chuyển qua mục `Tokenize Input và output` và bỏ qua bước này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAu0zPbdOfcy",
        "colab_type": "text"
      },
      "source": [
        "#### 8.2.1. Đọc và lưu dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfS6t45AMccg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/duyvuleo/VNTC.git\n",
        "!ls VNTC/Data/10Topics/Ver1.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJLquEjjNkbk",
        "colab_type": "text"
      },
      "source": [
        "Sau khi đã download dữ liệu về, chúng ta sẽ đọc và lưu các bài báo vào những list chứa nội dung và nhãn tương ứng theo 2 folders train và test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz8h_LPtzeYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob2\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_path = 'Train_Full/*/*.txt'\n",
        "test_path = 'Test_Full/*/*.txt'\n",
        "\n",
        "# Hàm đọc file txt\n",
        "def read_txt(path):\n",
        "  with open(path, 'r', encoding='utf-16') as f:\n",
        "    data = f.read()\n",
        "  return data\n",
        "\n",
        "# Hàm tạo dữ liệu huấn luyện cho tập train và test\n",
        "def make_data(path):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  for file_path in tqdm(glob2.glob(train_path)):\n",
        "    try:\n",
        "      content = read_txt(file_path)\n",
        "      label = file_path.split('/')[1]\n",
        "      texts.append(content)\n",
        "      labels.append(label)\n",
        "    except:\n",
        "      next\n",
        "  return texts, labels\n",
        "\n",
        "text_train, label_train = make_data(train_path)\n",
        "text_test, label_test = make_data(test_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3LXza7cOFOj",
        "colab_type": "text"
      },
      "source": [
        "Quá trình đọc files sẽ tốn khá nhiều thời gian. Do đó các bạn có thể tạo các hàm lưu trữ lại các list nội dung và nhãn và load lại cho lượt huấn luyện sau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEfxRTYWzlcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def _save_pkl(path, obj):\n",
        "  with open(path, 'wb') as f:\n",
        "    pickle.dump(obj, f)\n",
        "\n",
        "def _load_pkl(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    obj = pickle.load(f)\n",
        "  return obj\n",
        "\n",
        "# Lưu lại các files\n",
        "_save_pkl('text_train.pkl', text_train)\n",
        "_save_pkl('label_train.pkl', label_train)\n",
        "_save_pkl('text_test.pkl', text_test)\n",
        "_save_pkl('label_test.pkl', label_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLhEKmFM4Pt_",
        "colab_type": "code",
        "outputId": "838fd8f1-121a-4bec-c350-d74d25fe8f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "print('text content:\\n', text_train[0])\n",
        "print('label:\\n', label_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text content:\n",
            " Tấm hít nhỏ xinh\n",
            "Tủ lạnh hay phía tường trước bàn làm việc của bạn sẽ đẹp hơn nếu có những tấm hít nhỏ xinh để trang trí hoặc để dính những mảnh giấy ghi chú. Hãy bắt tay vào làm đi, không khó lắm đâu bạn ạ.\n",
            "Chuẩn bị: nam châm dày 3 mm; gỗ mỏng; sơn; keo dán gỗ; cọ, cưa.\n",
            "Thực hiện: \n",
            "Bước 1: Cưa 3 mảnh gỗ vuông làm nền, diện tích 4 cm2. Bạn có thể thay đổi kích thước lớn hoặc nhỏ hơn tuỳ theo ý thích. Tiếp theo, cưa gỗ thành những mảnh hình tam giác, hình vuông hoặc chữ nhật nhỏ có kích thước bằng nhau. Dùng cọ sơn màu lên các thanh gỗ nhỏ theo sự sáng tạo của bạn.\n",
            "Bước 2: Dùng keo dán những mảnh gỗ nhỏ vào mảnh gỗ nền. Cần chú ý phối màu, tạo nên những hình ghép lạ mắt. Dán nam châm vào mặt sau. Dùng cọ vẽ thêm chi tiết, hoa văn lên các mảnh ghép.\n",
            "Chú ý: Chọn loại gỗ thật mỏng, nếu không sản phẩm trông rất thô. Không dán các tấm hít lên máy vi tính vì từ tính của nam châm sẽ ảnh hưởng đến nam châm trong máy.\n",
            "\n",
            "\n",
            "label:\n",
            " Doi song\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiOrTcwg39dv",
        "colab_type": "text"
      },
      "source": [
        "#### 8.2.2. Tokenize nội dung\n",
        "\n",
        "Tiếp theo ta sẽ tokenize các câu văn sang chuỗi index và padding câu văn về cũng một độ dài."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCbNn4pcztV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sequence_length = 500\n",
        "\n",
        "def convert_lines(lines, vocab, bpe):\n",
        "  '''\n",
        "  lines: list các văn bản input\n",
        "  vocab: từ điển dùng để encoding subwords\n",
        "  bpe: \n",
        "  '''\n",
        "  # Khởi tạo ma trận output\n",
        "  outputs = np.zeros((len(lines), max_sequence_length)) # --> shape (number_lines, max_seq_len)\n",
        "  # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n",
        "  cls_id = 0\n",
        "  eos_id = 2\n",
        "  pad_id = 1\n",
        "\n",
        "  for idx, row in tqdm(enumerate(lines), total=len(lines)): \n",
        "    # Mã hóa subwords theo byte pair encoding(bpe)\n",
        "    subwords = bpe.encode('<s> '+ row +' </s>')\n",
        "    input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "    # Truncate input nếu độ dài vượt quá max_seq_len\n",
        "    if len(input_ids) > max_sequence_length: \n",
        "      input_ids = input_ids[:max_sequence_length] \n",
        "      input_ids[-1] = eos_id\n",
        "    else:\n",
        "      # Padding nếu độ dài câu chưa bằng max_seq_len\n",
        "      input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n",
        "    \n",
        "    outputs[idx,:] = np.array(input_ids)\n",
        "  return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeWJAX5HOwbo",
        "colab_type": "text"
      },
      "source": [
        "### 8.3. Tokenize Input và output\n",
        "\n",
        "Các bạn có thể download lại dữ liệu $\\mathbf{X, y}$ mà tôi đã chuẩn bị cho huấn luyện tại [Dữ liệu Tokenize](https://drive.google.com/drive/folders/1stRredI0fZ2vE5_SKGggrgDxnV1bxhr1?usp=sharing) và bỏ qua bước này. Thực hiện luôn bước tiếp theo `Load model BERT`.\n",
        "\n",
        "* Chuẩn bị X input: Tokenize nội dung các văn bản sang chuỗi indices.\n",
        "\n",
        "* Chuẩn bị y output: Encoding các label output thành indices đánh dấu số thứ tự của văn bản.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3SLt2y6Owu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "max_sequence_length = 256\n",
        "def convert_lines(lines, vocab, bpe):\n",
        "  '''\n",
        "  lines: list các văn bản input\n",
        "  vocab: từ điển dùng để encoding subwords\n",
        "  bpe: \n",
        "  '''\n",
        "  # Khởi tạo ma trận output\n",
        "  outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32) # --> shape (number_lines, max_seq_len)\n",
        "  # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n",
        "  cls_id = 0\n",
        "  eos_id = 2\n",
        "  pad_id = 1\n",
        "\n",
        "  for idx, row in tqdm(enumerate(lines), total=len(lines)): \n",
        "    # Mã hóa subwords theo byte pair encoding(bpe)\n",
        "    subwords = bpe.encode('<s> '+ row +' </s>')\n",
        "    input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "    # Truncate input nếu độ dài vượt quá max_seq_len\n",
        "    if len(input_ids) > max_sequence_length: \n",
        "      input_ids = input_ids[:max_sequence_length] \n",
        "      input_ids[-1] = eos_id\n",
        "    else:\n",
        "      # Padding nếu độ dài câu chưa bằng max_seq_len\n",
        "      input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n",
        "    \n",
        "    outputs[idx,:] = np.array(input_ids)\n",
        "  return outputs\n",
        "\n",
        "# Load the dictionary  \n",
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"PhoBERT_base_transformers/dict.txt\")\n",
        "\n",
        "\n",
        "# Test encode lines\n",
        "lines = ['Học_sinh được nghỉ học bắt dầu từ tháng 3 để tránh dịch covid-19', 'số lượng ca nhiễm bệnh đã giảm bắt đầu từ tháng 5 nhờ biện pháp mạnh tay']\n",
        "[x1, x2] = convert_lines(lines, vocab, phoBERT.bpe)\n",
        "print('x1 tensor encode: {}, shape: {}'.format(x1[:10], x1.size))\n",
        "print('x1 tensor decode: ', phoBERT_cls.decode(torch.tensor(x1))[:103])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s07ftdJPMGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "max_sequence_length = 256\n",
        "def convert_lines(lines, vocab, bpe):\n",
        "  '''\n",
        "  lines: list các văn bản input\n",
        "  vocab: từ điển dùng để encoding subwords\n",
        "  bpe: \n",
        "  '''\n",
        "  # Khởi tạo ma trận output\n",
        "  outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32) # --> shape (number_lines, max_seq_len)\n",
        "  # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n",
        "  cls_id = 0\n",
        "  eos_id = 2\n",
        "  pad_id = 1\n",
        "\n",
        "  for idx, row in tqdm(enumerate(lines), total=len(lines)): \n",
        "    # Mã hóa subwords theo byte pair encoding(bpe)\n",
        "    subwords = bpe.encode('<s> '+ row +' </s>')\n",
        "    input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "    # Truncate input nếu độ dài vượt quá max_seq_len\n",
        "    if len(input_ids) > max_sequence_length: \n",
        "      input_ids = input_ids[:max_sequence_length] \n",
        "      input_ids[-1] = eos_id\n",
        "    else:\n",
        "      # Padding nếu độ dài câu chưa bằng max_seq_len\n",
        "      input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n",
        "    \n",
        "    outputs[idx,:] = np.array(input_ids)\n",
        "  return outputs\n",
        "\n",
        "# Load the dictionary  \n",
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"PhoBERT_base_transformers/dict.txt\")\n",
        "\n",
        "\n",
        "# Test encode lines\n",
        "lines = ['Học_sinh được nghỉ học bắt dầu từ tháng 3 để tránh dịch covid-19', 'số lượng ca nhiễm bệnh đã giảm bắt đầu từ tháng 5 nhờ biện pháp mạnh tay']\n",
        "[x1, x2] = convert_lines(lines, vocab, phoBERT_cls.bpe)\n",
        "print('x1 tensor encode: {}, shape: {}'.format(x1[:10], x1.size))\n",
        "print('x1 tensor decode: ', phoBERT_cls.decode(torch.tensor(x1))[:103])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrIa1UAzPTGc",
        "colab_type": "text"
      },
      "source": [
        "Như vậy ta thấy rằng các câu văn đã được encode về token index. Từ token index có thể decode ngược trở lại thành câu input sau khi đã thêm các token đặc biệt đánh dấu vị trí bắt dầu: `<s>`, kết thúc: `</s>` câu và các vị trí nằm ngoài câu: `<pad>`. Ta sẽ token toàn bộ câu input sang index như sau:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKjEwDNpPWH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = convert_lines(text_train, vocab, phoBERT_cls.bpe)\n",
        "print('X shape: ', X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSUCZM1yPeTN",
        "colab_type": "text"
      },
      "source": [
        "Sau cùng ta thu được các chuỗi index có kích thước là 256, bằng với kích thước của các câu sau khi đã padding. Tiếp theo ta tạo output `y` bằng index cho các nhãn của câu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrebnYOQPlrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "lb.fit(label_train)\n",
        "y = lb.fit_transform(label_train)\n",
        "print(lb.classes_)\n",
        "print('Top 5 classes indices: ', y[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFiwLplTPqud",
        "colab_type": "text"
      },
      "source": [
        "Lưu lại dữ liệu $\\mathbf{X}$ và $\\mathbf{y}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdMP9VFcPtSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save dữ liệu\n",
        "_save_pkl('PhoBERT_pretrain/X1.pkl', X)\n",
        "_save_pkl('PhoBERT_pretrain/y1.pkl', y)\n",
        "_save_pkl('PhoBERT_pretrain/labelEncoder1.pkl', lb)\n",
        "\n",
        "# Load lại dữ liệu\n",
        "X = _load_pkl('PhoBERT_pretrain/X1.pkl')\n",
        "y = _load_pkl('PhoBERT_pretrain/y1.pkl')\n",
        "\n",
        "print('length of X: ', len(X))\n",
        "print('length of y: ', len(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIeDJBBt5jqd",
        "colab_type": "text"
      },
      "source": [
        "### 8.4. Load model BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J9li8Du5laj",
        "colab_type": "code",
        "outputId": "d73bdf9b-6f3a-417c-a829-a4778b6da58f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "\n",
        "phoBERT_cls = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "phoBERT_cls.eval()  # disable dropout (or leave in train mode to finetune\n",
        "\n",
        "# Load BPE\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "args = BPE()\n",
        "phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "\n",
        "# Add header cho classification với số lượng classes = 10\n",
        "phoBERT_cls.register_classification_head('new_task', num_classes=10)\n",
        "tokens = 'Học_sinh được nghỉ học bắt đầu từ tháng 3 do ảnh hưởng của dịch covid-19'\n",
        "token_idxs = phoBERT_cls.encode(tokens)\n",
        "logprobs = phoBERT_cls.predict('new_task', token_idxs)  # tensor([[-1.1050, -1.0672, -1.1245]], grad_fn=<LogSoftmaxBackward>)\n",
        "logprobs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading archive file PhoBERT_base_fairseq\n",
            "| dictionary: 64000 types\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.3722, -2.1128, -2.2945, -2.3484, -2.2294, -2.1600, -2.5104, -2.4261,\n",
              "         -2.4144, -2.2299]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIDiPTXQIMIA",
        "colab_type": "text"
      },
      "source": [
        "### 8.5. Huấn luyện model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVilFL_ofryJ",
        "colab_type": "text"
      },
      "source": [
        "Xây dựng hàm đánh giá mô hình theo 2 metric là `accuracy` và `f1_score`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyEul26gGd8o",
        "colab_type": "code",
        "outputId": "99f1991f-d168-4e0e-f380-4a3e8e2e3dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate(logits, targets):\n",
        "    \"\"\"\n",
        "    Đánh giá model sử dụng accuracy và f1 scores.\n",
        "    Args:\n",
        "        logits (B,C): torch.LongTensor. giá trị predicted logit cho class output.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        acc (float): the accuracy score\n",
        "        f1 (float): the f1 score\n",
        "    \"\"\"\n",
        "    # Tính accuracy score và f1_score\n",
        "    logits = logits.detach().cpu().numpy()    \n",
        "    y_pred = np.argmax(logits, axis = 1)\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    f1 = f1_score(targets, y_pred, average='weighted')\n",
        "    acc = accuracy_score(targets, y_pred)\n",
        "    return acc, f1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logits = torch.tensor([[0.1, 0.2, 0.7],\n",
        "                       [0.4, 0.1, 0.5],\n",
        "                       [0.1, 0.2, 0.7]]).to(device)\n",
        "targets = torch.tensor([1, 2, 2]).to(device)\n",
        "evaluate(logits, targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, 0.5333333333333333)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BckPyYTFA6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(valid_loader, model, device):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    f1s = []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in valid_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model.predict('new_task', x_batch)\n",
        "            logits = torch.exp(outputs)\n",
        "            acc, f1 = evaluate(logits, y_batch)\n",
        "            accs.append(acc)\n",
        "            f1s.append(f1)\n",
        "    \n",
        "    mean_acc = np.mean(accs)\n",
        "    mean_f1 = np.mean(f1s)\n",
        "    return mean_acc, mean_f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmXcURqDf4ff",
        "colab_type": "text"
      },
      "source": [
        "Hàm huấn luyện mô hình trên từng epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAxHCWLiO-jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainOnEpoch(train_loader, model, optimizer, epoch, num_epochs, criteria, device, log_aggr = 100):\n",
        "    model.train()\n",
        "    sum_epoch_loss = 0\n",
        "    sum_acc = 0\n",
        "    sum_f1 = 0\n",
        "    start = time.time()\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      y_pred = model.predict('new_task', x_batch)\n",
        "      logits = torch.exp(y_pred)\n",
        "      acc, f1 = evaluate(logits, y_batch)\n",
        "      loss = criteria(y_pred, y_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_val = loss.item()\n",
        "      sum_epoch_loss += loss_val\n",
        "      sum_acc += acc\n",
        "      sum_f1 += f1\n",
        "      iter_num = epoch * len(train_loader) + i + 1\n",
        "\n",
        "      if i % log_aggr == 0:\n",
        "            print('[TRAIN] epoch %d/%d  observation %d/%d batch loss: %.4f (avg %.4f),  avg acc: %.4f, avg f1: %.4f, (%.2f im/s)'\n",
        "                % (epoch + 1, num_epochs, i, len(train_loader), loss_val, sum_epoch_loss / (i + 1),  sum_acc/(i+1), sum_f1/(i+1),\n",
        "                  len(x_batch) / (time.time() - start)))\n",
        "      start = time.time()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udmWQ9mDbnmJ",
        "colab_type": "text"
      },
      "source": [
        "Quá trình huấn luyện một model classification trên pytorch sẽ bao gồm những bước chính sau đây:\n",
        "\n",
        "* Khởi tạo DataLoader để quản lý dữ liệu đưa vào huấn luyện và thẩm định.\n",
        "\n",
        "* Thiết lập kiến trúc mô hình.\n",
        "\n",
        "* Khai báo hàm loss function.\n",
        "\n",
        "* Phương pháp optimization giúp tối ưu loss function.\n",
        "\n",
        "* Huấn luyện mô hình qua các epochs.\n",
        "\n",
        "Bên dưới chúng ta sẽ lần lượt thực hiện các bước trên."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2Z5H1b78E_c",
        "colab_type": "code",
        "outputId": "fbe8c1ce-d847-41c7-998e-cc8cc083d158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "from transformers.modeling_utils import * \n",
        "from transformers import *\n",
        "\n",
        "# Khởi tạo argument\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 6\n",
        "ACCUMULATION_STEPS = 5\n",
        "FOLD = 4\n",
        "LR = 0.0001\n",
        "LR_DC_STEP = 80 \n",
        "LR_DC = 0.1\n",
        "CUR_DIR = os.path.dirname(os.getcwd())\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "FOLD = 4\n",
        "CKPT_PATH2 = 'model_ckpt2'\n",
        "\n",
        "if not os.path.exists(CKPT_PATH2):\n",
        "    os.mkdir(CKPT_PATH2)\n",
        "\n",
        "# Khởi tạo DataLoader\n",
        "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=123).split(X, y))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(splits):\n",
        "    best_score = 0\n",
        "    if fold != FOLD:\n",
        "        continue\n",
        "    print(\"Training for fold {}\".format(fold))\n",
        "    \n",
        "    # Create dataset\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X[train_idx],dtype=torch.long), torch.tensor(y[train_idx],dtype=torch.long))\n",
        "    valid_dataset = torch.utils.data.TensorDataset(torch.tensor(X[val_idx],dtype=torch.long), torch.tensor(y[val_idx],dtype=torch.long))\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Khởi tạo model:\n",
        "    MODEL_LAST_CKPT = os.path.join(CKPT_PATH2, 'latest_checkpoint.pth.tar')\n",
        "    if os.path.exists(MODEL_LAST_CKPT):\n",
        "      print('Load checkpoint model!')\n",
        "      phoBERT_cls = torch.load(MODEL_LAST_CKPT)\n",
        "    else:\n",
        "      print('Load model pretrained!')\n",
        "      # Load the model in fairseq\n",
        "      from fairseq.models.roberta import RobertaModel\n",
        "      from fairseq.data.encoders.fastbpe import fastBPE\n",
        "      from fairseq.data import Dictionary\n",
        "\n",
        "      phoBERT_cls = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "      phoBERT_cls.eval()  # disable dropout (or leave in train mode to finetune\n",
        "\n",
        "      # # Load BPE\n",
        "      # class BPE():\n",
        "      #   bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "      # args = BPE()\n",
        "      # phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "\n",
        "      # Add header cho classification với số lượng classes = 10\n",
        "      phoBERT_cls.register_classification_head('new_task', num_classes=10)\n",
        "      \n",
        "    ## Load BPE\n",
        "    print('Load BPE')\n",
        "    class BPE():\n",
        "      bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "    args = BPE()\n",
        "    phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "    phoBERT_cls.to(DEVICE)\n",
        "\n",
        "    # Khởi tạo optimizer và scheduler, criteria\n",
        "    print('Init Optimizer, scheduler, criteria')\n",
        "    param_optimizer = list(phoBERT_cls.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    num_train_optimization_steps = int(EPOCHS*len(train_dataset)/BATCH_SIZE/ACCUMULATION_STEPS)\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=LR, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # scheduler với linear warmup\n",
        "    scheduler0 = get_constant_schedule(optimizer)  # scheduler với hằng số\n",
        "    # optimizer = optim.Adam(phoBERT_cls.parameters(), LR)\n",
        "    criteria = nn.NLLLoss()\n",
        "    # scheduler = StepLR(optimizer, step_size = LR_DC_STEP, gamma = LR_DC)\n",
        "    avg_loss = 0.\n",
        "    avg_accuracy = 0.\n",
        "    frozen = True\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "        # warm up tại epoch đầu tiên, sau epoch đầu sẽ phá băng các layers\n",
        "        if epoch > 0 and frozen:\n",
        "            for child in phoBERT_cls.children():\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = True\n",
        "            frozen = False\n",
        "            del scheduler0\n",
        "            torch.cuda.empty_cache()\n",
        "        # Train model on EPOCH\n",
        "        print('Epoch: ', epoch)\n",
        "        trainOnEpoch(train_loader=train_loader, model=phoBERT_cls, optimizer=optimizer, epoch=epoch, num_epochs=EPOCHS, criteria=criteria, device=DEVICE, log_aggr=100)\n",
        "        # scheduler.step(epoch = epoch)\n",
        "        # Phá băng layers sau epoch đầu tiên\n",
        "        if not frozen:\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            scheduler0.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Validate on validation set\n",
        "        acc, f1 = validate(valid_loader, phoBERT_cls, device=DEVICE)\n",
        "        print('Epoch {} validation: acc: {:.4f}, f1: {:.4f} \\n'.format(epoch, acc, f1))\n",
        "\n",
        "        # Store best model checkpoint\n",
        "        ckpt_dict = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': phoBERT_cls.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        # Save model checkpoint into 'latest_checkpoint.pth.tar'\n",
        "        torch.save(ckpt_dict, MODEL_LAST_CKPT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for fold 4\n",
            "Load model pretrained!\n",
            "loading archive file PhoBERT_base_fairseq\n",
            "| dictionary: 64000 types\n",
            "Load BPE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Init Optimizer, scheduler, criteria\n",
            "Epoch:  0\n",
            "[TRAIN] epoch 1/20  observation 0/4502 batch loss: 2.3103 (avg 2.3103),  avg acc: 0.0000, avg f1: 0.0000, (14.41 im/s)\n",
            "[TRAIN] epoch 1/20  observation 100/4502 batch loss: 2.6594 (avg 2.4117),  avg acc: 0.1155, avg f1: 0.0517, (15.49 im/s)\n",
            "[TRAIN] epoch 1/20  observation 200/4502 batch loss: 2.0821 (avg 2.3707),  avg acc: 0.1045, avg f1: 0.0459, (15.22 im/s)\n",
            "[TRAIN] epoch 1/20  observation 300/4502 batch loss: 2.4174 (avg 2.3453),  avg acc: 0.1174, avg f1: 0.0542, (14.92 im/s)\n",
            "[TRAIN] epoch 1/20  observation 400/4502 batch loss: 2.4809 (avg 2.3335),  avg acc: 0.1239, avg f1: 0.0571, (15.05 im/s)\n",
            "[TRAIN] epoch 1/20  observation 500/4502 batch loss: 2.3568 (avg 2.3269),  avg acc: 0.1238, avg f1: 0.0567, (15.15 im/s)\n",
            "[TRAIN] epoch 1/20  observation 600/4502 batch loss: 2.2165 (avg 2.3171),  avg acc: 0.1301, avg f1: 0.0614, (15.43 im/s)\n",
            "[TRAIN] epoch 1/20  observation 700/4502 batch loss: 2.3694 (avg 2.3125),  avg acc: 0.1291, avg f1: 0.0609, (15.23 im/s)\n",
            "[TRAIN] epoch 1/20  observation 800/4502 batch loss: 2.2227 (avg 2.3070),  avg acc: 0.1294, avg f1: 0.0614, (15.47 im/s)\n",
            "[TRAIN] epoch 1/20  observation 900/4502 batch loss: 2.4972 (avg 2.2996),  avg acc: 0.1308, avg f1: 0.0628, (14.97 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1000/4502 batch loss: 2.2085 (avg 2.2985),  avg acc: 0.1290, avg f1: 0.0619, (15.05 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1100/4502 batch loss: 2.2106 (avg 2.2964),  avg acc: 0.1308, avg f1: 0.0627, (15.03 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1200/4502 batch loss: 2.2998 (avg 2.2935),  avg acc: 0.1324, avg f1: 0.0639, (15.21 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1300/4502 batch loss: 2.2586 (avg 2.2910),  avg acc: 0.1344, avg f1: 0.0647, (15.43 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1400/4502 batch loss: 2.0195 (avg 2.2893),  avg acc: 0.1369, avg f1: 0.0664, (15.01 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1500/4502 batch loss: 2.2185 (avg 2.2874),  avg acc: 0.1382, avg f1: 0.0670, (15.11 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1600/4502 batch loss: 2.1833 (avg 2.2854),  avg acc: 0.1411, avg f1: 0.0684, (15.04 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1700/4502 batch loss: 2.3772 (avg 2.2822),  avg acc: 0.1425, avg f1: 0.0688, (15.61 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1800/4502 batch loss: 2.1823 (avg 2.2801),  avg acc: 0.1442, avg f1: 0.0702, (15.03 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1900/4502 batch loss: 2.0841 (avg 2.2781),  avg acc: 0.1445, avg f1: 0.0705, (15.12 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2000/4502 batch loss: 2.0886 (avg 2.2765),  avg acc: 0.1443, avg f1: 0.0703, (15.06 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2100/4502 batch loss: 2.4768 (avg 2.2767),  avg acc: 0.1456, avg f1: 0.0709, (14.92 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2200/4502 batch loss: 2.1478 (avg 2.2762),  avg acc: 0.1458, avg f1: 0.0708, (15.06 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2300/4502 batch loss: 2.3609 (avg 2.2763),  avg acc: 0.1463, avg f1: 0.0712, (15.12 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2400/4502 batch loss: 2.2311 (avg 2.2765),  avg acc: 0.1449, avg f1: 0.0703, (15.03 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2500/4502 batch loss: 2.1588 (avg 2.2752),  avg acc: 0.1462, avg f1: 0.0708, (15.17 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2600/4502 batch loss: 2.4681 (avg 2.2748),  avg acc: 0.1459, avg f1: 0.0706, (15.00 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2700/4502 batch loss: 2.1340 (avg 2.2745),  avg acc: 0.1463, avg f1: 0.0707, (15.13 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2800/4502 batch loss: 2.1751 (avg 2.2749),  avg acc: 0.1460, avg f1: 0.0706, (15.24 im/s)\n",
            "[TRAIN] epoch 1/20  observation 2900/4502 batch loss: 2.4564 (avg 2.2741),  avg acc: 0.1465, avg f1: 0.0708, (14.98 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3000/4502 batch loss: 1.9886 (avg 2.2742),  avg acc: 0.1462, avg f1: 0.0707, (14.95 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3100/4502 batch loss: 2.1094 (avg 2.2729),  avg acc: 0.1476, avg f1: 0.0715, (15.07 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3200/4502 batch loss: 2.2159 (avg 2.2722),  avg acc: 0.1484, avg f1: 0.0718, (15.09 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3300/4502 batch loss: 2.3574 (avg 2.2725),  avg acc: 0.1486, avg f1: 0.0717, (15.22 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3400/4502 batch loss: 2.0553 (avg 2.2714),  avg acc: 0.1496, avg f1: 0.0726, (15.03 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3500/4502 batch loss: 2.4763 (avg 2.2713),  avg acc: 0.1496, avg f1: 0.0725, (15.22 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3600/4502 batch loss: 2.2705 (avg 2.2717),  avg acc: 0.1494, avg f1: 0.0722, (15.22 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3700/4502 batch loss: 2.3662 (avg 2.2716),  avg acc: 0.1492, avg f1: 0.0721, (15.45 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3800/4502 batch loss: 2.4145 (avg 2.2711),  avg acc: 0.1495, avg f1: 0.0725, (15.13 im/s)\n",
            "[TRAIN] epoch 1/20  observation 3900/4502 batch loss: 2.0096 (avg 2.2709),  avg acc: 0.1496, avg f1: 0.0727, (15.05 im/s)\n",
            "[TRAIN] epoch 1/20  observation 4000/4502 batch loss: 2.1369 (avg 2.2711),  avg acc: 0.1490, avg f1: 0.0724, (15.20 im/s)\n",
            "[TRAIN] epoch 1/20  observation 4100/4502 batch loss: 2.3201 (avg 2.2716),  avg acc: 0.1482, avg f1: 0.0721, (15.25 im/s)\n",
            "[TRAIN] epoch 1/20  observation 4200/4502 batch loss: 2.3649 (avg 2.2712),  avg acc: 0.1489, avg f1: 0.0724, (15.10 im/s)\n",
            "[TRAIN] epoch 1/20  observation 4300/4502 batch loss: 2.1837 (avg 2.2704),  avg acc: 0.1495, avg f1: 0.0726, (14.97 im/s)\n",
            "[TRAIN] epoch 1/20  observation 4400/4502 batch loss: 2.2823 (avg 2.2701),  avg acc: 0.1492, avg f1: 0.0724, (15.42 im/s)\n",
            "[TRAIN] epoch 1/20  observation 4500/4502 batch loss: 2.3849 (avg 2.2698),  avg acc: 0.1496, avg f1: 0.0726, (15.07 im/s)\n",
            "Epoch 0 validation: acc: 0.1551, f1: 0.1550 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  5%|▌         | 1/20 [31:43<10:02:51, 1903.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "[TRAIN] epoch 2/20  observation 0/4502 batch loss: 2.2254 (avg 2.2254),  avg acc: 0.1667, avg f1: 0.0476, (14.77 im/s)\n",
            "[TRAIN] epoch 2/20  observation 100/4502 batch loss: 2.2175 (avg 2.2692),  avg acc: 0.1518, avg f1: 0.0825, (15.01 im/s)\n",
            "[TRAIN] epoch 2/20  observation 200/4502 batch loss: 2.2196 (avg 2.2623),  avg acc: 0.1542, avg f1: 0.0749, (15.20 im/s)\n",
            "[TRAIN] epoch 2/20  observation 300/4502 batch loss: 2.3200 (avg 2.2563),  avg acc: 0.1512, avg f1: 0.0734, (14.88 im/s)\n",
            "[TRAIN] epoch 2/20  observation 400/4502 batch loss: 2.3139 (avg 2.2549),  avg acc: 0.1513, avg f1: 0.0743, (15.00 im/s)\n",
            "[TRAIN] epoch 2/20  observation 500/4502 batch loss: 2.3222 (avg 2.2571),  avg acc: 0.1500, avg f1: 0.0751, (15.11 im/s)\n",
            "[TRAIN] epoch 2/20  observation 600/4502 batch loss: 2.3370 (avg 2.2621),  avg acc: 0.1506, avg f1: 0.0734, (15.53 im/s)\n",
            "[TRAIN] epoch 2/20  observation 700/4502 batch loss: 2.2806 (avg 2.2639),  avg acc: 0.1495, avg f1: 0.0721, (15.21 im/s)\n",
            "[TRAIN] epoch 2/20  observation 800/4502 batch loss: 2.2306 (avg 2.2643),  avg acc: 0.1494, avg f1: 0.0710, (15.25 im/s)\n",
            "[TRAIN] epoch 2/20  observation 900/4502 batch loss: 2.0489 (avg 2.2629),  avg acc: 0.1506, avg f1: 0.0727, (15.14 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1000/4502 batch loss: 2.1215 (avg 2.2641),  avg acc: 0.1510, avg f1: 0.0730, (15.13 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1100/4502 batch loss: 2.2850 (avg 2.2644),  avg acc: 0.1518, avg f1: 0.0729, (15.14 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1200/4502 batch loss: 2.2908 (avg 2.2647),  avg acc: 0.1525, avg f1: 0.0729, (15.49 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1300/4502 batch loss: 2.4822 (avg 2.2634),  avg acc: 0.1533, avg f1: 0.0741, (15.58 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1400/4502 batch loss: 2.2544 (avg 2.2629),  avg acc: 0.1524, avg f1: 0.0734, (15.29 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1500/4502 batch loss: 2.3057 (avg 2.2631),  avg acc: 0.1513, avg f1: 0.0732, (15.01 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1600/4502 batch loss: 2.1348 (avg 2.2623),  avg acc: 0.1529, avg f1: 0.0738, (15.18 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1700/4502 batch loss: 2.3609 (avg 2.2627),  avg acc: 0.1531, avg f1: 0.0738, (15.60 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1800/4502 batch loss: 2.1783 (avg 2.2624),  avg acc: 0.1536, avg f1: 0.0745, (15.33 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1900/4502 batch loss: 2.3599 (avg 2.2621),  avg acc: 0.1544, avg f1: 0.0747, (15.08 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2000/4502 batch loss: 2.4153 (avg 2.2614),  avg acc: 0.1557, avg f1: 0.0755, (15.23 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2100/4502 batch loss: 2.2263 (avg 2.2617),  avg acc: 0.1560, avg f1: 0.0754, (15.61 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2200/4502 batch loss: 2.2423 (avg 2.2612),  avg acc: 0.1559, avg f1: 0.0752, (15.07 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2300/4502 batch loss: 2.2178 (avg 2.2612),  avg acc: 0.1548, avg f1: 0.0747, (14.98 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2400/4502 batch loss: 2.2597 (avg 2.2612),  avg acc: 0.1556, avg f1: 0.0751, (15.26 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2500/4502 batch loss: 2.3653 (avg 2.2615),  avg acc: 0.1558, avg f1: 0.0750, (15.24 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2600/4502 batch loss: 2.0051 (avg 2.2612),  avg acc: 0.1557, avg f1: 0.0746, (14.99 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2700/4502 batch loss: 2.1284 (avg 2.2606),  avg acc: 0.1556, avg f1: 0.0745, (15.03 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2800/4502 batch loss: 2.3040 (avg 2.2599),  avg acc: 0.1553, avg f1: 0.0747, (15.02 im/s)\n",
            "[TRAIN] epoch 2/20  observation 2900/4502 batch loss: 2.1391 (avg 2.2592),  avg acc: 0.1555, avg f1: 0.0747, (15.00 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3000/4502 batch loss: 2.0829 (avg 2.2585),  avg acc: 0.1561, avg f1: 0.0752, (15.08 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3100/4502 batch loss: 2.3332 (avg 2.2591),  avg acc: 0.1556, avg f1: 0.0749, (15.55 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3200/4502 batch loss: 2.3623 (avg 2.2591),  avg acc: 0.1555, avg f1: 0.0746, (15.09 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3300/4502 batch loss: 2.0588 (avg 2.2591),  avg acc: 0.1554, avg f1: 0.0746, (15.20 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3400/4502 batch loss: 2.5262 (avg 2.2595),  avg acc: 0.1557, avg f1: 0.0746, (15.16 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3500/4502 batch loss: 2.2194 (avg 2.2587),  avg acc: 0.1560, avg f1: 0.0749, (15.65 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3600/4502 batch loss: 2.4758 (avg 2.2588),  avg acc: 0.1560, avg f1: 0.0751, (15.15 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3700/4502 batch loss: 2.5184 (avg 2.2592),  avg acc: 0.1552, avg f1: 0.0745, (15.36 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3800/4502 batch loss: 2.2652 (avg 2.2590),  avg acc: 0.1557, avg f1: 0.0747, (15.12 im/s)\n",
            "[TRAIN] epoch 2/20  observation 3900/4502 batch loss: 2.3716 (avg 2.2593),  avg acc: 0.1555, avg f1: 0.0748, (15.25 im/s)\n",
            "[TRAIN] epoch 2/20  observation 4000/4502 batch loss: 2.1175 (avg 2.2594),  avg acc: 0.1556, avg f1: 0.0746, (14.91 im/s)\n",
            "[TRAIN] epoch 2/20  observation 4100/4502 batch loss: 2.3628 (avg 2.2593),  avg acc: 0.1557, avg f1: 0.0748, (15.50 im/s)\n",
            "[TRAIN] epoch 2/20  observation 4200/4502 batch loss: 2.5231 (avg 2.2594),  avg acc: 0.1559, avg f1: 0.0747, (15.11 im/s)\n",
            "[TRAIN] epoch 2/20  observation 4300/4502 batch loss: 2.1223 (avg 2.2590),  avg acc: 0.1559, avg f1: 0.0745, (14.86 im/s)\n",
            "[TRAIN] epoch 2/20  observation 4400/4502 batch loss: 2.1588 (avg 2.2590),  avg acc: 0.1552, avg f1: 0.0742, (15.02 im/s)\n",
            "[TRAIN] epoch 2/20  observation 4500/4502 batch loss: 2.1360 (avg 2.2591),  avg acc: 0.1552, avg f1: 0.0741, (14.96 im/s)\n",
            "Epoch 1 validation: acc: 0.1567, f1: 0.1566 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 10%|█         | 2/20 [1:03:29<9:31:17, 1904.29s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  2\n",
            "[TRAIN] epoch 3/20  observation 0/4502 batch loss: 2.2758 (avg 2.2758),  avg acc: 0.0000, avg f1: 0.0000, (14.24 im/s)\n",
            "[TRAIN] epoch 3/20  observation 100/4502 batch loss: 2.3369 (avg 2.2542),  avg acc: 0.1452, avg f1: 0.0655, (14.97 im/s)\n",
            "[TRAIN] epoch 3/20  observation 200/4502 batch loss: 2.0919 (avg 2.2412),  avg acc: 0.1592, avg f1: 0.0733, (15.08 im/s)\n",
            "[TRAIN] epoch 3/20  observation 300/4502 batch loss: 2.3493 (avg 2.2458),  avg acc: 0.1561, avg f1: 0.0701, (15.10 im/s)\n",
            "[TRAIN] epoch 3/20  observation 400/4502 batch loss: 2.1747 (avg 2.2504),  avg acc: 0.1579, avg f1: 0.0707, (15.65 im/s)\n",
            "[TRAIN] epoch 3/20  observation 500/4502 batch loss: 2.3183 (avg 2.2528),  avg acc: 0.1567, avg f1: 0.0691, (15.07 im/s)\n",
            "[TRAIN] epoch 3/20  observation 600/4502 batch loss: 2.2660 (avg 2.2523),  avg acc: 0.1550, avg f1: 0.0686, (15.50 im/s)\n",
            "[TRAIN] epoch 3/20  observation 700/4502 batch loss: 2.3860 (avg 2.2552),  avg acc: 0.1503, avg f1: 0.0659, (15.18 im/s)\n",
            "[TRAIN] epoch 3/20  observation 800/4502 batch loss: 2.4388 (avg 2.2553),  avg acc: 0.1511, avg f1: 0.0655, (14.97 im/s)\n",
            "[TRAIN] epoch 3/20  observation 900/4502 batch loss: 2.0492 (avg 2.2536),  avg acc: 0.1552, avg f1: 0.0675, (15.21 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1000/4502 batch loss: 2.0077 (avg 2.2530),  avg acc: 0.1562, avg f1: 0.0685, (15.21 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1100/4502 batch loss: 2.1336 (avg 2.2539),  avg acc: 0.1536, avg f1: 0.0672, (15.48 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1200/4502 batch loss: 2.1012 (avg 2.2546),  avg acc: 0.1533, avg f1: 0.0670, (15.12 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1300/4502 batch loss: 2.1907 (avg 2.2561),  avg acc: 0.1518, avg f1: 0.0661, (15.23 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1400/4502 batch loss: 2.1982 (avg 2.2566),  avg acc: 0.1530, avg f1: 0.0670, (15.00 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1500/4502 batch loss: 2.3044 (avg 2.2573),  avg acc: 0.1533, avg f1: 0.0672, (15.03 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1600/4502 batch loss: 2.3001 (avg 2.2573),  avg acc: 0.1553, avg f1: 0.0687, (14.97 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1700/4502 batch loss: 2.2693 (avg 2.2582),  avg acc: 0.1534, avg f1: 0.0678, (15.21 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1800/4502 batch loss: 2.2378 (avg 2.2579),  avg acc: 0.1547, avg f1: 0.0685, (15.11 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1900/4502 batch loss: 2.3656 (avg 2.2561),  avg acc: 0.1563, avg f1: 0.0697, (15.01 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2000/4502 batch loss: 2.0996 (avg 2.2557),  avg acc: 0.1558, avg f1: 0.0693, (15.56 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2100/4502 batch loss: 2.1228 (avg 2.2555),  avg acc: 0.1557, avg f1: 0.0692, (15.10 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2200/4502 batch loss: 2.1269 (avg 2.2545),  avg acc: 0.1556, avg f1: 0.0691, (15.17 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2300/4502 batch loss: 2.3005 (avg 2.2545),  avg acc: 0.1565, avg f1: 0.0697, (14.95 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2400/4502 batch loss: 2.2740 (avg 2.2548),  avg acc: 0.1565, avg f1: 0.0696, (15.55 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2500/4502 batch loss: 2.2559 (avg 2.2559),  avg acc: 0.1565, avg f1: 0.0694, (15.68 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2600/4502 batch loss: 2.3119 (avg 2.2569),  avg acc: 0.1558, avg f1: 0.0692, (15.57 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2700/4502 batch loss: 2.2693 (avg 2.2571),  avg acc: 0.1550, avg f1: 0.0687, (15.44 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2800/4502 batch loss: 2.2305 (avg 2.2562),  avg acc: 0.1560, avg f1: 0.0694, (15.03 im/s)\n",
            "[TRAIN] epoch 3/20  observation 2900/4502 batch loss: 2.1507 (avg 2.2565),  avg acc: 0.1559, avg f1: 0.0692, (15.07 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3000/4502 batch loss: 2.2645 (avg 2.2560),  avg acc: 0.1562, avg f1: 0.0695, (15.08 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3100/4502 batch loss: 2.3722 (avg 2.2561),  avg acc: 0.1561, avg f1: 0.0695, (15.23 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3200/4502 batch loss: 2.2835 (avg 2.2555),  avg acc: 0.1570, avg f1: 0.0699, (15.52 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3300/4502 batch loss: 2.3481 (avg 2.2558),  avg acc: 0.1567, avg f1: 0.0697, (14.93 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3400/4502 batch loss: 2.3637 (avg 2.2561),  avg acc: 0.1569, avg f1: 0.0698, (15.02 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3500/4502 batch loss: 2.3028 (avg 2.2565),  avg acc: 0.1567, avg f1: 0.0698, (15.45 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3600/4502 batch loss: 2.3619 (avg 2.2560),  avg acc: 0.1568, avg f1: 0.0698, (14.93 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3700/4502 batch loss: 2.4081 (avg 2.2560),  avg acc: 0.1568, avg f1: 0.0696, (15.20 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3800/4502 batch loss: 2.0547 (avg 2.2558),  avg acc: 0.1572, avg f1: 0.0699, (15.02 im/s)\n",
            "[TRAIN] epoch 3/20  observation 3900/4502 batch loss: 2.3855 (avg 2.2558),  avg acc: 0.1568, avg f1: 0.0697, (15.41 im/s)\n",
            "[TRAIN] epoch 3/20  observation 4000/4502 batch loss: 2.2651 (avg 2.2563),  avg acc: 0.1567, avg f1: 0.0696, (15.35 im/s)\n",
            "[TRAIN] epoch 3/20  observation 4100/4502 batch loss: 2.2085 (avg 2.2563),  avg acc: 0.1567, avg f1: 0.0695, (15.46 im/s)\n",
            "[TRAIN] epoch 3/20  observation 4200/4502 batch loss: 2.4297 (avg 2.2558),  avg acc: 0.1571, avg f1: 0.0698, (15.12 im/s)\n",
            "[TRAIN] epoch 3/20  observation 4300/4502 batch loss: 2.4325 (avg 2.2560),  avg acc: 0.1567, avg f1: 0.0696, (15.10 im/s)\n",
            "[TRAIN] epoch 3/20  observation 4400/4502 batch loss: 1.9734 (avg 2.2559),  avg acc: 0.1569, avg f1: 0.0696, (15.03 im/s)\n",
            "[TRAIN] epoch 3/20  observation 4500/4502 batch loss: 2.2264 (avg 2.2557),  avg acc: 0.1571, avg f1: 0.0697, (15.53 im/s)\n",
            "Epoch 2 validation: acc: 0.1567, f1: 0.1566 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 15%|█▌        | 3/20 [1:35:17<8:59:52, 1905.42s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  3\n",
            "[TRAIN] epoch 4/20  observation 0/4502 batch loss: 2.2080 (avg 2.2080),  avg acc: 0.1667, avg f1: 0.0476, (14.44 im/s)\n",
            "[TRAIN] epoch 4/20  observation 100/4502 batch loss: 2.1499 (avg 2.2527),  avg acc: 0.1749, avg f1: 0.0819, (15.01 im/s)\n",
            "[TRAIN] epoch 4/20  observation 200/4502 batch loss: 2.2836 (avg 2.2595),  avg acc: 0.1559, avg f1: 0.0702, (15.24 im/s)\n",
            "[TRAIN] epoch 4/20  observation 300/4502 batch loss: 2.2633 (avg 2.2563),  avg acc: 0.1561, avg f1: 0.0703, (14.97 im/s)\n",
            "[TRAIN] epoch 4/20  observation 400/4502 batch loss: 2.3912 (avg 2.2544),  avg acc: 0.1579, avg f1: 0.0699, (15.69 im/s)\n",
            "[TRAIN] epoch 4/20  observation 500/4502 batch loss: 2.2087 (avg 2.2522),  avg acc: 0.1593, avg f1: 0.0702, (15.16 im/s)\n",
            "[TRAIN] epoch 4/20  observation 600/4502 batch loss: 2.2016 (avg 2.2543),  avg acc: 0.1597, avg f1: 0.0706, (15.58 im/s)\n",
            "[TRAIN] epoch 4/20  observation 700/4502 batch loss: 2.2379 (avg 2.2557),  avg acc: 0.1598, avg f1: 0.0709, (15.22 im/s)\n",
            "[TRAIN] epoch 4/20  observation 800/4502 batch loss: 2.3873 (avg 2.2541),  avg acc: 0.1596, avg f1: 0.0706, (15.11 im/s)\n",
            "[TRAIN] epoch 4/20  observation 900/4502 batch loss: 2.3636 (avg 2.2536),  avg acc: 0.1602, avg f1: 0.0712, (14.89 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1000/4502 batch loss: 2.2601 (avg 2.2538),  avg acc: 0.1605, avg f1: 0.0714, (14.97 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1100/4502 batch loss: 2.2800 (avg 2.2540),  avg acc: 0.1602, avg f1: 0.0713, (14.86 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1200/4502 batch loss: 2.4918 (avg 2.2551),  avg acc: 0.1583, avg f1: 0.0704, (15.02 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1300/4502 batch loss: 2.1981 (avg 2.2540),  avg acc: 0.1590, avg f1: 0.0707, (14.96 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1400/4502 batch loss: 2.3863 (avg 2.2545),  avg acc: 0.1582, avg f1: 0.0701, (15.13 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1500/4502 batch loss: 2.5242 (avg 2.2543),  avg acc: 0.1587, avg f1: 0.0707, (15.35 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1600/4502 batch loss: 2.1757 (avg 2.2545),  avg acc: 0.1575, avg f1: 0.0702, (15.10 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1700/4502 batch loss: 2.3702 (avg 2.2544),  avg acc: 0.1585, avg f1: 0.0713, (15.02 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1800/4502 batch loss: 2.3270 (avg 2.2531),  avg acc: 0.1594, avg f1: 0.0717, (15.42 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1900/4502 batch loss: 2.2287 (avg 2.2537),  avg acc: 0.1597, avg f1: 0.0719, (15.21 im/s)\n",
            "[TRAIN] epoch 4/20  observation 2000/4502 batch loss: 2.1949 (avg 2.2531),  avg acc: 0.1603, avg f1: 0.0726, (14.98 im/s)\n",
            "[TRAIN] epoch 4/20  observation 2100/4502 batch loss: 2.2651 (avg 2.2527),  avg acc: 0.1602, avg f1: 0.0726, (15.35 im/s)\n",
            "[TRAIN] epoch 4/20  observation 2200/4502 batch loss: 2.1796 (avg 2.2524),  avg acc: 0.1601, avg f1: 0.0726, (15.04 im/s)\n",
            "[TRAIN] epoch 4/20  observation 2300/4502 batch loss: 2.1079 (avg 2.2526),  avg acc: 0.1598, avg f1: 0.0725, (14.94 im/s)\n",
            "[TRAIN] epoch 4/20  observation 2400/4502 batch loss: 2.3712 (avg 2.2527),  avg acc: 0.1597, avg f1: 0.0723, (14.93 im/s)\n",
            "[TRAIN] epoch 4/20  observation 2500/4502 batch loss: 2.3661 (avg 2.2522),  avg acc: 0.1603, avg f1: 0.0726, (14.99 im/s)\n",
            "[TRAIN] epoch 4/20  observation 2600/4502 batch loss: 2.2636 (avg 2.2525),  avg acc: 0.1603, avg f1: 0.0728, (15.24 im/s)\n",
            "[TRAIN] epoch 4/20  observation 2700/4502 batch loss: 2.2600 (avg 2.2518),  avg acc: 0.1609, avg f1: 0.0730, (15.16 im/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm7yU4WmThYW",
        "colab_type": "text"
      },
      "source": [
        "Thời gian huấn luyện sẽ khá lâu, các bạn nên kiên nhẫn chờ đợi. Ngoài cách fine tuning model từ fairseq như trên, các bạn có thể tham khảo thêm một cách khác của [PhoBERT-Sentiment-Classification Khoi Nguyen](https://github.com/suicao/PhoBert-Sentiment-Classification/) thực hiện fine tuning dựa trên pretrain model huấn luyện từ transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnekqLWIHzxy",
        "colab_type": "text"
      },
      "source": [
        "## 9. Huấn luyện RoBERTa trên dữ liệu của bạn\n",
        "\n",
        "Ngoài ra chúng ta có thể tự huấn luyện pretrain model BERT dựa trên kiến trúc RoBERTa theo hướng dẫn tại [roberta - README](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md). Việc thực hiện khá đơn giản, bài đã khá dài nên mình gửi link cho bạn đọc tự nghiên cứu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn9Gyep0Sf-V",
        "colab_type": "text"
      },
      "source": [
        "## 10. Tổng kết\n",
        "\n",
        "Như vậy mình đã giới thiệu với các bạn rất nhiều các ứng dụng khác nhau trong việc áp dụng các model pretrain RoBERTa. Trong đó có các tác vụ như: Filling mask, Classification và tìm từ đồng nghĩa.\n",
        "\n",
        "Qua bài viết này các bạn có thể nắm bắt được các phương tiện và công cụ mới trong việc tiếp cận các bài toán của NLP. Đừng quên like và share bài viết này nếu bạn cảm thấy kiến thức mình chia sẻ là hữu ích với bạn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXMcycjrT_Af",
        "colab_type": "text"
      },
      "source": [
        "## 11. Tài liệu\n",
        "\n",
        "1. [faiseq](https://github.com/pytorch/fairseq)\n",
        "\n",
        "2. [RoBERTa](https://github.com/pytorch/fairseq/tree/master/examples/roberta)\n",
        "\n",
        "3. [transformers](https://github.com/huggingface/transformers)\n",
        "\n",
        "4. [PhoBERT-Sentiment-Classification](https://github.com/suicao/PhoBert-Sentiment-Classification/)\n",
        "\n",
        "5. [PhoBERT](https://github.com/VinAIResearch/PhoBERT)"
      ]
    }
  ]
}