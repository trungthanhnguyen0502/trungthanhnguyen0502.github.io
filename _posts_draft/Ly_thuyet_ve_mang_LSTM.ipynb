{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I. Mạng nơ ron truy hồi (RNN - Recurrent Neural Network)\n\nTrong lý thuyết về ngôn ngữ, ngữ nghĩa của một câu được tạo thành từ mối liên kết của những từ trong câu theo một cấu trúc ngữ pháp. Nếu xét từng từ một đứng riêng lẻ ta không thể hiểu được nội dụng của toàn bộ câu, nhưng dựa trên những từ xung quanh ta có thể hiểu được trọn vẹn một câu nói. Như vậy cần phải có một kiến trúc đặc biệt hơn cho các mạng nơ ron biểu diễn ngôn ngữ nhằm mục đích liên kết các từ liền trước với các từ ở hiện tại để tạo ra mối liên hệ xâu chuỗi. Mạng nơ ron truy hồi đã được thiết kế đặc biệt để giải quyết yêu cầu này:\n\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png\" width=\"100px\">\n</img>\n> **Hình 1: Mạng nơ ron truy hồi với vòng lặp**\n\n\nHình trên biểu diễn kiến trúc của một mạng nơ ron truy hồi. Trong kiến trúc này mạng nơ ron sử dụng một đầu vào $x_t$ và trả ra đầu ra là một $h_t$. Đầu vào được đấu với một thân mạng nơ ron $A$ có tính chất truy hồi và từ thân này sẽ trả ra kết quả $h_t$.\n\nVòng lặp $A$ ở thân mạng nơ ron là điểm mấu chốt trong nguyên lý hoạt động của mạng nơ ron truy hồi. Đây là chuỗi sao chép nhiều lần của cùng một kiến trúc nhằm cho phép các thành phần có thể kết nối liền mạch với nhau theo mô hình chuỗi. Đầu ra của vòng lặp trước chính là đầu vào của vòng lặp sau. Nếu trải phẳng thân mạng nơ ron $A$ ta sẽ thu được một mô hình dạng:\n\n<img src='http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png' width=\"400px\"></img>\n> **Hình 2: Cấu trúc trải phẳng của mạng nơ ron truy hồi**\n\nKiến trúc mạng nơ ron truy hồi này tỏ ra khá thành công trong các tác vụ của deep learning như: Nhận diện dọng nói (*speech recognition*), các mô hình ngôn ngữ, mô hình dịch, chú thích hình ảnh (*image captioning*),....\n\n# II. Hạn chế của mạng nơ ron truy hồi\n\nMột trong những điểm đặc biệt của RNN đó là nó có khả năng kết nối các thông tin liền trước với nhiệm vụ hiện tại, chẳng hạn như trong câu văn: 'học sinh đang tới *trường học*'. Dường như trong một ngữ cảnh ngắn hạn, từ *trường học* có thể được dự báo ngay tức thì mà không cần thêm các thông tin từ những câu văn khác gần đó. Tuy nhiên có những tình huống đòi hỏi phải có nhiều thông tin hơn chẳng hạn như: 'hôm qua Bống đi học nhưng không mang áo mưa. Trên đường đi học trời mưa. Cặp sách của Bống bị *ướt*'. Chúng ta cần phải học để tìm ra từ *ướt* ở một ngữ cảnh dài hơn so với chỉ 1 câu. Tức là cần phải biết các sự kiện trước đó như *trời mưa*, *không mang áo mưa* để suy ra sự kiện bị *ướt*. Những sự liên kết ngữ nghĩa dài như vậy được gọi là `phụ thuộc dài hạn` (*long-term dependencies*).\nVề mặt lý thuyết mạng RNN có thể giải quyết được những sự phụ thuộc trong dài hạn. Tuy nhiên trên thực tế RNN lại cho thấy khả năng học trong dài hạn kém hơn. Để hiểu thêm lý do tại sao mạng RNN lại không có khả năng học trong dài hạn cùng đọc bài [Leanring Long - Term Dependencies with Gradient Descent is Difficult](http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf). Một trong những nguyên nhân chính được giải thích đó là sự triệt tiêu đạo hàm của hàm cost function sẽ diễn ra khi trải quả chuỗi dài các tính toán truy hồi. Một phiên bản mới của mạng RNN là mạng LSTM ra đời nhằm khắc phục hiện tường này nhờ một cơ chế đặc biệt."},{"metadata":{},"cell_type":"markdown","source":"# III. Mạng trí nhớ ngắn hạn định hướng dài hạn (LSTM - Long short term memory)\n\nMạng *trí nhớ ngắn hạn định hướng dài hạn* còn được viết tắt là LSTM làm một kiến trúc đặc biệt của RNN có khả năng học được sự phục thuộc trong dài hạn (*long-term dependencies*) được giới thiệu bởi [Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf). Kiến trúc này đã được phổ biến và sử dụng rộng rãi cho tới ngày nay. LSTM đã tỏ ra khắc phục được rất nhiều những hạn chế của RNN trước đây về triệt tiêu đạo hàm. Tuy nhiên cấu trúc của chúng có phần phức tạp hơn mặc dù vẫn dữ được tư tưởng chính của RNN là sự sao chép các kiến trúc theo dạng chuỗi. \n\nMột mạng RNN tiêu chuẩn sẽ có kiến trúc rất đơn giản chẳng hạn như đối với kiến trúc gồm một tầng ẩn là hàm tanh như bên dưới.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" width=\"600px\"></img>\n> **Hình 3: Sự lặp lại kiến trúc module trong mạng RNN chứa một tầng ẩn**\n\n\nLSTM cũng có một chuỗi dạng như thế nhưng phần kiến trúc lặp lại có cấu trúc khác biệt hơn. Thay vì chỉ có một tầng đơn, chúng có tới 4 tầng ẩn tương tác với nhau theo một cấu trúc đặc biệt.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=\"600px\">\n</img>\n>**Hình 4: Sự lặp lại kiến trúc module trong mạng LSTM chứa 4 tầng ẩn tương tác**\n\nCác kí hiệu có thể diễn giải như sau:\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png\" width=\"600px\"></img>\n>**Hình 5: Diễn giải các kí hiệu trong đồ thị mạng nơ ron (áp dụng chung cho toàn bộ bài)**\n\nTrong sở đồ tính toán trên, mỗi một phép tính sẽ triển khai trên một véc tơ. Trong đó hình tròn màu hồng biểu diễn một toán tử đối với véc tơ như phép cộng véc tơ, phép nhân véc tơ. Màu vàng thể hiện hàm activation mà mạng nơ ron sử dụng để học trong tầng ẩn. Kí hiệu 2 đường thẳng nhập vào thể hiện phép chập kết quả trong khi kí hiệu 2 đường thẳng rẽ nhánh thể hiện cho nội dung véc tơ trước đó được sao chép để đi tới một phần khác của mạng nơ ron."},{"metadata":{},"cell_type":"markdown","source":"# IV. Ý tưởng đằng sau LSTM\n\nÝ tưởng chính của LSTM là thành phần ô trạng thái (cell state) được thể hiện qua đường chạy ngang qua đỉnh đồ thị như hình vẽ bên dưới:\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\" width=\"800px\"></img>\n>**Hình 6: Đường đi của ô trạng thái (cell state) trong mạng LSTM**\n\nÔ trạng thái là một dạng băng chuyền chạy thẳng xuyên suốt toàn bộ chuỗi với chỉ một vài tương tác tuyến tính nhỏ giúp cho thông tin có thể chuyền dọc theo đồ thị mạng nơ ron ổn định.\n\nLSTM có khả năng xóa và thêm thông tin vào ô trạng thái và điều chỉnh các luồng thông tin này thông qua các cấu trúc gọi là cổng.\n\nCổng là cơ chế đặc biệt để điều chỉnh luồng thông tin đi qua. Chúng được tổng hợp bởi một tầng ẩn của hàm activation sigmoid và với một toán tử nhân như đồ thị.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png\" width=\"100px\"></img>\n>**Hình 7: Một cổng của hàm sigmoid trong LSTM**\n\nHàm sigmoid sẽ cho đầu ra là một giá trị xác xuất nằm trong khoảng từ 0 đến 1, thể hiện rằng có bao nhiêu phần thông tin sẽ đi qua cổng. Giá trị bằng 0 ngụ ý rằng không cho phép thông tin nào đi qua, giá trị bằng 1 sẽ cho toàn bộ thông tin đi qua.\n\nMột mạng LSTM sẽ có 3 cổng có kiến trúc dạng này để bảo vệ và kiểm soát các ô trạng thái."},{"metadata":{},"cell_type":"markdown","source":"# V. Thứ tự các bước của LSTM\n\nBước đầu tiên trong LSTM sẽ quyết định xem thông tin nào chúng ta sẽ cho phép đi qua ô trạng thái (cell state). Nó được kiểm soát bởi hàm sigmoid trong một tầng gọi là cổng quên (*forget gate layer*). Đầu tiên nó nhận đầu vào là 2 giá trị $h_{t-1}$ và $x_t$ và trả về một giá trị nằm trong khoảng 0 và 1 cho mỗi giá trị của ô trạng thái $C_{t-1}$. Nếu giá trị bằng 1 thể hiện 'giữ toàn bộ thông tin' và bằng 0 thể hiện 'bỏ qua toàn bộ chúng'.\n\nTrở lại ví dụ về ngôn ngữ, chúng ta đang cố gắng dự báo từ tiếp theo dựa trên toàn bộ những từ trước đó. Trong những bài toán như vậy, ô trạng thái có thể bao gồm loại của chủ ngữ hiện tại, để cho đại từ chính xác có thể được sử dụng (anh ấy, cô ấy hay tôi, bạn). Khi chúng ta nhìn thấy một chủ ngữ mới, chúng ta muốn quên đi loại của một chủ ngữ cũ.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" width=\"800px\"></img>\n>**Hình 8: Tầng cổng quên (*forget gate layer*)**\n\nBước tiếp theo chúng ta sẽ quyết định loại thông tin nào sẽ được lưu trữ trong ô trạng thái. Chúng ta sẽ có 2 phần. Phần đầu tiên là một tầng ẩn của hàm sigmoid được gọi là tầng cổng vào (*input gate layer*) quyết định chúng ta sẽ cập nhật giá trị nào. Tiếp theo, tầng ẩn hàm tanh sẽ tạo ra một véc tơ của một giá trị trạng thái mới $\\tilde{C}_t$ mà có thể được thêm vào trạng thái. Trong bước tiếp theo chúng ta sẽ kết hợp kết quả của tầng này để tạo thành một cập nhật cho trạng thái.\n\nTrong ví dụ của mô hình ngôn ngữ của chúng ta, chúng ta muốn thêm loại của một chủ ngữ mới vào ô trạng thái để thay thế phần trạng thái cũ mà chúng ta muốn quên đi.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" width=\"800px\"></img>\n>**Hình 9: Cập nhật giá trị cho ô trạng thái bằng cách kết hợp 2 kết quả từ tầng cổng vào và tẩng ẩn hàm tanh**\n\nĐây là thời điểm để cập nhật một ô trạng thái cũ, $C_{t-1}$ sang một trạng thái mới $C_t$. Những bước trước đó đã quyết định làm cái gì, chúng ta chỉ cần thực hiện nó.\n\nChúng ta nhân trạng thái cũ với $f_t$, quên những thứ chúng ta quyết định để quên sớm hơn. Chúng ta thêm $i_t * \\tilde{C}_t$. Đây là một giá trị đề cử mới được tính toán bằng bao nhiêu chúng ta quyết định để cập nhật mỗi giá trị trạng thái.\n\nTrong trường hợp của mô hình ngôn ngữ, đó là nơi chúng ta đã thực tế loại bỏ thông tin về các chủ đề của loại cũ và thêm thông tin mới khi chúng ta quyết định trong bước trước đó.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" width=\"800px\"></img>\n>**Hình 10: Ô trạng thái mới**\n\nCuối cùng chúng ta cần quyết định xem chúng ta sẽ có gì ở đầu ra. Kết quả ở đầu ra sẽ dựa trên ô trạng thái của chúng ta, nhưng sẽ là một phiên bản được lọc. Đầu tiên, chúng ta chạy qua một tầng sigmoid nơi quyết định phần nào của ô trạng thái sẽ ở đầu ra. Sau đó, chúng ta đưa ô trạng thái qua hàm tanh (để chuyển giá trị về khoảng -1 và 1) và nhân nó với đầu ra của một cổng sigmoid, do đó chúng ta chỉ trả ra những phần mà chúng ta quyết định.\n\nĐối với những mô hình ngôn ngữ, bởi vì nó chỉ nhìn vào chủ ngữ, nó có thể muốn thông tin đầu ra liên quan đến một động từ, trong trường hợp dự báo điều sẽ xảy ra tiếp theo. Chẳng hạn, Đó có thể trả về có hay không chủ ngữ là số ít hoặc số nhiều, để chúng ta biết dạng của động từ có thể được liên kết với những từ tiếp theo.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" width=\"800px\"></img>\n>**Hình 11: Điều chỉnh thông tin ở đầu ra thông qua hàm tanh**"},{"metadata":{},"cell_type":"markdown","source":"# VI. Các biến thể của LSTM\n\nNhững gì mà chúng ta vừa mổ tả cho đến giờ là một mạng LSTM rất thông thường. Nhưng không phải toàn bộ LSTM đều tương tự như trên. Trên thực tế, có vẻ như hầu hết mọi bài báo liên quan đến LSTM đều sử dụng những version khác nhau đôi chút. Sự khác biệt là rất nhỏ nhưng rất đáng để đề cập một ít trong số nhứng kiến trúc này.\n\nMột trong những biến thể nối tiếng nhất của LSTM được giới thiệu bởi [Gers & Schmidhuber (2000)](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf) thêm một kết nối ống tiểu (*peehole connection*) để các cổng có thể kết nối trực tiếp đến các ô trạng thái.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png\" width=\"800px\"></img>\n>**Hình 12: Kết nối ống tiểu (*peehole*) liên kết trực tiếp ô trạng thái với các cổng**\n\nMột biến thể khác là sử dụng cặp đôi cổng vào và cổng ra. Thay vì quyết định riêng rẽ bỏ qua thông tin nào và thêm mới thông tin nào, chúng ta sẽ quyết định chúng đồng thời. Chúng ta chỉ quên chúng khi chúng ta sẽ nhập vào một vài thông tin mới. Chúng ta chỉ nhập vào một vài giá trị mới cho trạng thái khi chúng ta quên những thứ cũ hơn.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png\" width=\"800px\"></img>\n>**Hình 13: Cấu trúc điều chỉnh thêm mới và bỏ qua thông tin đồng thời**\n\nMột dạng biến thể khá mạnh khác của LSTM là cổng truy hồi đơn vị [(*Gated Recurrent Unit - GRU*)](https://arxiv.org/pdf/1406.1078v3.pdf) được giới thiệu bởi Cho, et al. (2014). Nó kết hợp cổng quên và cổng vào thành một cổng đơn gọi là cổng cập nhật (*update gate*). Nó cũng nhập các ô trạng thái và trạng thái ẩn và thực hiện một số thay đổi khác. Kết quả của mô hình đơn giản hơn nhiều so với mô hình LSTM chuẩn, và đã trở nên khá phổ biến.\n\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\" width=\"800px\"></img>\n>**Hình 14: Cấu trúc cổng truy hồi đơn vị (*GRU - Gated Recurrent Unit*)**\n\nChỉ có một số lượng nhỏ những biến thể LSTM là đáng chú ý. Rất nhiều những biến thể khác, như [kiến trúc cổng sâu RNN](https://arxiv.org/pdf/1508.03790v2.pdf) (*Depth Gated RNN*) của Yao, et al. (2015) hay kiến trúc [đồng hồ RNN](https://arxiv.org/pdf/1402.3511v1.pdf) (*Clockword RNN*) của Koutnik, et al. (2014) nhằm giải quyết vấn đề phụ thuộc dài hạn (*long - term depencies*).\n\nVậy những biến thể nào là tốt nhất? Greff, et al. (2015) thực hiện một [so sánh biến thể LSTM](https://arxiv.org/pdf/1503.04069.pdf) và nhận thấy rằng tất cả chúng đều giống nhau. Jozefowicz, et al. (2015) đã thử nghiệm hơn mười nghìn kiến trúc RNN, tìm thấy một số hoạt động tốt hơn LSTM trên một số nhiệm vụ nhất định. Trong khi đó  Jozefowicz, et al. (2015) thực hiện [kiểm tra trên hơn 1000 kiến trúc RNN](http://proceedings.mlr.press/v37/jozefowicz15.pdf) khác nhau và nhận thấy một số hoạt động tốt hơn so với LSTM trong một vài tác vụ cụ thể."},{"metadata":{},"cell_type":"markdown","source":"# VII. Kết luận\n\nTrước đó, tôi đã đề cập đến những kết quả đáng chú ý mà mọi người đang đạt được với RNNs. Về cơ bản tất cả những điều này đều đạt được bằng cách sử dụng LSTM. Chúng thực sự làm việc tốt hơn rất nhiều cho hầu hết các nhiệm vụ!\n\nĐược viết dưới dạng một hệ phương trình, LSTM trông khá là phức tạp và có phần đáng sợ. Thông qua các bước diễn giải tuần tự nguyên lý hoạt động của nó tôi hi vọng sẽ khiến chúng trở nên dễ tiếp cận hơn.\n\nLSTM là một bước đột phá lớn mà ở đó chúng ta đã khắc phục được những hạn chế ở RNN đó là khả năng phụ thuộc dài hạn. Một số kĩ thuật học Attention gần đây được kết hợp với LSTM đã tạo ra những kết quả khá bất ngờ trong các tác vụ dịch máy cũng như phân loại nội dung, trích lọc thông tin,.... Các mô hình dịch máy của google đã ứng dụng kiểu kết hợp này trong các bài toán dịch thuật của mình và đã cải thiện được nội dung bản dịch một cách đáng kể."},{"metadata":{},"cell_type":"markdown","source":"# VIII. Thực hành mô hình sinh từ tự động\n## Xây dựng mô hình trên level kí tự\n\nBên dưới chúng ta sẽ áp dụng mô hình LSTM trong việc dự báo từ tiếp theo của một đoạn hoặc câu văn dựa vào bối cảnh của từ là những từ liền trước nó.\n\nDữ liệu được sử dụng là bộ truyện [alice ở xứ sở kỳ diệu](https://gist.githubusercontent.com/phillipj/4944029/raw/75ba2243dd5ec2875f629bf5d79f6c1e4b5a8b46/alice_in_wonderland.txt) đã được nhà xuất bản publish nên không vi phạm bản quyền. Mô hình dự báo sẽ được xây dựng trên level kí tự. Bên dưới, chúng ta sẽ đọc dữ liệu và chuyển các kí tự về in thường để giảm thiểu kích thước bộ mã hóa mà vẫn đảm bảo được nội dung văn bản. Dữ liệu được lưu trong kernel là file `wonderland.txt`.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nimport os\n\nfilename = '../input/wonderland.txt'\nraw_text = open(filename).read().lower()","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bên dưới chúng ta sẽ cùng tạo ra một dictionary gồm 59 kí tự được sử dụng trong bộ truyện. Key của các  kí tự là số thứ tự của chúng trong dictionary. Trong sơ đồ thiết kế mạng nơ ron chúng ta sẽ mã hóa một kí tự bằng một vector đơn vị sao cho phần từ 1 sẽ xuất hiện tại vị trí của key trong bộ từ điển và 0 là các phần tử còn lại."},{"metadata":{"trusted":true},"cell_type":"code","source":"chars = sorted(list(set(raw_text)))\nchar_to_int = dict((c, i) for i, c in enumerate(chars))\nprint('number of letters: ', len(char_to_int))\nprint(char_to_int)","execution_count":30,"outputs":[{"output_type":"stream","text":"number of letters:  59\n{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '?': 27, '@': 28, '[': 29, ']': 30, '_': 31, 'a': 32, 'b': 33, 'c': 34, 'd': 35, 'e': 36, 'f': 37, 'g': 38, 'h': 39, 'i': 40, 'j': 41, 'k': 42, 'l': 43, 'm': 44, 'n': 45, 'o': 46, 'p': 47, 'q': 48, 'r': 49, 's': 50, 't': 51, 'u': 52, 'v': 53, 'w': 54, 'x': 55, 'y': 56, 'z': 57, '\\ufeff': 58}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Chúng ta nhận thấy rằng mục đích chỉ là dự báo từ tiếp theo do đó cần lọc bỏ những kí tự không quyết định đến nghĩa của 1 từ chẳng hạn như các dấu đặc biệt `#, $, *, @, /`. Như vậy, chúng ta sẽ cần một bước chuẩn hóa dữ liệu nhằm giảm thiểu nhiễu và số lượng các khả năng có thể ở đầu ra. Điều này sẽ giúp cải thiện chất lượng và độ chính xác trong dự báo của mô hình đáng kể. Việc chuẩn hóa sẽ bao gồm như sau:\n1. Chỉ giữ lại các kí tự chữ cái vì chúng có ảnh hưởng đến nội dung của 1 từ.\n2. Chỉ giữ lại các dấu câu là `., !, ?` vì chúng thể hiện các loại câu khác nhau và sẽ ảnh hưởng đến từ tiếp theo khi dự báo. Chẳng hạn nếu dấu câu là `?` thì khả năng cao từ tiếp theo sẽ là `yes` hoặc `no`. Dấu câu là `.` thì từ tiếp theo có thể là một đại từ nhân xưng `i, you, we, they, he, she, it`.\n3. Giữ lại các dấu `,' '` vì chúng giúp tách các từ và tách các thành phần câu.\n4. Chuẩn hóa lại các các chữ số về 1 chữ số duy nhất là 0 vì các con số là ngẫu nhiên và không dự báo được. Chúng ta chỉ có thể dự báo ở vị trí nào có khả năng là số.\n5. Các kí tự nằm ngoài số liệt kê trên sẽ đưa vào nhóm `unk` tức unknown."},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nstring.ascii_lowercase\n# string.digits\n# string.punctuation\nchars_new = list(string.ascii_lowercase) + ['0', '.', ',', ' ', '!', '?', 'unk']\nchars_to_int = dict((v, k) for k, v in enumerate(chars_new))\nint_to_chars = dict((k, v) for k, v in enumerate(chars_new))\nprint('character to int:', chars_to_int)\nprint('int to character:', int_to_chars)\n# def _clean_char(text):\n#     return 1","execution_count":31,"outputs":[{"output_type":"stream","text":"character to int: {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '0': 26, '.': 27, ',': 28, ' ': 29, '!': 30, '?': 31, 'unk': 32}\nint to character: {0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: '0', 27: '.', 28: ',', 29: ' ', 30: '!', 31: '?', 32: 'unk'}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_chars = len(raw_text)\nn_vocab = len(chars_new)\nprint('Total characters: ', n_chars)\nprint('Total Vocab: ', n_vocab)","execution_count":32,"outputs":[{"output_type":"stream","text":"Total characters:  163693\nTotal Vocab:  33\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Như vậy sau chuẩn hóa văn bản của chúng ta sẽ bao gồm 163693 từ và 32 kí tự. Tiếp theo chúng ta sẽ viết một hàm chuyển hóa một câu thành một vector chỉ số các kí tự."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _encode_sen(text):\n    text = text.lower()\n    sen_vec = []\n    for let in text:\n        if let in chars_new[:-1]:\n            idx = chars_to_int[let]\n        else:\n            idx = chars_to_int['unk']\n        sen_vec.append(idx)\n    return sen_vec\n\nx_test = _encode_sen('Alice is a wonderful story. #')\nprint(x_test)","execution_count":33,"outputs":[{"output_type":"stream","text":"[0, 11, 8, 2, 4, 29, 8, 18, 29, 0, 29, 22, 14, 13, 3, 4, 17, 5, 20, 11, 29, 18, 19, 14, 17, 24, 27, 29, 32]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _decode_sen(vec):\n    text = []\n    for i in vec:\n        let = int_to_chars[i]\n        text.append(let)\n    text = ''.join(text)\n    return text\n\n_decode_sen(x_test)","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"'alice is a wonderful story. unk'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Tiếp theo chúng ta sẽ tạo ra một window input với độ dài là 100 kí tự. Mục đích của chúng ta là dự báo kí tự tiếp theo từ 100 kí tự đầu vào. Mỗi một phiên dự báo chúng ta sẽ tịnh tiến window input lên 1 kí tự để thu được các kí tự dự báo liên tiếp nhau và từ đó ghép lại thành một câu hoàn chỉnh."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the dataset of input to output pairs encoded as integers\nseq_length = 100\ndataX = []\ndataY = []\nfor i in range(0, n_chars - seq_length, 1):\n    # Lấy ra 100 kí tự liền trước\n    seq_in = raw_text[i:i + seq_length]\n    # Lấy ra kí tự liền sau 100 kí tự đó\n    seq_out = raw_text[i + seq_length]\n    dataX.append(_encode_sen(seq_in))\n    dataY.append(_encode_sen(seq_out)[0])\n    n_patterns = len(dataX)\nprint(\"Total Patterns: \", n_patterns)","execution_count":35,"outputs":[{"output_type":"stream","text":"Total Patterns:  163593\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Tiếp theo chúng ta cần chuẩn hóa đầu vào `X` thành một vector 3 chiều `samples, time steps, features`. Trong đó:\n\n1. samples: Số lượng các quan sát đầu vào (tức số lượng cửa sổ window 100 length).\n2. time steps: Độ dài của cửa sổ window để dự báo từ tiếp theo (ở đây là 100).\n3. features: Số lượng các biến được sử dụng (do mỗi kí tự được mã hóa là 1 số nên features là 1, trong trường hợp kí tự được mã hóa dưới dạng vecto thì số lượng feature có thể dài hơn)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape X to be [samples, time steps, features]\nX_train = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n# normalize\nX_train = X_train / float(n_vocab)\n# one hot encode the output variable\ny_train = np_utils.to_categorical(dataY)\nprint('X [samples, time steps, features] shape: ', X_train.shape)\nprint('Y shape: ', y_train.shape)","execution_count":72,"outputs":[{"output_type":"stream","text":"X [samples, time steps, features] shape:  (163593, 100, 1)\nY shape:  (163593, 33)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(X_train))\nprint(type(y_train))","execution_count":73,"outputs":[{"output_type":"stream","text":"<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Thống kê số lượng các kí tự theo nhóm."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sn\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (10, 5))\nsn.countplot(np.array(dataY))\nplt.xticks(np.arange(32),np.array(chars_new))","execution_count":74,"outputs":[{"output_type":"execute_result","execution_count":74,"data":{"text/plain":"([<matplotlib.axis.XTick at 0x7fa09d660f28>,\n  <matplotlib.axis.XTick at 0x7fa09d660860>,\n  <matplotlib.axis.XTick at 0x7fa09d6605c0>,\n  <matplotlib.axis.XTick at 0x7fa0b5aa1390>,\n  <matplotlib.axis.XTick at 0x7fa0b5aa1860>,\n  <matplotlib.axis.XTick at 0x7fa0b5aa1c18>,\n  <matplotlib.axis.XTick at 0x7fa0b5aab198>,\n  <matplotlib.axis.XTick at 0x7fa0b5aab668>,\n  <matplotlib.axis.XTick at 0x7fa0b5aabb70>,\n  <matplotlib.axis.XTick at 0x7fa0b5ab3128>,\n  <matplotlib.axis.XTick at 0x7fa0b5ab35c0>,\n  <matplotlib.axis.XTick at 0x7fa0b5ab3ac8>,\n  <matplotlib.axis.XTick at 0x7fa0b5ab3cf8>,\n  <matplotlib.axis.XTick at 0x7fa0b5ab3668>,\n  <matplotlib.axis.XTick at 0x7fa0b5aabb38>,\n  <matplotlib.axis.XTick at 0x7fa0b5aa1940>,\n  <matplotlib.axis.XTick at 0x7fa09d5a5748>,\n  <matplotlib.axis.XTick at 0x7fa0b5ab92b0>,\n  <matplotlib.axis.XTick at 0x7fa0b5ab9ac8>,\n  <matplotlib.axis.XTick at 0x7fa0b5ab9cf8>,\n  <matplotlib.axis.XTick at 0x7fa09d5c9518>,\n  <matplotlib.axis.XTick at 0x7fa09d5c9a20>,\n  <matplotlib.axis.XTick at 0x7fa09d5c9f28>,\n  <matplotlib.axis.XTick at 0x7fa09d5c9898>,\n  <matplotlib.axis.XTick at 0x7fa0b5ab9a90>,\n  <matplotlib.axis.XTick at 0x7fa0b5aab748>,\n  <matplotlib.axis.XTick at 0x7fa09d4257b8>,\n  <matplotlib.axis.XTick at 0x7fa09d425cc0>,\n  <matplotlib.axis.XTick at 0x7fa09d672208>,\n  <matplotlib.axis.XTick at 0x7fa09d672710>,\n  <matplotlib.axis.XTick at 0x7fa09d672c18>,\n  <matplotlib.axis.XTick at 0x7fa0b5ac2198>],\n <a list of 32 Text xticklabel objects>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x360 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Xây dựng một kiến trúc model gồm một layer LSTM kết nối tới 1 layer Dropout và kết nối tới Dense layer ở cuối."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(256, input_shape = (X_train.shape[1], X_train.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y.shape[1], activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","execution_count":75,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_5 (LSTM)                (None, 256)               264192    \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 33)                8481      \n=================================================================\nTotal params: 272,673\nTrainable params: 272,673\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = 'weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'\ncheckpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')\ncallback_list = [checkpoint]","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs = 5, batch_size = 128, validation_split=0.33, callbacks = callback_list, verbose = 1)","execution_count":77,"outputs":[{"output_type":"stream","text":"Train on 109607 samples, validate on 53986 samples\nEpoch 1/5\n109607/109607 [==============================] - 408s 4ms/step - loss: 2.8991 - acc: 0.1820 - val_loss: 2.8783 - val_acc: 0.1869\n\nEpoch 00001: val_acc improved from -inf to 0.18686, saving model to weights-improvement-01-2.8991.hdf5\nEpoch 2/5\n109607/109607 [==============================] - 406s 4ms/step - loss: 2.7600 - acc: 0.2182 - val_loss: 2.8217 - val_acc: 0.2149\n\nEpoch 00002: val_acc improved from 0.18686 to 0.21489, saving model to weights-improvement-02-2.7600.hdf5\nEpoch 3/5\n109607/109607 [==============================] - 404s 4ms/step - loss: 2.6864 - acc: 0.2389 - val_loss: 2.7827 - val_acc: 0.2152\n\nEpoch 00003: val_acc improved from 0.21489 to 0.21520, saving model to weights-improvement-03-2.6864.hdf5\nEpoch 4/5\n109607/109607 [==============================] - 405s 4ms/step - loss: 2.6287 - acc: 0.2517 - val_loss: 2.7331 - val_acc: 0.2350\n\nEpoch 00004: val_acc improved from 0.21520 to 0.23499, saving model to weights-improvement-04-2.6287.hdf5\nEpoch 5/5\n109607/109607 [==============================] - 403s 4ms/step - loss: 2.5696 - acc: 0.2664 - val_loss: 2.7077 - val_acc: 0.2429\n\nEpoch 00005: val_acc improved from 0.23499 to 0.24291, saving model to weights-improvement-05-2.5696.hdf5\n","name":"stdout"},{"output_type":"execute_result","execution_count":77,"data":{"text/plain":"<keras.callbacks.History at 0x7fa09d77fd30>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Dự báo kết quả từ tiếp theo từ một tập hợp kí tự đầu vào."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nbase_word = 'Alice was beginning to get very tired of sitting by her sister on the bank'\n\ndef _predict_let(text, len_sen = 1):\n    text_for = []\n    for i in range(len_sen):\n        x_input = np.array(_encode_sen(text)[-100:])/float(n_vocab)\n        if x_input.shape[0] < 100:\n            x_input = np.concatenate((np.zeros(100-x_input.shape[0]), x_input), axis = 0)\n        x_input = np.expand_dims(np.expand_dims(x_input, -1), 0)\n        y_prob = model.predict(x_input)\n        y_let = int_to_chars[np.argmax(y_prob, axis = 1)[0]]\n        text = text + y_let\n    return text[len_sen:]\n\n_predict_let(base_word, 100)","execution_count":78,"outputs":[{"output_type":"execute_result","execution_count":78,"data":{"text/plain":"'nd the  and the  and the  and the  and the  and the  and the  and the  and t'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Kiến trúc BiLSTM."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.layers import LSTM, Input, Bidirectional\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.metrics import categorical_accuracy\n\n#import spacy, and spacy english model\n# spacy is used to work on text\nimport spacy\nnlp = spacy.load('en')\n\n#import other libraries\nimport numpy as np\nimport random\nimport sys\nimport os\nimport time\nimport codecs\nimport collections\nfrom six.moves import cPickle\n\n#define parameters used in the tutorial\ndata_dir = '../input'# data directory containing raw texts\n# save_dir = 'save' # directory to store trained NN models\nfile_list = os.listdir('../input')\nvocab_file = os.path.join(\"words_vocab.pkl\")\nsequences_step = 1 #step to create sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_wordlist(doc):\n    wl = []\n    for word in doc:\n        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n            wl.append(word.text.lower())\n    return wl\n\nwordlist = []\n\nfor file_name in file_list:\n    input_file = os.path.join(data_dir, file_name)\n    #read data\n    with codecs.open(input_file, \"r\") as f:\n        data = f.read()\n        \n    #create sentences\n    doc = nlp(data)\n    wl = create_wordlist(doc)\n    wordlist = wordlist + wl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32 # minibatch size\nnum_epochs = 50 # number of epochs\n\ncallbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n#fit the model\nhistory = md.fit(X, y,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 epochs=num_epochs,\n                 callbacks=callbacks,\n                 validation_split=0.1)\n\n#save the model\nmd.save(save_dir + \"/\" + 'my_model_generate_sentences.h5')batch_size = 32 # minibatch size\nnum_epochs = 50 # number of epochs\n\ncallbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n#fit the model\nhistory = md.fit(X, y,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 epochs=num_epochs,\n                 callbacks=callbacks,\n                 validation_split=0.1)\n\n#save the model\nmd.save(save_dir + \"/\" + 'my_model_generate_sentences.h5')batch_size = 32 # minibatch size\nnum_epochs = 50 # number of epochs\n\ncallbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n#fit the model\nhistory = md.fit(X, y,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 epochs=num_epochs,\n                 callbacks=callbacks,\n                 validation_split=0.1)\n\n#save the model\nmd.save(save_dir + \"/\" + 'my_model_generate_sentences.h5')len(wordlist[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count the number of words\nword_counts = collections.Counter(wordlist)\n\n# Mapping from index to word : that's the vocabulary\nvocabulary_inv = [x[0] for x in word_counts.most_common()]\nvocabulary_inv = list(sorted(vocabulary_inv))\n\n# Mapping from word to index\nvocab = {x: i for i, x in enumerate(vocabulary_inv)}\nwords = [x[0] for x in word_counts.most_common()]\n\n#size of the vocabulary\nvocab_size = len(words)\nprint(\"vocab size: \", vocab_size)\n\n#save the words and vocabulary\nwith open(os.path.join(vocab_file), 'wb') as f:\n    cPickle.dump((words, vocab, vocabulary_inv), f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create sequences\nsequences = []\nnext_words = []\nfor i in range(0, len(wordlist) - seq_length, sequences_step):\n    sequences.append(wordlist[i: i + seq_length])\n    next_words.append(wordlist[i + seq_length])\n\nprint('nb sequences:', len(sequences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\ny = np.zeros((len(sequences), vocab_size), dtype=np.bool)\nfor i, sentence in enumerate(sequences):\n    for t, word in enumerate(sentence):\n        X[i, t, vocab[word]] = 1\n    y[i, vocab[next_words[i]]] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X shape: ', X.shape)\nprint('y shape: ', y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bidirectional_lstm_model(seq_length, vocab_size):\n    print('Build LSTM model.')\n    model = Sequential()\n    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n    model.add(Dropout(0.6))\n    model.add(Dense(vocab_size))\n    model.add(Activation('softmax'))\n    \n    optimizer = Adam(lr=learning_rate)\n    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n    print(\"model built!\")\n    return model\n\nrnn_size = 256 # size of RNN\nseq_length = 100 # sequence length\nlearning_rate = 0.001 #learning rate\n\nmd = bidirectional_lstm_model(seq_length, vocab_size)\nmd.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32 # minibatch size\nnum_epochs = 50 # number of epochs\n\ncallbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n           ModelCheckpoint(filepath=\"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n#fit the model\nhistory = md.fit(X, y,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 epochs=num_epochs,\n                 callbacks=callbacks,\n                 validation_split=0.1)\n\n#save the model\nmd.save(\"/\" + 'my_model_generate_sentences.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XI. Tài liệu\n\n1. [Understanding - LSTMs - Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs)\n\n2. [Leanring Long - Term Dependencies with Gradient Descent is Difficult](http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf)\n\n3. [(*Gated Recurrent Unit - GRU*)](https://arxiv.org/pdf/1406.1078v3.pdf)\n\n4. [so sánh biến thể LSTM](https://arxiv.org/pdf/1503.04069.pdf)\n\n5. [kiến trúc cổng sâu RNN (*Depth Gated RNN*)](https://arxiv.org/pdf/1508.03790v2.pdf) "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}