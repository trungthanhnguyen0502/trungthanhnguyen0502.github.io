{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSDModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiixSCqopl3P",
        "colab_type": "text"
      },
      "source": [
        "# 1. Giới thiệu SSD model\n",
        "\n",
        "Như chúng ta đã biết có khá nhiều các lớp mô hình khác nhau trong phát hiện vật thể. Các kiến trúc cũ hơn có thể kể đến như R-CNN, fast R-CNN. Đặc điểm của chúng là tốc độ xử lý thấp, không đáp ứng được trong việc object dection realtime. Các mạng start-of-art hơn như SSD và YOLOv2, YOLOv3 là những kiến trúc có tốc độ xử lý nhanh mà vẫn đảm bảo về độ chính xác nhờ những thay đổi trong kiến trúc mạng nhằm gói gọn quá trình phát hiện và phân loại vật thể trong 1 lần và cắt bớt được các xử lý không cần thiết.\n",
        "\n",
        "Trong bài này chúng ta sẽ tìm hiểu về kiến trúc, cách thức hoạt động đi kèm ví dụ thực tiễn để xây dựng một lớp mô hình SSD (Single Shot MultiBox Detector) trong object detection. \n",
        "\n",
        "Cũng giống như hầu hết các kiến trúc object detection khác, đầu vào của SSD là tọa độ bounding box của vật thể (hay còn gọi là offsets của bounding box) và nhãn của vật thể chứa trong bounding box. Điểm đặc biệt làm nên tốc độ của SSD model là mô hình sử dụng một mạng neural duy nhất. Cách tiếp cận của nó dựa trên việc nhận diện vật thể trong các features map (là một output shape 3D của một mạng deep CNN sau khi bỏ các fully connected layers cuối) có độ phân giải khác nhau. Mô hình sẽ tạo ra một lưới các ô vuông gọi là grid cells trên các feature map, mỗi ô được gọi là một cell và từ tâm của mỗi cell xác định một tợp hợp các boxes mặc định (default boxes) để dự đoán khung hình có khả năng bao quanh vật thể.  Tại thời điểm dự báo, mạng neural sẽ trả về 2 giá trị đó là: phân phối xác suất nhãn của vật thể chứa trong bounding box và một tọa độ gọi là offsets của bounding box. Quá trình huấn luyện cũng là quá trình tinh chỉnh xác suất nhãn và bounding box về đúng với các giá trị ground truth input của mô hình (gồm nhãn và offsets bounding box). \n",
        "\n",
        "Thêm nữa, network được kết hợp bởi rất nhiều các feature map với những độ phân giải khác nhau để giúp phát hiện được những vật thể có kích thước to và nhỏ. Trái với mô hình fast R-CNN, SSD bỏ qua bước sản sinh ra một mặt nạ region proposal network để đề xuất vùng vật thể. Thay vào đó tất cả quá trình phát hiện vật thể và phân loại vật thể được thực hiện trong cùng 1 mạng. Bản thân tên của mô hình - Single Shot MultiBox Detector cũng nói lên được rằng mô hình sử dụng nhiều khung hình box với tỷ lệ scales khác nhau để nhận diện vùng vật thể và phân loại vật thể, giảm thiểu được bước tạo region proposal network so với fast R-CNN nên tăng tốc độ xử lý lên nhiều lần mà tốc độ xử lý vẫn đảm bảo.\n",
        "Bên dưới là bảng so sánh tốc độ running của các mô hình\n",
        "\n",
        "\n",
        "![](https://imgur.com/WQIvw2C.png)\n",
        "\n",
        "**Hình 1**: Bảng so sánh tốc độ xử lý và độ chính xác của các lớp model object detection (source: [table 7 - SSD: Single Shot MultiBox Detector\n",
        "](https://arxiv.org/pdf/1512.02325.pdf)). Ta thấy SSD512 (mô hình SSD với kích thước đầu vào của ảnh là 512 x 512 x 3) có độ chính xác mAP là cao nhất trong khi tốc độ xử lý gần đạt mức realtime là 22 fps.\n",
        "\n",
        "Tóm gọn lại mô hình SSD sẽ là kết hợp của 2 bước:\n",
        "\n",
        "* Trích xuất các feature map từ mạng CNN.\n",
        "* Áp dụng convolutional filters (hoặc kernel filters) để phát hiện vật thể trên các feature map có độ phân giải (revolution) khác nhau.\n",
        "\n",
        "# Một số định nghĩa\n",
        "* **scale**: Tỷ lệ chiều dài và chiều rộng so với khung hình gốc. VD: Nếu khung hình gốc có giá trị là $(w, h)$ thì sau scale khung hình mới có kích thước là $(sw, sh)$. Gía trị của $s$ thường nằm trong khoảng $s \\in (0, 1]$. Scale sẽ kết hợp với aspect ratio để nhận được các khung hình có tỷ lệ cạnh $w/h$ khác nhau.\n",
        "* **aspect ratio**: Tỷ lệ hướng, được đo bằng tỷ lệ giữa $w/h$ nhằm xác định hình dạng tương đối của khung hình bao chứa vật thể. Chẳng hạn nếu vật thể là người thường có aspect ratio = $1:3$ hoặc xe cộ nhìn từ phía trước là $1:1$.\n",
        "* **bounding box**: Khung hình bao chứa vật thể được xác định trong quá trình huấn luyện.\n",
        "* **ground truth box**: Khung hình được xác định trước từ bộ dữ liệu thông qua tọa độ $(c_x, c_y , w, h)$ giúp xác định vật thể.\n",
        "* **offsets**: Các tọa độ $(c_x, c_y , w, h)$ để xác định vật thể.\n",
        "* **positive matching prediction**: Khung được dự báo (predicted box) là vùng có vật thể là đúng, được xác định dựa trên tỷ lệ IoU > 0.5 giữa predicted box với ground truth box.\n",
        "* **negative matching prediction**: Khung được dự báo (predicted box) là vùng không chứa vật thể là đúng, cũng được xác định dựa trên IoU < 0.5 giữa predicted box với ground truth box.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN8mRjqn2H5M",
        "colab_type": "code",
        "outputId": "cc046e6f-f829-452d-b7c1-3f9bebe79e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "path = '/content/gdrive/My Drive/data/tiny-imagenet-200/tiny-imagenet-200'\n",
        "os.chdir(path)\n",
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wnids.txt', 'words.txt', 'val', 'test', 'train', 'models']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OGf4o4S3BTd",
        "colab_type": "code",
        "outputId": "d13ac798-a075-46bd-8945-d085f1896487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install d2l\n",
        "!pip install mxnet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting d2l\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/d3/e9b92d14359953524609ed21f41508cfbba3814313668b125c904c7cc6e2/d2l-0.10.1.tar.gz\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l) (1.16.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l) (3.0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l) (0.24.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (4.6.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.6.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (4.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (7.5.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->d2l) (2018.9)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l) (5.3.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l) (4.5.3)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l) (4.3.2)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (4.4.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.8.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (2.1.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (2.10.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (3.1.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (4.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (1.4.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l) (1.0.16)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l) (3.5.1)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->d2l) (41.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->d2l) (1.12.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->jupyter->d2l) (17.0.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (4.7.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (4.4.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->d2l) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->d2l) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->d2l) (0.6.0)\n",
            "Building wheels for collected packages: d2l\n",
            "  Building wheel for d2l (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for d2l: filename=d2l-0.10.1-cp36-none-any.whl size=16622 sha256=d71df6fc0f4aaad992c00ccb1d9ff8b36be5edeb1b38d7710691c13e75d15525\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/35/5d/df7f13dd4b012de688ffcc516091c6cd67c765dc64c9d1f43f\n",
            "Successfully built d2l\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.10.1\n",
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/08/186a7d67998f1e38d6d853c71c149820983c547804348f06727f552df20d/mxnet-1.5.0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\n",
            "\u001b[K     |████████████████████████████████| 25.4MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.16.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvt8dSxG3O5d",
        "colab_type": "code",
        "outputId": "bfed9692-355f-4382-a0ab-acf002f62865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.path.exists('./train/n04067472/images/n04067472_499.JPEG')\n",
        "# os.getcwd()\n",
        "# print(os.listdir('./train/n04067472/images/n04067472_499.JPEG'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm7Lva2U18tn",
        "colab_type": "code",
        "outputId": "5fd26157-3164-4cf5-9c34-94d6be39e832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import d2l\n",
        "from mxnet import contrib, image, nd\n",
        "\n",
        "img = image.imread('./train/n04067472/images/n04067472_499.JPEG')\n",
        "h, w = img.shape[0:2]\n",
        "h, w"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3msx6rwr36h3",
        "colab_type": "code",
        "outputId": "4ad757c0-be29-4686-c32c-ad876aee8ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "def display_anchors(fmap_w, fmap_h, s):\n",
        "    d2l.set_figsize((3.5, 2.5))\n",
        "    # The values from the first two dimensions will not affect the output\n",
        "    fmap = nd.zeros((1, 10, fmap_w, fmap_h))\n",
        "    anchors = contrib.nd.MultiBoxPrior(fmap, sizes=s, ratios=[1, 2, 0.5, 0.2, 0.6])\n",
        "    bbox_scale = nd.array((w, h, w, h))\n",
        "    d2l.show_bboxes(d2l.plt.imshow(img.asnumpy()).axes,\n",
        "                    anchors[0] * bbox_scale)\n",
        "    \n",
        "    \n",
        "display_anchors(fmap_w=4, fmap_h=4, s=[0.15])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"170.478125pt\" version=\"1.1\" viewBox=\"0 0 173.525 170.478125\" width=\"173.525pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 170.478125 \nL 173.525 170.478125 \nL 173.525 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 146.6 \nL 162.825 146.6 \nL 162.825 10.7 \nL 26.925 10.7 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p4374e689c5)\">\n    <image height=\"136\" id=\"image501b0444bd\" transform=\"scale(1 -1)translate(0 -136)\" width=\"136\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-10.6\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 34.781718 18.556718 \nL 55.166719 18.556718 \nL 55.166719 38.941719 \nL 34.781718 38.941719 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 30.559847 21.542033 \nL 59.388591 21.542033 \nL 59.388591 35.956405 \nL 30.559847 35.956405 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 37.767033 14.334846 \nL 52.181405 14.334846 \nL 52.181405 43.163591 \nL 37.767033 43.163591 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 40.415994 5.958095 \nL 49.532444 5.958095 \nL 49.532444 51.540344 \nL 40.415994 51.540344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 37.079142 15.590757 \nL 52.869296 15.590757 \nL 52.869296 41.90768 \nL 37.079142 41.90768 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 68.75672 18.556718 \nL 89.141717 18.556718 \nL 89.141717 38.941719 \nL 68.75672 38.941719 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 64.534847 21.542033 \nL 93.363591 21.542033 \nL 93.363591 35.956405 \nL 64.534847 35.956405 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 71.742035 14.334846 \nL 86.156403 14.334846 \nL 86.156403 43.163591 \nL 71.742035 43.163591 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 74.390994 5.958095 \nL 83.507444 5.958095 \nL 83.507444 51.540344 \nL 74.390994 51.540344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 71.054144 15.590757 \nL 86.844294 15.590757 \nL 86.844294 41.90768 \nL 71.054144 41.90768 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 102.73172 18.556718 \nL 123.116717 18.556718 \nL 123.116717 38.941719 \nL 102.73172 38.941719 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 98.509851 21.542033 \nL 127.338587 21.542033 \nL 127.338587 35.956405 \nL 98.509851 35.956405 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 105.717035 14.334846 \nL 120.131403 14.334846 \nL 120.131403 43.163591 \nL 105.717035 43.163591 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 108.365994 5.958095 \nL 117.482444 5.958095 \nL 117.482444 51.540344 \nL 108.365994 51.540344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 105.029144 15.590757 \nL 120.819294 15.590757 \nL 120.819294 41.90768 \nL 105.029144 41.90768 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 136.70672 18.556718 \nL 157.091717 18.556718 \nL 157.091717 38.941719 \nL 136.70672 38.941719 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 132.484851 21.542033 \nL 161.313587 21.542033 \nL 161.313587 35.956405 \nL 132.484851 35.956405 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 139.692035 14.334846 \nL 154.106403 14.334846 \nL 154.106403 43.163591 \nL 139.692035 43.163591 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 142.340994 5.958095 \nL 151.457444 5.958095 \nL 151.457444 51.540344 \nL 142.340994 51.540344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 139.004144 15.590757 \nL 154.794294 15.590757 \nL 154.794294 41.90768 \nL 139.004144 41.90768 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 34.781718 52.53172 \nL 55.166719 52.53172 \nL 55.166719 72.916717 \nL 34.781718 72.916717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 30.559847 55.517031 \nL 59.388591 55.517031 \nL 59.388591 69.931407 \nL 30.559847 69.931407 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 37.767033 48.309847 \nL 52.181405 48.309847 \nL 52.181405 77.138591 \nL 37.767033 77.138591 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 40.415994 39.933095 \nL 49.532444 39.933095 \nL 49.532444 85.515344 \nL 40.415994 85.515344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 37.079142 49.565757 \nL 52.869296 49.565757 \nL 52.869296 75.88268 \nL 37.079142 75.88268 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 68.75672 52.53172 \nL 89.141717 52.53172 \nL 89.141717 72.916717 \nL 68.75672 72.916717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 64.534847 55.517031 \nL 93.363591 55.517031 \nL 93.363591 69.931407 \nL 64.534847 69.931407 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 71.742035 48.309847 \nL 86.156403 48.309847 \nL 86.156403 77.138591 \nL 71.742035 77.138591 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 74.390994 39.933095 \nL 83.507444 39.933095 \nL 83.507444 85.515344 \nL 74.390994 85.515344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 71.054144 49.565757 \nL 86.844294 49.565757 \nL 86.844294 75.88268 \nL 71.054144 75.88268 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 102.73172 52.53172 \nL 123.116717 52.53172 \nL 123.116717 72.916717 \nL 102.73172 72.916717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 98.509851 55.517031 \nL 127.338587 55.517031 \nL 127.338587 69.931407 \nL 98.509851 69.931407 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 105.717035 48.309847 \nL 120.131403 48.309847 \nL 120.131403 77.138591 \nL 105.717035 77.138591 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 108.365994 39.933095 \nL 117.482444 39.933095 \nL 117.482444 85.515344 \nL 108.365994 85.515344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 105.029144 49.565757 \nL 120.819294 49.565757 \nL 120.819294 75.88268 \nL 105.029144 75.88268 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 136.70672 52.53172 \nL 157.091717 52.53172 \nL 157.091717 72.916717 \nL 136.70672 72.916717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 132.484851 55.517031 \nL 161.313587 55.517031 \nL 161.313587 69.931407 \nL 132.484851 69.931407 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 139.692035 48.309847 \nL 154.106403 48.309847 \nL 154.106403 77.138591 \nL 139.692035 77.138591 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 142.340994 39.933095 \nL 151.457444 39.933095 \nL 151.457444 85.515344 \nL 142.340994 85.515344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_42\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 139.004144 49.565757 \nL 154.794294 49.565757 \nL 154.794294 75.88268 \nL 139.004144 75.88268 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_43\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 34.781718 86.50672 \nL 55.166719 86.50672 \nL 55.166719 106.891717 \nL 34.781718 106.891717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 30.559847 89.492035 \nL 59.388591 89.492035 \nL 59.388591 103.906403 \nL 30.559847 103.906403 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 37.767033 82.284843 \nL 52.181405 82.284843 \nL 52.181405 111.113595 \nL 37.767033 111.113595 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 40.415994 73.908093 \nL 49.532444 73.908093 \nL 49.532444 119.490344 \nL 40.415994 119.490344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_47\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 37.079142 83.540757 \nL 52.869296 83.540757 \nL 52.869296 109.85768 \nL 37.079142 109.85768 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_48\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 68.75672 86.50672 \nL 89.141717 86.50672 \nL 89.141717 106.891717 \nL 68.75672 106.891717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_49\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 64.534847 89.492035 \nL 93.363591 89.492035 \nL 93.363591 103.906403 \nL 64.534847 103.906403 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_50\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 71.742035 82.284843 \nL 86.156403 82.284843 \nL 86.156403 111.113595 \nL 71.742035 111.113595 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_51\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 74.390994 73.908093 \nL 83.507444 73.908093 \nL 83.507444 119.490344 \nL 74.390994 119.490344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_52\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 71.054144 83.540757 \nL 86.844294 83.540757 \nL 86.844294 109.85768 \nL 71.054144 109.85768 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_53\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 102.73172 86.50672 \nL 123.116717 86.50672 \nL 123.116717 106.891717 \nL 102.73172 106.891717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_54\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 98.509851 89.492035 \nL 127.338587 89.492035 \nL 127.338587 103.906403 \nL 98.509851 103.906403 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_55\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 105.717035 82.284843 \nL 120.131403 82.284843 \nL 120.131403 111.113595 \nL 105.717035 111.113595 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_56\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 108.365994 73.908093 \nL 117.482444 73.908093 \nL 117.482444 119.490344 \nL 108.365994 119.490344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_57\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 105.029144 83.540757 \nL 120.819294 83.540757 \nL 120.819294 109.85768 \nL 105.029144 109.85768 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_58\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 136.70672 86.50672 \nL 157.091717 86.50672 \nL 157.091717 106.891717 \nL 136.70672 106.891717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_59\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 132.484851 89.492035 \nL 161.313587 89.492035 \nL 161.313587 103.906403 \nL 132.484851 103.906403 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_60\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 139.692035 82.284843 \nL 154.106403 82.284843 \nL 154.106403 111.113595 \nL 139.692035 111.113595 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_61\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 142.340994 73.908093 \nL 151.457444 73.908093 \nL 151.457444 119.490344 \nL 142.340994 119.490344 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_62\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 139.004144 83.540757 \nL 154.794294 83.540757 \nL 154.794294 109.85768 \nL 139.004144 109.85768 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_63\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 34.781718 120.48172 \nL 55.166719 120.48172 \nL 55.166719 140.866717 \nL 34.781718 140.866717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_64\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 30.559847 123.467035 \nL 59.388591 123.467035 \nL 59.388591 137.881403 \nL 30.559847 137.881403 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_65\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 37.767033 116.259843 \nL 52.181405 116.259843 \nL 52.181405 145.088595 \nL 37.767033 145.088595 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_66\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 40.415994 107.883093 \nL 49.532444 107.883093 \nL 49.532444 153.465336 \nL 40.415994 153.465336 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_67\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 37.079142 117.515757 \nL 52.869296 117.515757 \nL 52.869296 143.83268 \nL 37.079142 143.83268 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_68\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 68.75672 120.48172 \nL 89.141717 120.48172 \nL 89.141717 140.866717 \nL 68.75672 140.866717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_69\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 64.534847 123.467035 \nL 93.363591 123.467035 \nL 93.363591 137.881403 \nL 64.534847 137.881403 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_70\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 71.742035 116.259843 \nL 86.156403 116.259843 \nL 86.156403 145.088595 \nL 71.742035 145.088595 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_71\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 74.390994 107.883093 \nL 83.507444 107.883093 \nL 83.507444 153.465336 \nL 74.390994 153.465336 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_72\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 71.054144 117.515757 \nL 86.844294 117.515757 \nL 86.844294 143.83268 \nL 71.054144 143.83268 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_73\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 102.73172 120.48172 \nL 123.116717 120.48172 \nL 123.116717 140.866717 \nL 102.73172 140.866717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_74\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 98.509851 123.467035 \nL 127.338587 123.467035 \nL 127.338587 137.881403 \nL 98.509851 137.881403 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_75\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 105.717035 116.259843 \nL 120.131403 116.259843 \nL 120.131403 145.088595 \nL 105.717035 145.088595 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_76\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 108.365994 107.883093 \nL 117.482444 107.883093 \nL 117.482444 153.465336 \nL 108.365994 153.465336 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_77\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 105.029144 117.515757 \nL 120.819294 117.515757 \nL 120.819294 143.83268 \nL 105.029144 143.83268 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_78\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 136.70672 120.48172 \nL 157.091717 120.48172 \nL 157.091717 140.866717 \nL 136.70672 140.866717 \nz\n\" style=\"fill:none;stroke:#0000ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_79\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 132.484851 123.467035 \nL 161.313587 123.467035 \nL 161.313587 137.881403 \nL 132.484851 137.881403 \nz\n\" style=\"fill:none;stroke:#008000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_80\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 139.692035 116.259843 \nL 154.106403 116.259843 \nL 154.106403 145.088595 \nL 139.692035 145.088595 \nz\n\" style=\"fill:none;stroke:#ff0000;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_81\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 142.340994 107.883093 \nL 151.457444 107.883093 \nL 151.457444 153.465336 \nL 142.340994 153.465336 \nz\n\" style=\"fill:none;stroke:#bf00bf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_82\">\n    <path clip-path=\"url(#p4374e689c5)\" d=\"M 139.004144 117.515757 \nL 154.794294 117.515757 \nL 154.794294 143.83268 \nL 139.004144 143.83268 \nz\n\" style=\"fill:none;stroke:#00bfbf;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m901953a1a8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"27.986719\" xlink:href=\"#m901953a1a8\" y=\"146.6\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(24.805469 161.198437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.455469\" xlink:href=\"#m901953a1a8\" y=\"146.6\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(64.092969 161.198437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.924219\" xlink:href=\"#m901953a1a8\" y=\"146.6\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(106.561719 161.198437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"155.392969\" xlink:href=\"#m901953a1a8\" y=\"146.6\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(149.030469 161.198437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m48710636c1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m48710636c1\" y=\"11.761719\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 15.560937)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m48710636c1\" y=\"54.230469\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 58.029687)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m48710636c1\" y=\"96.699219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 40 -->\n      <g transform=\"translate(7.2 100.498437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m48710636c1\" y=\"139.167969\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 60 -->\n      <g transform=\"translate(7.2 142.967187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_83\">\n    <path d=\"M 26.925 146.6 \nL 26.925 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_84\">\n    <path d=\"M 162.825 146.6 \nL 162.825 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_85\">\n    <path d=\"M 26.925 146.6 \nL 162.825 146.6 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_86\">\n    <path d=\"M 26.925 10.7 \nL 162.825 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4374e689c5\">\n   <rect height=\"135.9\" width=\"135.9\" x=\"26.925\" y=\"10.7\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEq5IgFp1zU8",
        "colab_type": "text"
      },
      "source": [
        "# 2. Single Shot Detector là gì?\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/LkrGKUh.png)\n",
        "\n",
        "**Hình 2**: Cách thức phân chia feature map để nhận diện các hình ảnh với những kích thước khác nhau.\n",
        "\n",
        "SSD chỉ cần duy nhất đầu vào là 1 bức ảnh và các ground truth boxes ám chỉ vị trí bounding box các vật thể trong suốt quá trình huấn luyện. Trong quá trình phát hiện vật thể, trên mỗi một feature map, chúng ta đánh giá các một tợp hợp nhỏ gồm những default boxes tương ứng với các tỷ lệ hướng khác nhau (aspect ratio) lên các features map có kích thước (scales) khác nhau (chẳng hạn kích thước 8x8 và 4x4 trong hình (b) và (c)). Đối với mỗi default box (các boxes nét đứt trong hình) ta cần dự báo một phân phối xác suất $\\mathbf{c} = (c_1, c_2, ..., c_n)$ tương ứng với các class $C = \\{C_1, C_2, ..., C_n\\}$. Tại thời điểm huấn luyện, đầu tiên chúng ta cần match default boxes với ground truth boxes sao cho mức độ sai số được đo lường qua localization loss là nhỏ nhất (thường là hàm Smooth L1). Sau đó ta sẽ tìm cách tối thiểu hóa sai số của nhãn dự báo tương ứng với mỗi vật thể được phát hiện trong default boxes thông qua confidence loss (thường là hàm softmax).\n",
        "\n",
        "Như vậy loss function của object detection sẽ khác với loss function của các tác vụ image classification ở chỗ có thêm localization loss về sai số vị trí của predicted boxes so với ground truth boxes.\n",
        "\n",
        "Đó là nguyên lý hoạt động chung nhất của SSD. Tuy nhiên kiến trúc các layers và hàm loss function của SSD cụ thể là gì ta sẽ tìm hiểu biên dưới.\n",
        "\n",
        "## 2.1. Kiến trúc của mô hình\n",
        "\n",
        "![](https://i.imgur.com/QW2VHUI.png)\n",
        "\n",
        "**Hình 3**: Sơ đồ kiến trúc của mạng SSD.\n",
        "\n",
        "SSD dựa trên một tiến trình lan truyền thuận của một kiến trúc chuẩn (chẳng hạn VGG16) để tạo ra một khối feature map output gồm 3 chiều ở giai đoạn sớm. Chúng ta gọi kiến trúc mạng này là base network. Sau đó chúng ta sẽ thêm những kiến trúc phía sau base network để tiến hành nhận diện vật thể như trong sơ đồ. Các layers này được diễn giải đơn giản như sau:\n",
        "\n",
        "* **Các layer của mô hình SSD**: \n",
        "  * Input Layer: Nhận input đầu vào là các bức ảnh có kích thước `(width x height x channels) = 300 x 300 x 3` đối với kiến trúc SSD300 hoặc `500 x 500 x 3` đối với kiến trúc SSD500.\n",
        "  * Conv5_3 Layer: Chính là base network sử dụng kiến trúc của VGG16 nhưng loại bỏ một số layers fully connected ở cuối cùng. Output của layer này chính là Conv4_3 Layer và là một feature map có kích thước `38 x 38 x 512`.\n",
        "  * Conv4_3 Layer: Ta có thể coi Conv4_3 là một feature map có kích thước `38 x 38 x 512`. Trên feature map này ta sẽ áp dụng 2 biến đổi chính đó là: Áp dụng một convolutional layer như một mạng CNN thông thường để thu được output layer tiếp theo. Cụ thể convolutional layer có convolutional kernel có kích thước `3 x 3 x 1024`, đầu ra thu được Conv6 có kích thước là `19 x 19 x 1024`.\n",
        "  Đồng thời ở bước này ta cũng áp dụng một classifier (như trong sơ đồ) và cũng dựa trên convolutional filter kích thước `3 x 3` để nhằm nhận diện vật thể trên feature map. Đây là một quá trình khá phức tạp vì nó phải đảm bảo phát hiện vật thể (thông qua phát hiện bounding box) và phân loại vật thể. Quá trình này thực hiện tương tự như mô tả ở hình 2. Chẳng hạn như feature map của chúng ta là `38 x 38 x 512` thì sẽ cần chia nó thành một grid cell kích thước `38 x 38` (bỏ qua độ sâu vì ta sẽ thực hiện tích chập trên toàn bộ độ sâu).\n",
        "  Sau đó mỗi một cell trên grid cell sẽ tạo ra 4 default bounding boxes với các tỷ lệ khung hình khác nhau (aspect ratio), mỗi một default bounding box ta cần tìm các tham số sau: phân phối xác suất của nhãn là một véc tơ có n_classes + 1 chiều (Lưu ý số lượng classes luôn cộng thêm 1 cho background). Đồng thời chúng ta cần thêm 4 tham số là offsets để xác định bounding box của vật thể trong khung hình. Do đó trên một default bounding box sẽ có n_classes + 4 tham số và trên 1 cell sẽ có 4*(n_classes+4) output cần dự báo. Nhân với số cells của Conv4_3 để thu được số lượng output là một tensor kích thước `38 x 38 x 4 x (n_classes+5)`, trong trường hợp coi background cũng là 1 nhãn thì tensor có kích thước `38 x 38 x 4 x (n_classes+4)`. Và số lượng các bounding box được sản sinh ra là `38 x 38 x 4`.\n",
        "  \n",
        "  * Qúa trình áp dụng classifier lên feature map cũng tương tự với các layer Conv7, Conv8_2, Conv_9, Conv10_2, Conv11_2. Shape của các layer sau sẽ phụ thuộc vào cách thức áp dụng mạng tích chập ở layer trước. Cụ thể ở đây là kernel_size áp dụng (như trong sơ đồ trên thì kernel_size luôn là `3 x 3`) và stride (độ lớn bước nhảy) của kernel_size.\n",
        "  Số lượng các default boxes sản sinh ra ở các layers tiếp theo lần lượt như sau:\n",
        "  * Conv7: 19×19×6 = 2166 boxes (6 boxes/cell)\n",
        "  * Conv8_2: 10×10×6 = 600 boxes (6 boxes/cell)\n",
        "  * Conv9_2: 5×5×6 = 150 boxes (6 boxes/cell)\n",
        "  * Conv10_2: 3×3×4 = 36 boxes (4 boxes/cell)\n",
        "  * Conv11_2: 1×1×4 = 4 boxes (4 boxes/cell)\n",
        "  Tổng số lượng các boxes là: 5776 + 2166 + 600 + 150 + 36 +4 = 8732. Tức là chúng ta cần phải dự đoán class cho khoảng 8732 khung hình ở output. Số lượng này lớn hơn rất nhiều so với YOLO khi chỉ phải dự đoán chỉ 98 khung hình ở output. Các bạn có hình dung ra một khung hình được đại diện bởi những tham số nào không? Tôi xin nhắc lại đó là các tham số:\n",
        "  \n",
        "  $$y^{T} = [\\underbrace{x, y, w, h}_{\\text{bounding box}}, \\underbrace{c_1, c_2,..., c_C}_{\\text{scores of C classes}}]$$\n",
        "  \n",
        "* **Áp dụng các feature map với các kích thước khác nhau**: Sau khi thu được feature map base ở base network. Chúng ta sẽ tiếp tục thêm các convolutional layers. Những layers này sẽ nhằm giảm kích thước của feature map và cho phép dự báo và nhận diện vật thể ở nhiều kích thước khác nhau. Những feature map có kích thước lớn sẽ phát hiện tốt các vật thể nhỏ và các feature map kích thước nhỏ giúp phát hiện tốt hơn các vật thể lớn. Cụ thể hơn về kích thước kernel filters sẽ xem ở sơ đồ kiến trúc trong phần Extra Features Layers.\n",
        "\n",
        "* **Dự báo thông qua mạng tích chập đối với object**: Mỗi một feature layer thêm vào ở Extra Features Layers sẽ tạo ra một tợp hợp cố định các nhận diện dự báo (detection prediction) khác nhau nhờ sử dụng một tợp hợp các kernel filters với đa dạng các kích thước. Kích thước ở đầu ra (`with x height x chanel`) ở mỗi loại kích thước feature layer sẽ phụ thuộc vào kernal filters và được tính toán hoàn toàn tương tự như đối với mạng neural tích chập thông thường. Xem thêm mục 1. lý thuyết về mạng tích chập trong [Giới thiệu mạng neural tích chập](https://www.kaggle.com/phamdinhkhanh/convolutional-neural-network) để hiểu tích chập được tính như thế nào và output shape là bao nhiêu.\n",
        "\n",
        "* **Default box và tỷ lệ hướng (aspect ratio)**: Chúng ta cần liên kết một tợp hợp default bounding boxes với mỗi một cell trên feature map, thực hiện tương tự cho các features map khác ở cuối của network. Các default boxes sẽ chạy lát gạch trên feature map theo thứ tự từ trên xuống dưới và từ trái qua phải để tính tích chập, do đó vị trí của mỗi default box tương ứng với cell mà nó liên kết là cố định tương đối trên bức ảnh. Tại mỗi một ô của feature map chúng ta dự báo 4 offsets tương ứng với một kích thước default box. 4 offsets ở đây được hiểu là một tọa độ gồm 4 tham số $(c_x, c_y, w, h)$. Trong đó $(c_x, c_y)$ giúp xác định tâm và $(w, h)$ là kích thước dài rộng của bounding box. Thành phần thứ 2 được dự báo là điểm số của bounding box tương ứng với mỗi class. Lưu ý ta sẽ có thêm một class thứ $C+1$ để đánh dấu trường hợp default bounding box không có vật thể (hoặc rơi vào background).\n",
        "\n",
        "  * Ví dụ đối với một feature map có kích thước `m x n` tương ứng với `p` channels (chẳng hạn như kích thước 8 x 8 hoặc 4 x 4), một kernel filter kích thước `3 x 3 x p` sẽ được áp dụng trên toàn bộ feature layer. \n",
        "  * Các giá trị trong kernel này chính là các tham số của mô hình và được tinh chỉnh trong quá trình training. \n",
        "  * Các kernel filter sẽ dự đoán đồng thời **Xác suất nhãn** và **kích thước offset** tương ứng với tọa độ của default box. \n",
        "  * Với mỗi location (hoặc cell) nằm trên feature map ta sẽ liên kết nó với $k$ bounding boxes. Các boxes này có kích thước khác nhau và tỷ lệ hướng khác nhau.\n",
        "  * Với mỗi một bounding box, chúng ta tính được phân phối điểm của $C$ classes là $c = (c_1, c_2, ... ,c_C)$ và 4 offsets tương ứng với kích thước ban đầu của default bounding box.\n",
        "  * Kết quả cuối cùng ta thu được $(C+4)\\times mnk$ outputs.\n",
        "  \n",
        "  Các default box của chúng ta tương tự như anchor boxes trong mạng faster R-CNN nhưng được áp dụng trên một vài feature maps với những độ phân giải khác nhau. Điều này cho phép các default bounding box phân biệt hiệu quả kích thước vật thể khác nhau.\n",
        "  \n",
        "Kết thúc phần này chúng ta đã hiểu được kiến trúc các layer của mạng SSD. Tuy nhiên quá trình huấn luyện và hàm loss function của SSD vẫn còn là một điều bí ẩn. Liệu hàm loss function của SSD có gì khác so với các thuật toán Image classification? Quá trình tối ưu cần xét đến những mất mát nào? Hãy tìm  hiểu ở phần tiếp theo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmOdKZzl-9eB",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Qúa trình huấn luyện\n",
        "\n",
        "**Chiến lược mapping default box** Trong suốt quá trình huấn luyện ta cần mapping các default boxes có tỷ lệ aspect ratio khác nhau với ground truth box. Để mapping được chúng với nhau ta cần đo lường chỉ số IoU (Intersection of Union) hoặc chỉ số [Jaccard overlap index](https://en.wikipedia.org/wiki/Jaccard_index) được dùng để đo lường tỷ lệ diện tích giao nhau giữa 2 vùng hình ảnh so với tổng diện tích (không tính phần giao nhau) của chúng. Chúng ta sẽ match các default boxes với bất kì ground truth nào có threshold > 0.5. \n",
        "\n",
        "Như chúng ta đã biết trên mỗi cell chúng ta chỉ qui định một số lượng nhất định (4 hoặc 6, tùy từng feature map) các default bounding box. Vậy các default bounding box này được xác định trước thông qua aspect ratio và scale hay ngẫu nhiên? Trên thực tế là chúng hầu hết được xác định từ trước để giảm thiểu số lượng khung hình/cell cần dự báo. Tợp hợp các khung hình được xác định ra phải đảm bảo sao cho mỗi một ground truth bất kì đều có thể tìm được một default bounding box gần nó nhất. Do đó một thuật toán K-mean clustering được thực hiện trên aspect ratio của mỗi ground truth image nhằm phân cụm các khung hình trên thực tế. Tâm của các clusters (còn gọi là centroids) sẽ được dùng làm các giá trị aspect ratio đại diện để tính default bounding box. Tôi hi vọng các bạn hiểu những gì tôi vừa trình bày? Không quá phức tạp phải không?\n",
        "\n",
        "**Huấn luyện để tìm ra object**: Việc dự báo các object sẽ được thực hiện trên tợp hợp các khung hình output của mạng SSD. Đặt $x_{ij}^k = \\{0, 1\\}$ là chỉ số đánh giá cho việc matching giữa default bounding box thứ $i$ với ground truth box thứ $j$ đối với nhãn thứ $k$. Trong quá trình mapping chúng ta có thể có nhiều bounding box được map vào cùng 1 ground truth box với cùng 1 nhãn dự báo nên tổng $\\sum_{i}x_{ij}^k \\geq 1$. Hàm loss function là tổng có trọng số của localization loss (loc) và confidence loss (conf):\n",
        "\n",
        "$$L(x, c, p, g) = \\frac{1}{N}(L_{conf}(x, c) + \\alpha L_{loc}(x, p, g)) \\tag{1}$$\n",
        "\n",
        "Trong đó $N$ là số lượng các default boxes matching với ground truth boxes. Ta nhận thấy giá trị của hàm loss function của SSD hoàn toàn giống với faster R-CNN và bao gồm 2 thành phần:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBysO2T6bSLL",
        "colab_type": "text"
      },
      "source": [
        "1. **localization loss** là một hàm Smooth L1 đo lường sai số giữa tham số của box dự báo (predicted box) ($p$) và ground truth box ($g$) như bên dưới:. Chúng ta sẽ cần hồi qui các offsets cho tâm $(x, y)$ và của default bounding box ($d$) và các chiều dài $h$ và chiều rộng $w$.\n",
        "$$L_{loc}(x, p ,g) = \\sum_{i \\in Pos}^{N}\\sum_{m \\in \\{x, y, w, h\\}} x^{k}_{ij} \\space L_1^\\text{smooth}(p_i^m - \\hat{g}_j^m)$$\n",
        "Như vậy Localization loss chỉ xét trên các positive matching ($i \\in Pos$) giữa predicted bounding box với ground truth bounding box. Nếu $IoU > 0.5$ thì được coi là positive matching (tức predicted bounding box chứa vật thể). Trái lại, nếu $IoU <= 0.5$ ta không cần quan tâm và coi như xóa các predicted bounding box này khỏi hình ảnh. Thành phần $\\sum_{m \\in \\{x, y, w, h\\}} x^{k}_{ij} \\space L_1^\\text{smooth}(p_i^m - \\hat{g}_j^m)$ chính là tổng khoảng cách giữa predicted box ($p$) và ground truth box ($g$) trên 4 offsets $\\{x, y, w, h\\}$.\n",
        "\n",
        "Nếu để nguyên các giá trị tọa độ tâm và kích thước $(w, h)$ của khung hình sẽ rất khó để xác định sai số một cách chuẩn xá. Cụ thể ta hãy so sánh sai số trong trường hợp khung hình lớn và khung hình bé. Trong trường hợp khung hình lớn có predicted box và ground truth box rất khớp nhau. Tuy nhiên do khung hình quá to nên khoảng cách tâm của chúng sẽ lớn một chút, giả định là aspect ratio của chúng bằng nhau. Còn trường hợp khung hình bé, sai số của tâm giữa predicted box và ground truth box có thể bé hơn trường hợp khung hình lớn về số tuyệt đối. Nhưng điều đó không có nghĩa rằng predicted box và ground truth box của khung hình bé là rất khớp nhau. Chúng có thể cách nhau rất xa.\n",
        "\n",
        "Do đó chúng ta cần phải chuẩn hóa kích thước width, height và tâm sao cho không có sự khác biệt trong trường hợp khung hình bé và lớn. Một phép chuẩn hóa các offset được thực hiện nhưa sau:\n",
        "\n",
        "Các tham số $\\hat{g}^m$ được tính thông qua đo lường chênh lệch khoảng cách tâm và mức độ thay đổi kích thước (scale) dài và rộng giữa lần lượt ground truth box so với default box. Để hiều rõ hơn cách tính $\\hat{g}^m$ như thế nào ta xem hình minh họa đối với tính $\\hat{g}^{m}$ như bên dưới:\n",
        "![Transform predict box to ground truth box](https://imgur.com/ntyb7BW.png)\n",
        "\n",
        "> **Hình 4:** Hình chữ nhận viền đen đại diện cho ground truth box và hình chữ nhật viền đỏ đại diện cho default bounding box. $(d_w, d_h)$ lần lượt là kích thước dài rộng và $(d_x, d_y)$ là tọa độ tâm của default bounding box. Khi đó để chuyển từ tâm của default bounding box sang tâm của ground truth box ta cần 1 phép dịch chuyển tuyến tính các khoảng $(d_wt_x, d_ht_y)$. Kích thước các chiều dài và rộng được scale so với default bounding box số lần $(e^{t_h}, e^{t_w})$.\n",
        "\n",
        "  * Khoảng cách tâm $(p_x, p_y)$ của predicted box so với tâm $(d_x, d_y)$ của default box:\n",
        "$$\\hat{g}_x = \\frac{g_x-d_x}{d_w} \\triangleq t_{x}$$\n",
        "$$\\hat{g}_y = \\frac{g_y-d_y}{d_h} \\triangleq t_{y}$$\n",
        "\n",
        "  * Độ scale theo chiều dài và rộng $(p_w, p_h)$ của predicted box so với chiều dài và rộng $(d_w, d_h)$ của ground truth box:\n",
        "$$\\hat{g}_w = log(\\frac{d_w}{g_w}) \\triangleq t_{w}$$\n",
        "$$\\hat{g}_h = log(\\frac{d_h}{g_h}) \\triangleq t_{h}$$\n",
        "Kí hiệu $\\triangleq$ nghĩa là đặt vế trái bằng vế phải.\n",
        "Ta nhận thấy các giá trị $t_{x}, t_{y}, t_w, t_h$ là những tham số tinh chỉnh kích thước của bounding box nhận giá trị trong khoảng từ $(-\\infty, +\\infty)$. Nếu các giá trị của $t_x, t_y$ càng lớn thì khoảng cách giữa 2 tâm ground truth box và default box càng lớn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1fgkdmUKzPV",
        "colab_type": "text"
      },
      "source": [
        "Gía trị của $t_w, t_h$ càng lớn, tỷ lệ chêch lệch kích thước dài rộng giữa ground truth box và default box càng lớn. Ta gọi giá trị bộ tham số $(t_x, t_y, t_w, t_h)$ là bộ tham số kích thước chuẩn hóa của ground truth box theo default box. Tương tự ta cũng tính được bộ tham số kích thước chuẩn hóa của predicted box theo default box bằng cách thế $g_m$ bằng $p_m$ với $m \\in \\{x, y, w, h\\}$ trong những phương trình trên. Để hiểu hơn về các tham số chuẩn hóa có thể xem [Faster R-CNN - Mục A Loss Function for Learning Region Proposals](https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf). Khi đó khoảng cách giữa predicted box và ground truth box sẽ càng gần nếu khoảng cách giữa các bộ tham số chuẩn hóa giữa chúng càng gần. Tức khoảng cách giữa 2 véc tơ $p$ và $\\hat{g}$ càng nhỏ càng tốt.\n",
        "\n",
        "Khoảng cách này được đo lường thông qua hàm $L_1^\\text{smooth}$ là một kết hợp giữa norm chuẩn bậc 1 (đối với các giá trị tuyệt đối của $x$ lớn) và norm chuẩn bậc 2 (đối với các giá trị tuyệt đối của $x$ nhỏ) theo công thức sau:\n",
        "$$L_1^\\text{smooth}(x) = \\begin{cases}\n",
        "    0.5 x^2             & \\text{if } \\vert x \\vert < 1\\\\\n",
        "    \\vert x \\vert - 0.5 & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "Trong trường hợp $x$ là một véc tơ thì thay $x$ ở vế phải bằng giá trị norm chuẩn bậc 1 của $x$ kí hiệu là $|x|$. Việc lựa chọn hàm loss function là smooth L1 là để giá trị của đạo hàm gradient descent cố định khi $|x|$ lớn và smoothing khi $x$ nhỏ. Về norm chuẩn các bạn có thể xem trong bài tổng hợp [ML appendix](https://www.kaggle.com/phamdinhkhanh/ml-appendix). \n",
        "Trong phương trình của hàm localization loss thì các hằng số mà ta đã biết chính là $\\hat{g}$. Biến cần tìm giá trị tối ưu chính là $p$. Sau khi tìm ra được nghiệm tối ưu của $p$ ta sẽ tính ra predicted box nhờ phép chuyển đổi từ default box tương ứng.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KspxKIvdA09M",
        "colab_type": "text"
      },
      "source": [
        "2. **confidence loss** là một hàm mất mát được tính toán dựa trên sai số dự báo nhãn. Đối với mỗi một positive match prediction, chúng ta phạt loss function theo confidence score của các nhãn tương ứng. Đối với mỗi một negative match prediction, chúng ta phạt loss function theo confidence score của nhãn '0' là nhãn đại diện cho background không chứa vật thể. Cụ thể hàm confidence loss như bên dưới:\n",
        "$$L_{conf}(x, c) = -\\sum_{i \\in Pos} x_{ij}^{k}*log(\\hat{c}_{i}^k) - \\sum_{i \\in Neg}log(\\hat{c}_{i}^0)$$\n",
        "Trong trường hợp positive match prediction thì vùng được dự báo có vật thể chính xác là chứa vật thể. Do đó việc dự báo nhãn cho nó sẽ tương tự như một bài toán classification với hàm softmax thông thường có dạng $-\\sum_{i \\in Pos} x_{ij}^{k}*log(\\hat{c}_{i}^p)$. Trong trường hợp negative match prediction tức vùng được dự báo là không chứa vật thể chúng ta sẽ chỉ có duy nhất một nhãn là 0. Và tất nhiên ta đã biết trước bounding box là không chứa vật thể nên xác xuất để xảy ra nhóm 0 là $x_{ij}^{0} = 1$. Do đó hàm softmax có dạng $- \\sum_{i \\in Neg}log(\\hat{c}_{i}^0)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_NZwvhGNa0v",
        "colab_type": "text"
      },
      "source": [
        "Hàm loss function cuối cùng được tính là tổng của 2 confidence loss và localization loss như (1).\n",
        "\n",
        "**Lựa chọn kích cỡ (scales) và tỷ lệ hướng (aspect ratio):** \n",
        "\n",
        "Các default boundary box được lựa chọn thông qua aspect ratio và scales. SSD sẽ xác định một tỷ lệ scale tương ứng với mỗi một features map trong Extra Feature Layers. Bắt đầu từ bên trái, Conv4_3 phát hiện các object tại các scale nhỏ nhất là $s_{min} = 0.2$ (đôi khi là 0.1) và sau đó gia tăng tuyến tính để layer cuối cùng ở phía bên phải có scale là $s_{max} = 0.9$ theo công thức:\n",
        "\n",
        "$$s_k = s_{min} + \\frac{s_{max} - s_{min}}{m-1}(k-1), k \\in [1,m]$$\n",
        "\n",
        "Với $k$ là số thứ tự của layers. Kết hợp giữa giá trị scale với aspect ratio chúng ta sẽ tính được width và height của default boxes. Với các layers có 6 dự báo, SSD sẽ tạo ra 5 default boxes với các aspect ratios lần lượt là: 1, 2, 3, 1/2, 1/3. Sau đó width và height của default boxes được tính theo công thức:\n",
        "\n",
        "$$w = scale * \\sqrt{\\text{aspect ratio}}$$\n",
        "\n",
        "$$h = \\frac{scale}{\\sqrt{\\text{aspect ratio}}}$$\n",
        "\n",
        "Trong trường hợp aspect ratio = 1 thì ta sẽ thêm một default bounding box thứ 6 với scale được tính theo công thức:\n",
        "\n",
        "$$s_k' = \\sqrt{s_ks_{k+1}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiBOx44fG_I6",
        "colab_type": "text"
      },
      "source": [
        "# 3. Code\n",
        "\n",
        "Thuật toán SDD là một thuật toán rất phức tạp, có nhiều layers và các phases xử lý khác nhau. Tất nhiên code bên dưới là tôi tham khảo từ những nhà nghiên cứu xuất sắc trên thế giới. Đối với các kiến thức tôi trình bày ở các bài viết của mình cũng như vậy. Tôi cũng không phủ nhận đó là những kiến thức tổng hợp từ nhiều nguồn bởi không ai có thể nghĩ ra tất cả. Và tôi cũng nhận được một số lời chỉ trích là [cop nhặt](https://www.facebook.com/groups/machinelearningcoban/?multi_permalinks=778367739287302). Nhưng điều đó không làm tôi dừng lại bởi tôi nghĩ rằng việc học một kiến thức mới và viết lại theo cách hiểu của mình là hữu ích để lưu lại kiến thức cho mình. Và sẽ hữu ích hơn khi chia sẻ những kiến thức, những giá trị mà thuộc về của chung ấy cho mọi người. Vừa tốt cho tôi vừa đóng góp cho cộng đồng. Tôi tin rằng những lời chỉ trích chỉ là số rất nhỏ trong những sự động viên ủng hộ và đó là động lực để tôi tiếp tục viết bài. Sau tất cả, trên hết tôi luôn tôn trọng bản quyền các bài viết gốc mà tôi đã đọc và để trích dẫn trong mục tham khảo.\n",
        "\n",
        "Xin lỗi các bạn vì hơi lan man chút. Quay trở lại phần code mình họa cho thuật toán SSD được lấy từ nguồn [SSD_keras - git repository](https://github.com/pierluigiferrari/ssd_keras). Trong git project tác giả cũng hướng dẫn khá chi tiết cách thức vận hành. Tôi có đọc tham khảo và trích dẫn một số phần xử lý minh họa cho thuật toán."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJnLQD6_FXTo",
        "colab_type": "code",
        "outputId": "78a17ca1-d56e-4bb5-cb2c-d644a18078ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "# Mount folder\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path = '/content/gdrive/My Drive/Colab Notebooks/ssd_keras/ssd_keras-master'\n",
        "os.chdir(path)\n",
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.gitattributes',\n",
              " '.github',\n",
              " '.gitignore',\n",
              " 'CONTRIBUTING.md',\n",
              " 'ISSUE_TEMPLATE.md',\n",
              " 'LICENSE.txt',\n",
              " 'README.md',\n",
              " '__init__.py',\n",
              " 'bounding_box_utils',\n",
              " 'data_generator',\n",
              " 'eval_utils',\n",
              " 'examples',\n",
              " 'keras_layers',\n",
              " 'keras_loss_function',\n",
              " 'misc_utils',\n",
              " 'models',\n",
              " 'ssd300_evaluation.ipynb',\n",
              " 'ssd300_inference.ipynb',\n",
              " 'ssd512_inference.ipynb',\n",
              " 'ssd7_training.ipynb',\n",
              " 'ssd_encoder_decoder',\n",
              " 'training_summaries',\n",
              " 'weight_sampling_tutorial.ipynb',\n",
              " 'ssd300_evaluation_COCO.ipynb',\n",
              " 'pretrain_model',\n",
              " 'ssd300_training.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13zS2d4WQm67",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Keras Layers\n",
        "## 3.1.2. Anchor Box\n",
        "\n",
        "Phần tinh túy nhất của SSD có lẽ là việc xác định các layers output của anchor box (hoặc default bounding box) ở các feature map. anchor box layer sẽ nhận đầu vào ra một feature map có kích thước `(feature_width, feature_height, n_channels)` và các scales, aspect ratios, trả ra đầu ra là một tensor kích thước `(feature_width, feature_height, n_boxes, 4)`, trong đó chiều cuối cùng đại diện cho 4 offsets của bounding box như mô tả trong **Default box và tỷ lệ hướng (aspect ratio)** của mục 2.1.\n",
        "\n",
        "Code biến đổi khá phức tạp. Tôi trong đó các phần biến đổi chính được thực hiện trong hàm `call()`.\n",
        "\n",
        "* **Bước 1**: Từ scale, size (giá trị lớn nhất của with và height), và aspect ratio ta xác định kích thước các cạnh của các bounding box theo công thức:\n",
        "$$\\begin{cases}\n",
        "\\textit{box_h} = \\textit{scale * size} /\n",
        "\\sqrt{\\textit{aspect ratio}}\\\\\n",
        "\\textit{box_w} = \\textit{scale * size} * \\sqrt{\\textit{aspect ratio}}\n",
        "\\end{cases}$$\n",
        "* **Bước 2**: Từ các cell trên feature map chiếu lại trên ảnh input image để thu được step khoảng cách giữa các center point của mỗi cell theo công thức:\n",
        "\n",
        "$$\\begin{cases}\n",
        "\\textit{step_h} = \\textit{img_h} /\n",
        "\\textit{feature_map_h}\\\\\n",
        "\\textit{step_w} = \\textit{img_w} / \\textit{feature_map_w}\n",
        "\\end{cases}$$\n",
        "\n",
        "* **Bước 3**: Tính tọa độ các điểm $(c_x, c_y, w, h)$ trên hình ảnh gốc dựa trên phép linear interpolation qua hàm [np.linspace()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html).\n",
        "$$\\begin{cases}\n",
        "   c_x = \\textit{np.linspace(start_w, end_w, feature_map_w)}\\\\\n",
        "    c_y = \\textit{np.linspace(start_h, end_h, feature_map_h)}\n",
        "\\end{cases}$$\n",
        "Kết quả trả về là một tensor có shape là `(feature_width, feature_height, n_boxes, 4)`, trong đó chiếu cuối cùng = 4 tương ứng với các offsets của default bounding box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRypv-q3Qsk8",
        "colab_type": "code",
        "outputId": "e9e803e4-70ea-4224-8d0d-c822d0b83d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "from bounding_box_utils.bounding_box_utils import convert_coordinates\n",
        "\n",
        "class AnchorBoxes(Layer):\n",
        "    '''\n",
        "    Tác dụng: Tạo ra một output tensor chứa tọa độ của các anchor box và các biến thể dựa trên input tensor.\n",
        "    Một tợp hợp các 2D anchor boxes được tạo ra dựa trên aspect ratios và scale trên mỗi một cells của grid cells. Các hộp được tham số hóa bằng các tọa độ `(xmin, xmax, ymin, ymax)`\n",
        "    \n",
        "    Input shape:\n",
        "        4D tensor shape `(batch, channels, height, width)` nếu `dim_ordering = 'th'`\n",
        "        or `(batch, height, width, channels)` nếu `dim_ordering = 'tf'`.\n",
        "\n",
        "    Output shape:\n",
        "        5D tensor of shape `(batch, height, width, n_boxes, 8)`. \n",
        "        Chiều cuối cùng gồm 4 tọa độ của anchor box và 4 giá trị biến thể ở mỗi box.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_height,\n",
        "                 img_width,\n",
        "                 this_scale,\n",
        "                 next_scale,\n",
        "                 aspect_ratios=[0.5, 1.0, 2.0],\n",
        "                 two_boxes_for_ar1=True,\n",
        "                 this_steps=None,\n",
        "                 this_offsets=None,\n",
        "                 clip_boxes=False,\n",
        "                 variances=[0.1, 0.1, 0.2, 0.2],\n",
        "                 coords='centroids',\n",
        "                 normalize_coords=False,\n",
        "                 **kwargs):\n",
        "        '''\n",
        "\n",
        "        Arguments:\n",
        "            img_height (int): chiều cao input images.\n",
        "            img_width (int): chiều rộng input images.\n",
        "            this_scale (float): một giá trị float thuộc [0, 1], nhân tố scaling kích thước để tạo các anchor boxes dựa trên một tỷ lệ so với cạnh ngắn hơn trong width và height.\n",
        "            next_scale (float): giá trị tiếp theo của scale. Được thiết lập khi vào chỉ khi\n",
        "                `self.two_boxes_for_ar1 == True`.\n",
        "            aspect_ratios (list, optional): tợp hợp các aspect ratios của các default boxes được tạo ra từ layer này.\n",
        "            two_boxes_for_ar1 (bool, optional): Được sử dụng chỉ khi `aspect_ratios` = 1.\n",
        "                Nếu `True`, hai default boxes được tạo ra khi aspect ratio = 1. default box đầu tiên sử dụng scaling factor của layer tương ứng,\n",
        "                default box thứ 2 sử dụng trung bình hình học giữa scaling factor và next scaling factor.\n",
        "            clip_boxes (bool, optional): Nếu đúng `True`, giới hạn tọa độ anchor box nằm bên trong hình ảnh.\n",
        "            variances (list, optional): Tợp hợp gồm 4 giá trị floats > 0. Là các anchor box offset tương ứng với mỗi tọa độ chia cho giá trị variances tương ứng của nó.\n",
        "            coords (str, optional): Tọa độ của box được sử dụng trong model. Có thể là centroids định dạng `(cx, cy, w, h)` (tọa độ box center, width, height),\n",
        "                hoặc 'corners' định dạng `(xmin, ymin, xmax,  ymax)`, hoặc 'minmax' định dạng `(xmin, xmax, ymin, ymax)`.\n",
        "            normalize_coords (bool, optional): Nếu `True` mô hình sử dụng tọa độ tương đối thay vì tuyệt đối. Chẳng hạn mô hình dự đoán tọa độ nằm trong [0, 1] thay vì tọa độ tuyệt đối.\n",
        "        '''\n",
        "        if K.backend() != 'tensorflow':\n",
        "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
        "\n",
        "        if (this_scale < 0) or (next_scale < 0) or (this_scale > 1):\n",
        "            raise ValueError(\"`this_scale` must be in [0, 1] and `next_scale` must be >0, but `this_scale` == {}, `next_scale` == {}\".format(this_scale, next_scale))\n",
        "\n",
        "        if len(variances) != 4:\n",
        "            raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "        variances = np.array(variances)\n",
        "        if np.any(variances <= 0):\n",
        "            raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.this_scale = this_scale\n",
        "        self.next_scale = next_scale\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "        self.two_boxes_for_ar1 = two_boxes_for_ar1\n",
        "        self.this_steps = this_steps\n",
        "        self.this_offsets = this_offsets\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.variances = variances\n",
        "        self.coords = coords\n",
        "        self.normalize_coords = normalize_coords\n",
        "        # Tính toán số lượng boxes trên 1 cell. TH aspect ratios = 1 thì thêm 1 box.\n",
        "        if (1 in aspect_ratios) and two_boxes_for_ar1:\n",
        "            self.n_boxes = len(aspect_ratios) + 1\n",
        "        else:\n",
        "            self.n_boxes = len(aspect_ratios)\n",
        "        super(AnchorBoxes, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(AnchorBoxes, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        '''\n",
        "        Return: Trả về 1 anchor box tensor dựa trên shape của input tensor.\n",
        "\n",
        "        Tensor này được thiết kế như là hằng số và không tham gia vào quá trình tính toán.\n",
        "\n",
        "        Arguments:\n",
        "            x (tensor): 4D tensor có shape `(batch, channels, height, width)` nếu `dim_ordering = 'th'`\n",
        "                hoặc `(batch, height, width, channels)` nếu `dim_ordering = 'tf'`. Input cho layer này phải là output của các localization predictor layer.\n",
        "        '''\n",
        "        #####################################################\n",
        "        # Bước 1: Tính toán with và heigth của box với mỗi aspect ratio\n",
        "        #####################################################\n",
        "        # Cạnh ngẵn hơn của hình ảnh có thể được sử dụng để tính `w` và `h` sử dụng `scale` và `aspect_ratios`.\n",
        "        size = min(self.img_height, self.img_width)\n",
        "        # Tính toán box widths và heights cho toàn bộ aspect ratios\n",
        "        wh_list = []\n",
        "        for ar in self.aspect_ratios:\n",
        "            if (ar == 1):\n",
        "                # Tính anchor box thông thường khi aspect ratio = 1.\n",
        "                box_height = box_width = self.this_scale * size\n",
        "                wh_list.append((box_width, box_height))\n",
        "                if self.two_boxes_for_ar1:\n",
        "                    # Tính version lớn hơn của anchor box sử dụng the geometric mean của scale và next scale.\n",
        "                    box_height = box_width = np.sqrt(self.this_scale * self.next_scale) * size\n",
        "                    wh_list.append((box_width, box_height))\n",
        "            else:\n",
        "                # Trường hợp còn lại box_height = scale/sqrt(aspect ratio); box_width = scale*sqrt(aspect ratio)\n",
        "                box_height = self.this_scale * size // np.sqrt(ar)\n",
        "                box_width = int(self.this_scale * size * np.sqrt(ar))\n",
        "                wh_list.append((box_width, box_height))\n",
        "        # append vào width height list\n",
        "        wh_list = np.array(wh_list)\n",
        "\n",
        "        # Định hình input shape \n",
        "        if K.common.image_dim_ordering() == 'tf':\n",
        "            batch_size, feature_map_height, feature_map_width, feature_map_channels = x.get_shape().as_list()\n",
        "        else:\n",
        "            batch_size, feature_map_channels, feature_map_height, feature_map_width = x.get_shape().as_list()\n",
        "        \n",
        "        # Tính các center points của grid of box. Chúng là duy nhất đối với các aspect ratios.\n",
        "        #####################################################\n",
        "        # Bước 2: Tính các step size. Khoảng cách là bao xa giữa các anchor box center point theo chiều width và height.\n",
        "        #####################################################\n",
        "        if (self.this_steps is None):\n",
        "            step_height = self.img_height // feature_map_height\n",
        "            step_width = self.img_width // feature_map_width\n",
        "        else:\n",
        "            if isinstance(self.this_steps, (list, tuple)) and (len(self.this_steps) == 2):\n",
        "                step_height = self.this_steps[0]\n",
        "                step_width = self.this_steps[1]\n",
        "            elif isinstance(self.this_steps, (int, float)):\n",
        "                step_height = self.this_steps\n",
        "                step_width = self.this_steps\n",
        "        # Tính toán các offsets cho anchor box center point đầu tiên từ góc trên cùng bên trái của hình ảnh.\n",
        "        if (self.this_offsets is None):\n",
        "            offset_height = 0.5\n",
        "            offset_width = 0.5\n",
        "        else:\n",
        "            if isinstance(self.this_offsets, (list, tuple)) and (len(self.this_offsets) == 2):\n",
        "                offset_height = self.this_offsets[0]\n",
        "                offset_width = self.this_offsets[1]\n",
        "            elif isinstance(self.this_offsets, (int, float)):\n",
        "                offset_height = self.this_offsets\n",
        "                offset_width = self.this_offsets\n",
        "        #####################################################\n",
        "        # Bước 3: Tính toán các tọa độ của (cx, cy, w, h) theo tọa độ của image gốc.\n",
        "        #####################################################\n",
        "        # Bây h chúng ta có các offsets và step sizes, tính grid của anchor box center points.\n",
        "        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_height - 1) * step_height, feature_map_height)\n",
        "        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_width - 1) * step_width, feature_map_width)\n",
        "        cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
        "        cx_grid = np.expand_dims(cx_grid, -1) \n",
        "        cy_grid = np.expand_dims(cy_grid, -1) \n",
        "        \n",
        "\n",
        "        # Tạo một 4D tensor có shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        # Chiều cuối cùng sẽ chứa `(cx, cy, w, h)`\n",
        "        boxes_tensor = np.zeros((feature_map_height, feature_map_width, self.n_boxes, 4))\n",
        "\n",
        "        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, self.n_boxes)) # đặt cx\n",
        "        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, self.n_boxes)) # đặt cy\n",
        "        boxes_tensor[:, :, :, 2] = wh_list[:, 0] # đặt w\n",
        "        boxes_tensor[:, :, :, 3] = wh_list[:, 1] # đặt h\n",
        "        \n",
        "        # Chuyển `(cx, cy, w, h)` sang `(xmin, xmax, ymin, ymax)`\n",
        "        boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners')\n",
        "\n",
        "        # Nếu `clip_boxes` = True, giới hạn các tọa độ nằm trên boundary của hình ảnh\n",
        "        if self.clip_boxes:\n",
        "            x_coords = boxes_tensor[:,:,:,[0, 2]]\n",
        "            x_coords[x_coords >= self.img_width] = self.img_width - 1\n",
        "            x_coords[x_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[0, 2]] = x_coords\n",
        "            y_coords = boxes_tensor[:,:,:,[1, 3]]\n",
        "            y_coords[y_coords >= self.img_height] = self.img_height - 1\n",
        "            y_coords[y_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[1, 3]] = y_coords\n",
        "\n",
        "        # Nếu `normalize_coords` = True, chuẩn hóa các tọa độ nằm trong khoảng [0,1]\n",
        "        if self.normalize_coords:\n",
        "            boxes_tensor[:, :, :, [0, 2]] /= self.img_width\n",
        "            boxes_tensor[:, :, :, [1, 3]] /= self.img_height\n",
        "\n",
        "        if self.coords == 'centroids':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` to `(cx, cy, w, h)`.\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids', border_pixels='half')\n",
        "        elif self.coords == 'minmax':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2minmax', border_pixels='half')\n",
        "\n",
        "        # Tạo một tensor chứa các variances và append vào `boxes_tensor`. \n",
        "        variances_tensor = np.zeros_like(boxes_tensor) # shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        variances_tensor += self.variances # Mở rộng thêm variances\n",
        "        # Bây h `boxes_tensor` trở thành tensor kích thước `(feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.concatenate((boxes_tensor, variances_tensor), axis=-1)\n",
        "\n",
        "        # Bây h chuẩn bị trước một chiều cho `boxes_tensor` đại diện cho batch size và di chuyển copy theo chiều đó (theo kiểu lợp ngói, xem thêm np.tile)\n",
        "        #  ta được một 5D tensor kích thước `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.expand_dims(boxes_tensor, axis=0)\n",
        "        boxes_tensor = K.tile(K.constant(boxes_tensor, dtype='float32'), (K.shape(x)[0], 1, 1, 1, 1))\n",
        "\n",
        "        return boxes_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if K.common.image_dim_ordering() == 'tf':\n",
        "            batch_size, feature_map_height, feature_map_width, feature_map_channels = input_shape\n",
        "        else: \n",
        "            batch_size, feature_map_channels, feature_map_height, feature_map_width = input_shape\n",
        "        return (batch_size, feature_map_height, feature_map_width, self.n_boxes, 8)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'img_height': self.img_height,\n",
        "            'img_width': self.img_width,\n",
        "            'this_scale': self.this_scale,\n",
        "            'next_scale': self.next_scale,\n",
        "            'aspect_ratios': list(self.aspect_ratios),\n",
        "            'two_boxes_for_ar1': self.two_boxes_for_ar1,\n",
        "            'clip_boxes': self.clip_boxes,\n",
        "            'variances': list(self.variances),\n",
        "            'coords': self.coords,\n",
        "            'normalize_coords': self.normalize_coords\n",
        "        }\n",
        "        base_config = super(AnchorBoxes, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxViOYkD_UgS",
        "colab_type": "code",
        "outputId": "2b17b9e2-48b8-48b1-f98b-2b1deb4e56ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Test output of Anchor box\n",
        "import tensorflow as tf\n",
        "x = tf.random.normal(shape = (4, 38, 38, 512))\n",
        "\n",
        "aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
        "                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                         [1.0, 2.0, 0.5],\n",
        "                         [1.0, 2.0, 0.5]]\n",
        "two_boxes_for_ar1=True\n",
        "steps=[8, 16, 32, 64, 100, 300]\n",
        "offsets=None\n",
        "clip_boxes=False\n",
        "variances=[0.1, 0.1, 0.2, 0.2]\n",
        "coords='centroids'\n",
        "normalize_coords=True\n",
        "subtract_mean=[123, 117, 104]\n",
        "divide_by_stddev=None\n",
        "swap_channels=[2, 1, 0]\n",
        "confidence_thresh=0.01\n",
        "iou_threshold=0.45\n",
        "top_k=200\n",
        "nms_max_output_size=400\n",
        "\n",
        "\n",
        "# Thiết lập tham số\n",
        "img_height = 300 # Height of the model input images\n",
        "img_width = 300 # Width of the model input images\n",
        "img_channels = 3 # Number of color channels of the model input images\n",
        "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
        "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
        "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
        "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
        "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
        "scales = scales_pascal\n",
        "aspect_ratios = [[1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
        "two_boxes_for_ar1 = True\n",
        "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
        "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
        "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
        "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
        "normalize_coords = True\n",
        "\n",
        "\n",
        "anchors = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2])(x)\n",
        "print('anchors shape: ', anchors.get_shape())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "anchors shape:  (4, 38, 38, 4, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuPLgJBV6DE9",
        "colab_type": "text"
      },
      "source": [
        "## L2Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIA_p9BC6Dqs",
        "colab_type": "code",
        "outputId": "3877ca5e-38c2-41aa-d5f6-b502b843023c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "class L2Normalization(Layer):\n",
        "    '''\n",
        "    Performs L2 normalization on the input tensor with a learnable scaling parameter\n",
        "    as described in the paper \"Parsenet: Looking Wider to See Better\" (see references)\n",
        "    and as used in the original SSD model.\n",
        "\n",
        "    Arguments:\n",
        "        gamma_init (int): The initial scaling parameter. Defaults to 20 following the\n",
        "            SSD paper.\n",
        "\n",
        "    Input shape:\n",
        "        4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
        "        or `(batch, height, width, channels)` if `dim_ordering = 'tf'`.\n",
        "\n",
        "    Returns:\n",
        "        The scaled tensor. Same shape as the input tensor.\n",
        "\n",
        "    References:\n",
        "        http://cs.unc.edu/~wliu/papers/parsenet.pdf\n",
        "    '''\n",
        "\n",
        "    def __init__(self, gamma_init=20, **kwargs):\n",
        "        if K.common.image_dim_ordering() == 'tf':\n",
        "            self.axis = 3\n",
        "        else:\n",
        "            self.axis = 1\n",
        "        self.gamma_init = gamma_init\n",
        "        super(L2Normalization, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        gamma = self.gamma_init * np.ones((input_shape[self.axis],))\n",
        "        self.gamma = K.variable(gamma, name='{}_gamma'.format(self.name))\n",
        "        self.trainable_weights = [self.gamma]\n",
        "        super(L2Normalization, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output = K.l2_normalize(x, self.axis)\n",
        "        return output * self.gamma\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'gamma_init': self.gamma_init\n",
        "        }\n",
        "        base_config = super(L2Normalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "import tensorflow as tf\n",
        "x = tf.random.normal(shape = (4, 38, 38, 512))\n",
        "conv4_3 = L2Normalization(gamma_init=20, name='conv4_3_norm')(x)\n",
        "print(conv4_3.get_shape())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 38, 38, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaAwXAE2w_eB",
        "colab_type": "text"
      },
      "source": [
        "### 3.1.2. Model\n",
        "\n",
        "Các bước thực hiện để khởi tạo cấu trúc của mạng ssd_300 bao gồm:\n",
        "* **Bước 1**: Xây dựng kiến trúc mạng bao gồm:\n",
        "  * Bước 1.1: Xây dựng kiến trúc mạng base network theo VGG16 đã loại bỏ các fully connected layers ở cuối.\n",
        "  * Bước 1.2: Áp dụng các convolutional filter có kích thước `(3 x 3)` để tính toán ra features map.\n",
        "  * Bước 1.3: Xác định output phân phối xác suất theo các classes ứng với mỗi một default bounding box.\n",
        "  * Bước 1.4: Xác định output các tham số offset của default bounding boxes tương ứng với mỗi cell trên các features map.\n",
        "  * Bước 1.5: Bước 1.5: Tính toán các AnchorBoxes làm cơ sở để dự báo offsets cho các predicted bounding boxes bao quan vật thể. Gía trị của các AnchorBoxes chỉ hỗ trợ trong quá trình tính toán offsets và không xuất hiện ở output như giá trị cần dự báo.\n",
        "* **Bước 2**: Reshape lại các output để đưa chúng về kích thước của `(feature_map_w, feature_map_h, n_boxes, -1)`. Trong đó -1 đại diện cho chiều cuối cùng được tính dựa vào các chiều còn lại theo hàm reshape.\n",
        "* **Bước 3**: Liên kết các khối tensorflow output của bước 2 được tính từ confidence, các offsets của bounding box và các offsets của anchor box.\n",
        "* **Bước 4**: Kết nối với output. Thêm layers softmax trước confidence của bounding box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1dFWEJOxBTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, ZeroPadding2D, Reshape, Concatenate\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "\n",
        "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
        "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
        "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
        "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
        "\n",
        "def ssd_300(image_size,\n",
        "            n_classes,\n",
        "            mode='training',\n",
        "            l2_regularization=0.0005,\n",
        "            min_scale=None,\n",
        "            max_scale=None,\n",
        "            scales=None,\n",
        "            aspect_ratios_global=None,\n",
        "            aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5],\n",
        "                                     [1.0, 2.0, 0.5]],\n",
        "            two_boxes_for_ar1=True,\n",
        "            steps=[8, 16, 32, 64, 100, 300],\n",
        "            offsets=None,\n",
        "            clip_boxes=False,\n",
        "            variances=[0.1, 0.1, 0.2, 0.2],\n",
        "            coords='centroids',\n",
        "            normalize_coords=True,\n",
        "            subtract_mean=[123, 117, 104],\n",
        "            divide_by_stddev=None,\n",
        "            swap_channels=[2, 1, 0],\n",
        "            confidence_thresh=0.01,\n",
        "            iou_threshold=0.45,\n",
        "            top_k=200,\n",
        "            nms_max_output_size=400,\n",
        "            return_predictor_sizes=False):\n",
        "    '''\n",
        "    Xây dựng model SSD300 với keras.\n",
        "    Base network được sử dụng là VGG16.\n",
        "\n",
        "    Chú ý: Yêu cầu Keras>=v2.0; TensorFlow backend>=v1.0.\n",
        "\n",
        "    Arguments:\n",
        "        image_size (tuple): Kích thước image input `(height, width, channels)`.\n",
        "        n_classes (int): Số classes, chẳng hạn 20 cho Pascal VOC dataset, 80 cho MS COCO dataset.\n",
        "        mode (str, optional): Một trong những dạng 'training', 'inference' và 'inference_fast'. \n",
        "            'training' mode: Đầu ra của model là raw prediction tensor.\n",
        "            'inference' và 'inference_fast' modes: raw predictions được decoded thành tọa độ đã được filtered thông qua threshold.\n",
        "        l2_regularization (float, optional): L2-regularization rate. Áp dụng cho toàn bộ các convolutional layers.\n",
        "        min_scale (float, optional): Nhân tố scaling nhỏ nhất cho các size của anchor boxes. Tỷ lệ này được tính trên so sánh với cạnh ngắn hơn\n",
        "        của hình ảnh input.\n",
        "        max_scale (float, optional): Nhân tố scale lớn nhất cho các size của anchor boxes.\n",
        "        scales (list, optional): List các số floats chứa các nhân tố scaling của các convolutional predictor layer.\n",
        "            List này phải lớn hơn số lượng các predictor layers là 1 để sử dụng cho trường hợp aspect ratio = 1 sẽ tính thêm next scale.\n",
        "            Trong TH sử dụng scales thì interpolate theo min_scale và max_scale để tính list scales sẽ không được sử dụng.\n",
        "        aspect_ratios_global (list, optional): List của các aspect ratios mà các anchor boxes được tạo thành. List này được áp dụng chung trên toàn bộ các prediction layers.\n",
        "        aspect_ratios_per_layer (list, optional): List của các list aspect ratio cho mỗi một prediction layer.\n",
        "            Nếu được truyền vào sẽ override `aspect_ratios_global`.\n",
        "        two_boxes_for_ar1 (bool, optional): Chỉ áp dụng khi aspect ratio lists chứa 1. Sẽ bị loại bỏ trong các TH khác.\n",
        "            Nếu `True`, 2 anchor boxes sẽ được tạo ra ứng với aspect ratio = 1. anchor box đầu tiên tạo thành bằng cách sử scale, anchor box thứ 2 \n",
        "            được tạo thành bằng trung bình hình học của scale và next scale.\n",
        "        steps (list, optional): `None` hoặc là list với rất nhiều các phần tử có số lượng bằng với số lượng layers.\n",
        "            Mỗi phần tử đại diện cho mỗi một predictor layer có bao nhiêu pixels khoảng cách giữa các tâm của anchor box.\n",
        "            steps có thể gồm 2 số đại diện cho (step_width, step_height).\n",
        "            nếu không có steps nào được đưa ra thì chúng ta sẽ tính để cho khoảng các giữa các tâm của anchor box là bằng nhau\n",
        "        offsets (list, optional): None hoặc là các con số đại diện cho mỗi một predictor layer bao nhiêu pixels từ góc trên và bên trái mở rộng của ảnh\n",
        "        clip_boxes (bool, optional): Nếu `True`, giới hạn tọa độ các anchor box để nằm trong boundaries của image.\n",
        "        variances (list, optional): Một list gồm 4 số floats >0. Một anchor box offset tương ứng với mỗi tọa độ sẽ được chi cho giá trị variance tương ứng.\n",
        "        coords (str, optional): Tọa độ của box được sử dụng bên trong model (chẳng hạn, nó không là input format của ground truth labels). \n",
        "            Có thể là dạng 'centroids' format `(cx, cy, w, h)` (box center coordinates, width,\n",
        "            and height), 'minmax' format `(xmin, xmax, ymin, ymax)`, hoặc 'corners' format `(xmin, ymin, xmax, ymax)`.\n",
        "        normalize_coords (bool, optional): Được đặt là `True` nếu model được giả định sử dụng tọa độ tương đối thay vì tuyệt đối coordinates,\n",
        "            chẳng hạn nếu model dự báo tọa độ box nằm trong [0, 1] thay vì tọa độ tuyệt đối.\n",
        "        subtract_mean (array-like, optional): `None` hoặc một array object với bất kì shape nào mà dạng mở rộng phù hợp với shape của ảnh. Gía trị của nó được bớt đi từ độ lớn pixel của ảnh. The elements of this array will be\n",
        "            Chẳng hạn truyền vào một list gồm 3 số nguyên để tính toán trung bình chuẩn hóa cho các kênh của ảnh.\n",
        "        divide_by_stddev (array-like, optional): `None` hoặc một array object. Tương tự như subtract_mean nhưng được chia cho từ độ lớn của ảnh để tính chuẩn hóa.\n",
        "        swap_channels (list, optional): Là `False` hoặc một list các số nguyên biểu diễn thứ tự kì vọng mà trong đó đầu vào các channels của ảnh có thể được hoán đổi.\n",
        "        confidence_thresh (float, optional): Một số float nằm trong khoảng [0,1), là ngưỡng tin cậy nhỏ nhất trong phân loại của một lớp xảy ra.\n",
        "        iou_threshold (float, optional): Một float nằm trong khoảng [0,1]. Tất cả các boxes có chỉ số Jaccard similarity lớn hơn hoặc bằng `iou_threshold`\n",
        "            sẽ được xem xét là chứa vệt thể bên trong nó.\n",
        "        top_k (int, optional): Điểm dự báo cáo nhất được giữ trong mỗi batch item sau bước non-maximum suppression stage.\n",
        "        nms_max_output_size (int, optional): Số lượng lớn nhất các dự báo sẽ được chuyển qua bước NMS stage.\n",
        "        return_predictor_sizes (bool, optional): Nếu `True`, hàm số này sẽ không chỉ trả về mô hình, mà còn trả về \n",
        "            một list chứa các chiều của predictor layers.\n",
        "\n",
        "    Returns:\n",
        "        model: The Keras SSD300 model.\n",
        "        predictor_sizes (optional): Một numpy array chứa các phần `(height, width)` của output tensor shape tương ứng với mỗi convolutional predictor layer.\n",
        "\n",
        "    References:\n",
        "        https://arxiv.org/abs/1512.02325v5\n",
        "    '''\n",
        "\n",
        "    n_predictor_layers = 6 # Số lượng các preductor convolutional layers trong network là 6 cho original SSD300.\n",
        "    n_classes += 1 # Số lượng classes, + 1 để tính thêm background class.\n",
        "    l2_reg = l2_regularization # tham số chuẩn hóa của norm chuẩn l2.\n",
        "    img_height, img_width, img_channels = image_size[0], image_size[1], image_size[2]\n",
        "\n",
        "    ############################################################################\n",
        "    # Một số lỗi ngoại lệ.\n",
        "    ############################################################################\n",
        "    \n",
        "    if aspect_ratios_global is None and aspect_ratios_per_layer is None:\n",
        "        raise ValueError(\"`aspect_ratios_global` and `aspect_ratios_per_layer` cannot both be None. At least one needs to be specified.\")\n",
        "    if aspect_ratios_per_layer:\n",
        "        if len(aspect_ratios_per_layer) != n_predictor_layers:\n",
        "            raise ValueError(\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == {}, but len(aspect_ratios_per_layer) == {}.\".format(n_predictor_layers, len(aspect_ratios_per_layer)))\n",
        "    \n",
        "    # Tạo list scales\n",
        "    if (min_scale is None or max_scale is None) and scales is None:\n",
        "        raise ValueError(\"Either `min_scale` and `max_scale` or `scales` need to be specified.\")\n",
        "    if scales:\n",
        "        if len(scales) != n_predictor_layers+1:\n",
        "            raise ValueError(\"It must be either scales is None or len(scales) == {}, but len(scales) == {}.\".format(n_predictor_layers+1, len(scales)))\n",
        "    else: \n",
        "        scales = np.linspace(min_scale, max_scale, n_predictor_layers+1)\n",
        "\n",
        "    if len(variances) != 4:\n",
        "        raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "    variances = np.array(variances)\n",
        "    if np.any(variances <= 0):\n",
        "        raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "    if (not (steps is None)) and (len(steps) != n_predictor_layers):\n",
        "        raise ValueError(\"You must provide at least one step value per predictor layer.\")\n",
        "\n",
        "    if (not (offsets is None)) and (len(offsets) != n_predictor_layers):\n",
        "        raise ValueError(\"You must provide at least one offset value per predictor layer.\")\n",
        "\n",
        "    ############################################################################\n",
        "    # Tính các tham số của anchor box.\n",
        "    ############################################################################\n",
        "\n",
        "    # Thiết lập aspect ratios cho mỗi predictor layer (chỉ cần thiết cho tính toán anchor box layers).\n",
        "    if aspect_ratios_per_layer:\n",
        "        aspect_ratios = aspect_ratios_per_layer\n",
        "    else:\n",
        "        aspect_ratios = [aspect_ratios_global] * n_predictor_layers\n",
        "\n",
        "    # Tính số lượng boxes được dự báo / 1 cell cho mỗi predictor layer.\n",
        "    # Chúng ta cần biết bao nhiêu channels các predictor layers cần có.\n",
        "    if aspect_ratios_per_layer:\n",
        "        n_boxes = []\n",
        "        for ar in aspect_ratios_per_layer:\n",
        "            if (1 in ar) & two_boxes_for_ar1:\n",
        "                n_boxes.append(len(ar) + 1) # +1 cho trường hợp aspect ratio = 1\n",
        "            else:\n",
        "                n_boxes.append(len(ar))\n",
        "    else: # Nếu chỉ 1 global aspect ratio list được truyền vào thì số lượng boxes là như nhau cho mọi layers.\n",
        "        if (1 in aspect_ratios_global) & two_boxes_for_ar1:\n",
        "            n_boxes = len(aspect_ratios_global) + 1\n",
        "        else:\n",
        "            n_boxes = len(aspect_ratios_global)\n",
        "        n_boxes = [n_boxes] * n_predictor_layers\n",
        "\n",
        "    if steps is None:\n",
        "        steps = [None] * n_predictor_layers\n",
        "    if offsets is None:\n",
        "        offsets = [None] * n_predictor_layers\n",
        "\n",
        "    ############################################################################\n",
        "    # Xác định các hàm số cho Lambda layers bên dưới.\n",
        "    ############################################################################\n",
        "\n",
        "    def identity_layer(tensor):\n",
        "        return tensor\n",
        "\n",
        "    def input_mean_normalization(tensor):\n",
        "        return tensor - np.array(subtract_mean)\n",
        "\n",
        "    def input_stddev_normalization(tensor):\n",
        "        return tensor / np.array(divide_by_stddev)\n",
        "\n",
        "    def input_channel_swap(tensor):\n",
        "        if len(swap_channels) == 3:\n",
        "            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]]], axis=-1)\n",
        "        elif len(swap_channels) == 4:\n",
        "            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]], tensor[...,swap_channels[3]]], axis=-1)\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 1: Xây dựng network.\n",
        "    ############################################################################\n",
        "\n",
        "    x = Input(shape=(img_height, img_width, img_channels))\n",
        "\n",
        "    x1 = Lambda(identity_layer, output_shape=(img_height, img_width, img_channels), name='identity_layer')(x)\n",
        "    if not (subtract_mean is None):\n",
        "        x1 = Lambda(input_mean_normalization, output_shape=(img_height, img_width, img_channels), name='input_mean_normalization')(x1)\n",
        "    if not (divide_by_stddev is None):\n",
        "        x1 = Lambda(input_stddev_normalization, output_shape=(img_height, img_width, img_channels), name='input_stddev_normalization')(x1)\n",
        "    if swap_channels:\n",
        "        x1 = Lambda(input_channel_swap, output_shape=(img_height, img_width, img_channels), name='input_channel_swap')(x1)\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 1.1: Tính toán base network là mạng VGG16\n",
        "    ############################################################################\n",
        "\n",
        "    conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_1')(x1)\n",
        "    conv1_2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_2')(conv1_1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool1')(conv1_2)\n",
        "\n",
        "    conv2_1 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_1')(pool1)\n",
        "    conv2_2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_2')(conv2_1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool2')(conv2_2)\n",
        "\n",
        "    conv3_1 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_1')(pool2)\n",
        "    conv3_2 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_2')(conv3_1)\n",
        "    conv3_3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_3')(conv3_2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool3')(conv3_3)\n",
        "\n",
        "    conv4_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_1')(pool3)\n",
        "    conv4_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_2')(conv4_1)\n",
        "    conv4_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3')(conv4_2)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool4')(conv4_3)\n",
        "\n",
        "    conv5_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_1')(pool4)\n",
        "    conv5_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_2')(conv5_1)\n",
        "    conv5_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_3')(conv5_2)\n",
        "    pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='pool5')(conv5_3)\n",
        "        \n",
        "    ############################################################################\n",
        "    # Bước 1.2: Áp dụng các convolutional filter có kích thước (3 x 3) để tính toán ra features map.\n",
        "    ############################################################################\n",
        "\n",
        "    fc6 = Conv2D(1024, (3, 3), dilation_rate=(6, 6), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc6')(pool5)\n",
        "    print('fully connected 6: ', fc6.get_shape())\n",
        "    fc7 = Conv2D(1024, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7')(fc6)\n",
        "    print('fully connected 7: ', fc7.get_shape())\n",
        "    conv6_1 = Conv2D(256, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_1')(fc7)\n",
        "    conv6_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv6_padding')(conv6_1)\n",
        "    conv6_2 = Conv2D(512, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2')(conv6_1)\n",
        "    print('conv6_2: ', conv6_2.get_shape())\n",
        "    conv7_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_1')(conv6_2)\n",
        "    conv7_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv7_padding')(conv7_1)\n",
        "    conv7_2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2')(conv7_1)\n",
        "    print('conv7_2: ', conv7_2.get_shape())\n",
        "    conv8_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_1')(conv7_2)\n",
        "    conv8_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2')(conv8_1)\n",
        "    print('conv8_2: ', conv8_2.get_shape())\n",
        "    conv9_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_1')(conv8_2)\n",
        "    conv9_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2')(conv9_1)\n",
        "    print('conv9_2: ', conv9_2.get_shape())\n",
        "    # Feed conv4_3 vào the L2 normalization layer\n",
        "    conv4_3_norm = L2Normalization(gamma_init=20, name='conv4_3_norm')(conv4_3)\n",
        "    print('conv4_3_norm.shape: ', conv4_3_norm.get_shape())\n",
        "    \n",
        "    ############################################################################\n",
        "    # Bước 1.3: Xác định output phân phối xác suất theo các classes ứng với mỗi một default bounding box.\n",
        "    ############################################################################\n",
        "\n",
        "    ### Xây dựng các convolutional predictor layers tại top của base network\n",
        "    # Chúng ta dự báo các giá trị confidence cho mỗi box, do đó confidence predictors có độ sâu `n_boxes * n_classes`\n",
        "    # Đầu ra của confidence layers có shape: `(batch, height, width, n_boxes * n_classes)`\n",
        "    conv4_3_norm_mbox_conf = Conv2D(n_boxes[0] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_conf')(conv4_3_norm)\n",
        "    print('conv4_3_norm_mbox_conf.shape: ', conv4_3_norm_mbox_conf.get_shape())\n",
        "    fc7_mbox_conf = Conv2D(n_boxes[1] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_conf')(fc7)\n",
        "    print('fc7_mbox_conf.shape: ', fc7_mbox_conf.get_shape())\n",
        "    conv6_2_mbox_conf = Conv2D(n_boxes[2] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_conf')(conv6_2)\n",
        "    conv7_2_mbox_conf = Conv2D(n_boxes[3] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_conf')(conv7_2)\n",
        "    conv8_2_mbox_conf = Conv2D(n_boxes[4] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_conf')(conv8_2)\n",
        "    conv9_2_mbox_conf = Conv2D(n_boxes[5] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_conf')(conv9_2)\n",
        "    print('conv9_2_mbox_conf: ', conv9_2_mbox_conf.get_shape())\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 1.4: Xác định output các tham số offset của default bounding boxes tương ứng với mỗi cell trên các features map.\n",
        "    ############################################################################\n",
        "\n",
        "    # Chúng ta dự báo 4 tọa độ cho mỗi box, do đó localization predictors có độ sâu `n_boxes * 4`\n",
        "    # Output shape của localization layers: `(batch, height, width, n_boxes * 4)`\n",
        "    conv4_3_norm_mbox_loc = Conv2D(n_boxes[0] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_loc')(conv4_3_norm)\n",
        "    print('conv4_3_norm_mbox_loc: ', conv4_3_norm_mbox_loc.get_shape())\n",
        "    fc7_mbox_loc = Conv2D(n_boxes[1] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_loc')(fc7)\n",
        "    conv6_2_mbox_loc = Conv2D(n_boxes[2] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_loc')(conv6_2)\n",
        "    conv7_2_mbox_loc = Conv2D(n_boxes[3] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_loc')(conv7_2)\n",
        "    conv8_2_mbox_loc = Conv2D(n_boxes[4] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_loc')(conv8_2)\n",
        "    conv9_2_mbox_loc = Conv2D(n_boxes[5] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_loc')(conv9_2)\n",
        "    print('conv9_2_mbox_loc: ', conv9_2_mbox_loc.get_shape())\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 1.5: Tính toán các AnchorBoxes làm cơ sở để dự báo offsets cho các predicted bounding boxes bao quan vật thể\n",
        "    ############################################################################\n",
        "\n",
        "    ### Khởi tạo các anchor boxes (được gọi là \"priors\" trong code gốc Caffe/C++ của mô hình)\n",
        "    # Shape output của anchors: `(batch, height, width, n_boxes, 8)`\n",
        "    conv4_3_norm_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], aspect_ratios=aspect_ratios[0],\n",
        "                                             two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[0], this_offsets=offsets[0], clip_boxes=clip_boxes,\n",
        "                                             variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv4_3_norm_mbox_priorbox')(conv4_3_norm_mbox_loc)\n",
        "    print('conv4_3_norm_mbox_priorbox: ', conv4_3_norm_mbox_priorbox.get_shape())\n",
        "    fc7_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], aspect_ratios=aspect_ratios[1],\n",
        "                                    two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[1], this_offsets=offsets[1], clip_boxes=clip_boxes,\n",
        "                                    variances=variances, coords=coords, normalize_coords=normalize_coords, name='fc7_mbox_priorbox')(fc7_mbox_loc)\n",
        "    print('fc7_mbox_priorbox: ', fc7_mbox_priorbox.get_shape())\n",
        "    conv6_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], aspect_ratios=aspect_ratios[2],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[2], this_offsets=offsets[2], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv6_2_mbox_priorbox')(conv6_2_mbox_loc)\n",
        "    print('conv6_2_mbox_priorbox: ', conv6_2_mbox_priorbox.get_shape())\n",
        "    conv7_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], aspect_ratios=aspect_ratios[3],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[3], this_offsets=offsets[3], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv7_2_mbox_priorbox')(conv7_2_mbox_loc)\n",
        "    print('conv7_2_mbox_priorbox: ', conv7_2_mbox_priorbox.get_shape())\n",
        "    conv8_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[4], next_scale=scales[5], aspect_ratios=aspect_ratios[4],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[4], this_offsets=offsets[4], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv8_2_mbox_priorbox')(conv8_2_mbox_loc)\n",
        "    print('conv8_2_mbox_priorbox: ', conv8_2_mbox_priorbox.get_shape())\n",
        "    conv9_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[5], next_scale=scales[6], aspect_ratios=aspect_ratios[5],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[5], this_offsets=offsets[5], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv9_2_mbox_priorbox')(conv9_2_mbox_loc)\n",
        "    print('conv9_2_mbox_priorbox: ', conv9_2_mbox_priorbox.get_shape())\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 2: Reshape lại các output tensor shape\n",
        "    ############################################################################\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 2.1: Reshape output của class predictions\n",
        "    ############################################################################\n",
        "\n",
        "    # Reshape các class predictions, trả về 3D tensors có shape `(batch, height * width * n_boxes, n_classes)`\n",
        "    # Chúng ta muốn các classes là tách biệt nhau trên last axis để tính softmax trên chúng.\n",
        "    conv4_3_norm_mbox_conf_reshape = Reshape((-1, n_classes), name='conv4_3_norm_mbox_conf_reshape')(conv4_3_norm_mbox_conf)\n",
        "    fc7_mbox_conf_reshape = Reshape((-1, n_classes), name='fc7_mbox_conf_reshape')(fc7_mbox_conf)\n",
        "    conv6_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv6_2_mbox_conf_reshape')(conv6_2_mbox_conf)\n",
        "    conv7_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv7_2_mbox_conf_reshape')(conv7_2_mbox_conf)\n",
        "    conv8_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv8_2_mbox_conf_reshape')(conv8_2_mbox_conf)\n",
        "    conv9_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv9_2_mbox_conf_reshape')(conv9_2_mbox_conf)\n",
        "    print('conv4_3_norm_mbox_conf_reshape: ', conv4_3_norm_mbox_conf_reshape.get_shape())\n",
        "    print('fc7_mbox_conf_reshape: ', fc7_mbox_conf_reshape.get_shape())\n",
        "    print('conv9_2_mbox_conf_reshape: ', conv9_2_mbox_conf_reshape.get_shape())\n",
        "    print('conv9_2_mbox_conf_reshape: ', conv9_2_mbox_conf_reshape.get_shape())\n",
        "    print('conv9_2_mbox_conf_reshape: ', conv9_2_mbox_conf_reshape.get_shape())\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 2.2: Reshape output của bounding box predictions\n",
        "    ############################################################################\n",
        "\n",
        "    # Reshape các box predictions, trả về 3D tensors có shape `(batch, height * width * n_boxes, 4)`\n",
        "    # Chúng ta muốn 4 tọa độ box là tách biệt nhau trên last axis để tính hàm smooth L1 loss\n",
        "    conv4_3_norm_mbox_loc_reshape = Reshape((-1, 4), name='conv4_3_norm_mbox_loc_reshape')(conv4_3_norm_mbox_loc)\n",
        "    fc7_mbox_loc_reshape = Reshape((-1, 4), name='fc7_mbox_loc_reshape')(fc7_mbox_loc)\n",
        "    conv6_2_mbox_loc_reshape = Reshape((-1, 4), name='conv6_2_mbox_loc_reshape')(conv6_2_mbox_loc)\n",
        "    conv7_2_mbox_loc_reshape = Reshape((-1, 4), name='conv7_2_mbox_loc_reshape')(conv7_2_mbox_loc)\n",
        "    conv8_2_mbox_loc_reshape = Reshape((-1, 4), name='conv8_2_mbox_loc_reshape')(conv8_2_mbox_loc)\n",
        "    conv9_2_mbox_loc_reshape = Reshape((-1, 4), name='conv9_2_mbox_loc_reshape')(conv9_2_mbox_loc)\n",
        "    print('conv4_3_norm_mbox_loc_reshape: ', conv4_3_norm_mbox_loc_reshape.get_shape())\n",
        "    print('fc7_mbox_loc_reshape: ', fc7_mbox_loc_reshape.get_shape())\n",
        "    print('conv6_2_mbox_loc_reshape: ', conv6_2_mbox_loc_reshape.get_shape())\n",
        "    print('conv7_2_mbox_loc_reshape: ', conv7_2_mbox_loc_reshape.get_shape())\n",
        "    print('conv8_2_mbox_loc_reshape: ', conv8_2_mbox_loc_reshape.get_shape())\n",
        "    print('conv9_2_mbox_loc_reshape: ', conv9_2_mbox_loc_reshape.get_shape())\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 2.3: Reshape output của anchor box\n",
        "    ############################################################################\n",
        "\n",
        "    # Reshape anchor box tensors, trả về 3D tensors có shape `(batch, height * width * n_boxes, 8)`\n",
        "    conv4_3_norm_mbox_priorbox_reshape = Reshape((-1, 8), name='conv4_3_norm_mbox_priorbox_reshape')(conv4_3_norm_mbox_priorbox)\n",
        "    fc7_mbox_priorbox_reshape = Reshape((-1, 8), name='fc7_mbox_priorbox_reshape')(fc7_mbox_priorbox)\n",
        "    conv6_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv6_2_mbox_priorbox_reshape')(conv6_2_mbox_priorbox)\n",
        "    conv7_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv7_2_mbox_priorbox_reshape')(conv7_2_mbox_priorbox)\n",
        "    conv8_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv8_2_mbox_priorbox_reshape')(conv8_2_mbox_priorbox)\n",
        "    conv9_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv9_2_mbox_priorbox_reshape')(conv9_2_mbox_priorbox)\n",
        "    print('conv4_3_norm_mbox_priorbox_reshape: ', conv4_3_norm_mbox_priorbox_reshape.get_shape())\n",
        "    print('fc7_mbox_priorbox_reshape: ', fc7_mbox_priorbox_reshape.get_shape())\n",
        "    print('conv6_2_mbox_priorbox_reshape: ', conv6_2_mbox_priorbox_reshape.get_shape())\n",
        "    print('conv7_2_mbox_priorbox_reshape: ', conv7_2_mbox_priorbox_reshape.get_shape())\n",
        "    print('conv8_2_mbox_priorbox_reshape: ', conv8_2_mbox_priorbox_reshape.get_shape())\n",
        "    print('conv9_2_mbox_priorbox_reshape: ', conv9_2_mbox_priorbox_reshape.get_shape())\n",
        "    ### Concatenate các predictions từ các layers khác nhau\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 3: Concatenate các boxes trên layers\n",
        "    ############################################################################\n",
        "    \n",
        "    ############################################################################\n",
        "    # Bước 3.1: Concatenate confidence output box\n",
        "    ############################################################################\n",
        "\n",
        "    # Axis 0 (batch) và axis 2 (n_classes hoặc 4) là xác định duy nhất cho toàn bộ các predictions layer\n",
        "    # nên chúng ta muốn concatenate theo axis 1, số lượng các boxes trên layer\n",
        "    # Output shape của `mbox_conf`: (batch, n_boxes_total, n_classes)\n",
        "    mbox_conf = Concatenate(axis=1, name='mbox_conf')([conv4_3_norm_mbox_conf_reshape,\n",
        "                                                       fc7_mbox_conf_reshape,\n",
        "                                                       conv6_2_mbox_conf_reshape,\n",
        "                                                       conv7_2_mbox_conf_reshape,\n",
        "                                                       conv8_2_mbox_conf_reshape,\n",
        "                                                       conv9_2_mbox_conf_reshape])\n",
        "    print('mbox_conf.shape: ', mbox_conf.get_shape())\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 3.2: Concatenate location output box\n",
        "    ############################################################################\n",
        "\n",
        "    # Output shape của `mbox_loc`: (batch, n_boxes_total, 4)\n",
        "    mbox_loc = Concatenate(axis=1, name='mbox_loc')([conv4_3_norm_mbox_loc_reshape,\n",
        "                                                     fc7_mbox_loc_reshape,\n",
        "                                                     conv6_2_mbox_loc_reshape,\n",
        "                                                     conv7_2_mbox_loc_reshape,\n",
        "                                                     conv8_2_mbox_loc_reshape,\n",
        "                                                     conv9_2_mbox_loc_reshape])\n",
        "\n",
        "    print('mbox_loc.shape: ', mbox_loc.get_shape())\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 3.3: Concatenate anchor output box\n",
        "    ############################################################################\n",
        "\n",
        "    # Output shape của `mbox_priorbox`: (batch, n_boxes_total, 8)\n",
        "    mbox_priorbox = Concatenate(axis=1, name='mbox_priorbox')([conv4_3_norm_mbox_priorbox_reshape,\n",
        "                                                               fc7_mbox_priorbox_reshape,\n",
        "                                                               conv6_2_mbox_priorbox_reshape,\n",
        "                                                               conv7_2_mbox_priorbox_reshape,\n",
        "                                                               conv8_2_mbox_priorbox_reshape,\n",
        "                                                               conv9_2_mbox_priorbox_reshape])\n",
        "    \n",
        "    print('mbox_priorbox.shape: ', mbox_priorbox.get_shape())\n",
        "    \n",
        "    ############################################################################\n",
        "    # Bước 4: Tính toán output\n",
        "    ############################################################################\n",
        "\n",
        "    ############################################################################\n",
        "    # Bước 4.1 : Xây dựng các hàm loss function cho confidence\n",
        "    ############################################################################\n",
        "\n",
        "    # tọa độ của box predictions sẽ được truyền vào hàm loss function,\n",
        "    # nhưng cho các dự báo lớp, chúng ta sẽ áp dụng một hàm softmax activation layer đầu tiên\n",
        "    mbox_conf_softmax = Activation('softmax', name='mbox_conf_softmax')(mbox_conf)\n",
        "\n",
        "    # Concatenate các class và box predictions và the anchors thành một large predictions vector\n",
        "    # Đầu ra của `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n",
        "    predictions = Concatenate(axis=2, name='predictions')([mbox_conf_softmax, mbox_loc, mbox_priorbox])\n",
        "    print('predictions.shape: ', predictions.get_shape())\n",
        "    if mode == 'training':\n",
        "        model = Model(inputs=x, outputs=predictions)\n",
        "    elif mode == 'inference':\n",
        "        decoded_predictions = DecodeDetections(confidence_thresh=confidence_thresh,\n",
        "                                               iou_threshold=iou_threshold,\n",
        "                                               top_k=top_k,\n",
        "                                               nms_max_output_size=nms_max_output_size,\n",
        "                                               coords=coords,\n",
        "                                               normalize_coords=normalize_coords,\n",
        "                                               img_height=img_height,\n",
        "                                               img_width=img_width,\n",
        "                                               name='decoded_predictions')(predictions)\n",
        "        model = Model(inputs=x, outputs=decoded_predictions)\n",
        "    elif mode == 'inference_fast':\n",
        "        decoded_predictions = DecodeDetectionsFast(confidence_thresh=confidence_thresh,\n",
        "                                                   iou_threshold=iou_threshold,\n",
        "                                                   top_k=top_k,\n",
        "                                                   nms_max_output_size=nms_max_output_size,\n",
        "                                                   coords=coords,\n",
        "                                                   normalize_coords=normalize_coords,\n",
        "                                                   img_height=img_height,\n",
        "                                                   img_width=img_width,\n",
        "                                                   name='decoded_predictions')(predictions)\n",
        "        model = Model(inputs=x, outputs=decoded_predictions)\n",
        "    else:\n",
        "        raise ValueError(\"`mode` must be one of 'training', 'inference' or 'inference_fast', but received '{}'.\".format(mode))\n",
        "\n",
        "    if return_predictor_sizes:\n",
        "        predictor_sizes = np.array([conv4_3_norm_mbox_conf._keras_shape[1:3],\n",
        "                                     fc7_mbox_conf._keras_shape[1:3],\n",
        "                                     conv6_2_mbox_conf._keras_shape[1:3],\n",
        "                                     conv7_2_mbox_conf._keras_shape[1:3],\n",
        "                                     conv8_2_mbox_conf._keras_shape[1:3],\n",
        "                                     conv9_2_mbox_conf._keras_shape[1:3]])\n",
        "        return model, predictor_sizes\n",
        "    else:\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MOlkqxFxbGk",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Khởi tạo model\n",
        "\n",
        "Để khởi tạo mô hình chúng ta cần khai báo các tham số chính bao gồm:\n",
        "* img_height: Chiều cao hình ảnh input\n",
        "* img_width: Chiều rộng hình ảnh input\n",
        "* img_channels: Số kênh của hình ảnh input.\n",
        "* n_classes: Số lượng nhãn của bộ dữ liệu\n",
        "* scales: List các giá trị scales của mô hình ở mỗi một layer detector.\n",
        "* aspect_ratios: List các aspect ratios tương ứng ở mỗi layer detector.\n",
        "* variances: Các tham số biến đổi dùng để tính các anchor box.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u2Pg5G7GHEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# from models.keras_ssd300 import ssd_300\n",
        "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
        "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
        "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
        "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
        "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
        "\n",
        "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
        "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
        "\n",
        "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
        "from data_generator.object_detection_2d_geometric_ops import Resize\n",
        "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
        "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
        "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZFhOkdXGL3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_height = 300 # Height of the model input images\n",
        "img_width = 300 # Width of the model input images\n",
        "img_channels = 3 # Number of color channels of the model input images\n",
        "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
        "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
        "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
        "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
        "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
        "scales = scales_pascal\n",
        "aspect_ratios = [[1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
        "two_boxes_for_ar1 = True\n",
        "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
        "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
        "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
        "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
        "normalize_coords = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrREDVFeHCtd",
        "colab_type": "code",
        "outputId": "dcf4f1e6-5b09-429b-c689-47fd53150bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1: Build the Keras model.\n",
        "\n",
        "K.clear_session() # Clear previous models from memory.\n",
        "\n",
        "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
        "                n_classes=n_classes,\n",
        "                mode='training',\n",
        "                l2_regularization=0.0005,\n",
        "                scales=scales,\n",
        "                aspect_ratios_per_layer=aspect_ratios,\n",
        "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                steps=steps,\n",
        "                offsets=offsets,\n",
        "                clip_boxes=clip_boxes,\n",
        "                variances=variances,\n",
        "                normalize_coords=normalize_coords,\n",
        "                subtract_mean=mean_color,\n",
        "                swap_channels=swap_channels)\n",
        "\n",
        "# 2: Load some weights into the model.\n",
        "\n",
        "# TODO: Set the path to the weights you want to load.\n",
        "weights_path = 'pretrain_model/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
        "\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "\n",
        "# 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
        "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
        "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
        "\n",
        "#adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
        "\n",
        "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
        "\n",
        "model.compile(optimizer=sgd, loss=ssd_loss.compute_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "fully connected 6:  (?, 19, 19, 1024)\n",
            "fully connected 7:  (?, 19, 19, 1024)\n",
            "conv6_2:  (?, 10, 10, 512)\n",
            "conv7_2:  (?, 5, 5, 256)\n",
            "conv8_2:  (?, 3, 3, 256)\n",
            "conv9_2:  (?, 1, 1, 256)\n",
            "conv4_3_norm.shape:  (?, 38, 38, 512)\n",
            "conv4_3_norm_mbox_conf.shape:  (?, 38, 38, 84)\n",
            "fc7_mbox_conf.shape:  (?, 19, 19, 126)\n",
            "conv9_2_mbox_conf:  (?, 1, 1, 84)\n",
            "conv4_3_norm_mbox_loc:  (?, 38, 38, 16)\n",
            "conv9_2_mbox_loc:  (?, 1, 1, 16)\n",
            "conv4_3_norm_mbox_priorbox:  (?, 38, 38, 4, 8)\n",
            "fc7_mbox_priorbox:  (?, 19, 19, 6, 8)\n",
            "conv6_2_mbox_priorbox:  (?, 10, 10, 6, 8)\n",
            "conv7_2_mbox_priorbox:  (?, 5, 5, 6, 8)\n",
            "conv8_2_mbox_priorbox:  (?, 3, 3, 4, 8)\n",
            "conv9_2_mbox_priorbox:  (?, 1, 1, 4, 8)\n",
            "conv4_3_norm_mbox_conf_reshape:  (?, ?, 21)\n",
            "fc7_mbox_conf_reshape:  (?, ?, 21)\n",
            "conv9_2_mbox_conf_reshape:  (?, ?, 21)\n",
            "conv9_2_mbox_conf_reshape:  (?, ?, 21)\n",
            "conv9_2_mbox_conf_reshape:  (?, ?, 21)\n",
            "conv4_3_norm_mbox_loc_reshape:  (?, ?, 4)\n",
            "fc7_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv6_2_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv7_2_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv8_2_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv9_2_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv4_3_norm_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "fc7_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "conv6_2_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "conv7_2_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "conv8_2_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "conv9_2_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "mbox_conf.shape:  (?, ?, 21)\n",
            "mbox_loc.shape:  (?, ?, 4)\n",
            "mbox_priorbox.shape:  (?, ?, 8)\n",
            "predictions.shape:  (?, ?, 33)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/ssd_keras/ssd_keras-master/keras_loss_function/keras_ssd_loss.py:133: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/ssd_keras/ssd_keras-master/keras_loss_function/keras_ssd_loss.py:74: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/ssd_keras/ssd_keras-master/keras_loss_function/keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljJxIEiGKN9b",
        "colab_type": "text"
      },
      "source": [
        "2 phần xử lý trên chính là những xử lý mấu chốt của thuật toán mà chúng ta cần nắm được. Nếu bạn đọc muốn tìm hiểu kĩ hơn về toàn bộ các xử lý hãy tham khảo code gốc tại [SSD_keras - git repository](https://github.com/pierluigiferrari/ssd_keras) rất chi tiết.\n",
        "\n",
        "Khi đưa vào 1 hình ảnh, thuật toán sẽ trả về kết quả bao gồm các khung hình bao quan vật thể kèm theo nhãn và xác suất của lớp mà vật thể bao trong khung hình có thể thuộc về nhất. Thuật toán có thể dự báo nhiều vật thể có kích thước to nhỏ khác nhau.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qVzsf6ONWIl",
        "colab_type": "text"
      },
      "source": [
        "# 4. Tổng kết\n",
        "\n",
        "Như vậy qua bài viết này tôi đã trình bày cho bạn đọc tổng quát kiến trúc và cách thức hoạt động của thuật toán SSD. Đây là một trong những thuật toán có độ chính xác cao và tốc độ xử lý tương đối nhanh. Tôi xin tổng kết lại một số ý chính:\n",
        "\n",
        "* Kiến trúc của mô hình SSD bao gồm một base network là một mạng deep CNN được lược bỏ các layers fully connected ở giai đoạn đầu nhằm trích lọc các features.\n",
        "* Các bộ lọc tích chập kích thước (3 x 3) được áp dụng trên các features map ở những layers tiếp theo nhằm làm giảm kích thước của ảnh. Từ đó giúp nhận diện được các hình ảnh ở nhiều kích thước khác nhau.\n",
        "* Trên mỗi một cell của feature map ta tạo ra một tợp hợp các default bounding box có scale và aspect ratio khác nhau. Tọa độ của các default bounding box đựa sử dụng để dự báo offsets của khung hình bao quan vật thể.\n",
        "\n",
        "Hi vọng rằng chúng ta có thể nắm vững được thuật toán và tự xây dựng cho mình một mạng SSD để nhận diện vật thể.\n",
        "\n",
        "Cuối cùng không thể thiếu là các tài liệu mà tôi đã tham khảo để xây dựng bài viết này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1KIVG57PnDD",
        "colab_type": "text"
      },
      "source": [
        "# 5. Tài liệu.\n",
        "\n",
        "1. [SSD: Single Shot MultiBox Detector - Wei liu và cộng sự](https://arxiv.org/abs/1512.02325)\n",
        "2. [SSD: object detection single shot multibox detector for real time processing - jonathan hui](https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06)\n",
        "3. [SSD_keras github repository](https://github.com/pierluigiferrari/ssd_keras)\n",
        "4. [SSD caffe github repository - Weiliu](https://github.com/weiliu89/caffe/tree/ssd)\n",
        "5. [Bài 12 - Các thuật toán Object Detection](https://phamdinhkhanh.github.io/2019/09/29/OverviewObjectDetection.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8hRGbBeE0HU",
        "colab_type": "text"
      },
      "source": [
        "# Load pretrain model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPbQqLdKE2iy",
        "colab_type": "code",
        "outputId": "0e6e8285-21a3-4f23-86be-fa09d63a56fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "# TODO: Set the path to the `.h5` file of the model to be loaded.\n",
        "K.clear_session() # Clear previous models from memory.\n",
        "\n",
        "model2 = ssd_300(image_size=(img_height, img_width, img_channels),\n",
        "                n_classes=n_classes,\n",
        "                mode='training',\n",
        "                l2_regularization=0.0005,\n",
        "                scales=scales,\n",
        "                aspect_ratios_per_layer=aspect_ratios,\n",
        "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                steps=steps,\n",
        "                offsets=offsets,\n",
        "                clip_boxes=clip_boxes,\n",
        "                variances=variances,\n",
        "                normalize_coords=normalize_coords,\n",
        "                subtract_mean=mean_color,\n",
        "                swap_channels=swap_channels)\n",
        "\n",
        "model_path = 'pretrain_model/VGG_VOC0712_SSD_300x300_iter_120000.h5'\n",
        "\n",
        "\n",
        "# We need to create an SSDLoss object in order to pass that to the model loader.\n",
        "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
        "\n",
        "model2.load_weights(model_path, by_name = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fully connected 6:  (?, 19, 19, 1024)\n",
            "fully connected 7:  (?, 19, 19, 1024)\n",
            "conv6_2:  (?, 10, 10, 512)\n",
            "conv7_2:  (?, 5, 5, 256)\n",
            "conv8_2:  (?, 3, 3, 256)\n",
            "conv9_2:  (?, 1, 1, 256)\n",
            "conv4_3_norm.shape:  (?, 38, 38, 512)\n",
            "conv4_3_norm_mbox_conf.shape:  (?, 38, 38, 84)\n",
            "fc7_mbox_conf.shape:  (?, 19, 19, 126)\n",
            "conv9_2_mbox_conf:  (?, 1, 1, 84)\n",
            "conv4_3_norm_mbox_loc:  (?, 38, 38, 16)\n",
            "conv9_2_mbox_loc:  (?, 1, 1, 16)\n",
            "conv4_3_norm_mbox_priorbox:  (?, 38, 38, 4, 8)\n",
            "fc7_mbox_priorbox:  (?, 19, 19, 6, 8)\n",
            "conv6_2_mbox_priorbox:  (?, 10, 10, 6, 8)\n",
            "conv7_2_mbox_priorbox:  (?, 5, 5, 6, 8)\n",
            "conv8_2_mbox_priorbox:  (?, 3, 3, 4, 8)\n",
            "conv9_2_mbox_priorbox:  (?, 1, 1, 4, 8)\n",
            "conv4_3_norm_mbox_conf_reshape:  (?, ?, 21)\n",
            "fc7_mbox_conf_reshape:  (?, ?, 21)\n",
            "conv9_2_mbox_conf_reshape:  (?, ?, 21)\n",
            "conv9_2_mbox_conf_reshape:  (?, ?, 21)\n",
            "conv9_2_mbox_conf_reshape:  (?, ?, 21)\n",
            "conv4_3_norm_mbox_loc_reshape:  (?, ?, 4)\n",
            "fc7_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv6_2_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv7_2_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv8_2_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv9_2_mbox_loc_reshape:  (?, ?, 4)\n",
            "conv4_3_norm_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "fc7_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "conv6_2_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "conv7_2_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "conv8_2_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "conv9_2_mbox_priorbox_reshape:  (?, ?, 8)\n",
            "mbox_conf.shape:  (?, ?, 21)\n",
            "mbox_loc.shape:  (?, ?, 4)\n",
            "mbox_priorbox.shape:  (?, ?, 8)\n",
            "predictions.shape:  (?, ?, 33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj4q0yn_Dk_x",
        "colab_type": "text"
      },
      "source": [
        "# Setup Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkrClxG84mBF",
        "colab_type": "code",
        "outputId": "d9f049df-8380-4339-e1cc-a69ab2f835e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
        "from data_generator.object_detection_2d_geometric_ops import Resize\n",
        "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
        "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
        "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
        "\n",
        "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
        "\n",
        "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
        "\n",
        "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
        "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
        "\n",
        "# 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
        "\n",
        "# TODO: Set the paths to the datasets here.\n",
        "\n",
        "# The directories that contain the images.\n",
        "VOC_2007_images_dir      = '../../datasets/VOCdevkit/VOC2007/JPEGImages/'\n",
        "VOC_2012_images_dir      = '../../datasets/VOCdevkit/VOC2012/JPEGImages/'\n",
        "\n",
        "# The directories that contain the annotations.\n",
        "VOC_2007_annotations_dir      = '../../datasets/VOCdevkit/VOC2007/Annotations/'\n",
        "VOC_2012_annotations_dir      = '../../datasets/VOCdevkit/VOC2012/Annotations/'\n",
        "\n",
        "# The paths to the image sets.\n",
        "VOC_2007_train_image_set_filename    = '../../datasets/VOCdevkit/VOC2007/ImageSets/Main/train.txt'\n",
        "VOC_2012_train_image_set_filename    = '../../datasets/VOCdevkit/VOC2012/ImageSets/Main/train.txt'\n",
        "VOC_2007_val_image_set_filename      = '../../datasets/VOCdevkit/VOC2007/ImageSets/Main/val.txt'\n",
        "VOC_2012_val_image_set_filename      = '../../datasets/VOCdevkit/VOC2012/ImageSets/Main/val.txt'\n",
        "VOC_2007_trainval_image_set_filename = '../../datasets/VOCdevkit/VOC2007/ImageSets/Main/trainval.txt'\n",
        "VOC_2012_trainval_image_set_filename = '../../datasets/VOCdevkit/VOC2012/ImageSets/Main/trainval.txt'\n",
        "VOC_2007_test_image_set_filename     = '../../datasets/VOCdevkit/VOC2007/ImageSets/Main/test.txt'\n",
        "\n",
        "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
        "classes = ['background',\n",
        "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "           'bottle', 'bus', 'car', 'cat',\n",
        "           'chair', 'cow', 'diningtable', 'dog',\n",
        "           'horse', 'motorbike', 'person', 'pottedplant',\n",
        "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir,\n",
        "                                     VOC_2012_images_dir],\n",
        "                        image_set_filenames=[VOC_2007_trainval_image_set_filename,\n",
        "                                             VOC_2012_trainval_image_set_filename],\n",
        "                        annotations_dirs=[VOC_2007_annotations_dir,\n",
        "                                          VOC_2012_annotations_dir],\n",
        "                        classes=classes,\n",
        "                        include_classes='all',\n",
        "                        exclude_truncated=False,\n",
        "                        exclude_difficult=False,\n",
        "                        ret=False)\n",
        "\n",
        "val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
        "                      image_set_filenames=[VOC_2007_test_image_set_filename],\n",
        "                      annotations_dirs=[VOC_2007_annotations_dir],\n",
        "                      classes=classes,\n",
        "                      include_classes='all',\n",
        "                      exclude_truncated=False,\n",
        "                      exclude_difficult=True,\n",
        "                      ret=False)\n",
        "\n",
        "# Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
        "# speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
        "# option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
        "# want to create HDF5 datasets, comment out the subsequent two function calls.\n",
        "\n",
        "train_dataset.create_hdf5_dataset(file_path='dataset_pascal_voc_07+12_trainval.h5',\n",
        "                                  resize=False,\n",
        "                                  variable_image_size=True,\n",
        "                                  verbose=True)\n",
        "\n",
        "val_dataset.create_hdf5_dataset(file_path='dataset_pascal_voc_07_test.h5',\n",
        "                                resize=False,\n",
        "                                variable_image_size=True,\n",
        "                                verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-99c6ea6c37bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssd_300\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ssd_300' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR5t5uSCLu2f",
        "colab_type": "code",
        "outputId": "2fe771ae-54ad-48ac-d8f6-728074cfc2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "# 3: Set the batch size.\n",
        "\n",
        "batch_size = 32 # Change the batch size if you like, or if you run into GPU memory issues.\n",
        "\n",
        "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
        "\n",
        "# For the training generator:\n",
        "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
        "                                            img_width=img_width,\n",
        "                                            background=mean_color)\n",
        "\n",
        "# For the validation generator:\n",
        "convert_to_3_channels = ConvertTo3Channels()\n",
        "resize = Resize(height=img_height, width=img_width)\n",
        "\n",
        "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
        "\n",
        "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
        "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
        "\n",
        "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
        "                                    img_width=img_width,\n",
        "                                    n_classes=n_classes,\n",
        "                                    predictor_sizes=predictor_sizes,\n",
        "                                    scales=scales,\n",
        "                                    aspect_ratios_per_layer=aspect_ratios,\n",
        "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                                    steps=steps,\n",
        "                                    offsets=offsets,\n",
        "                                    clip_boxes=clip_boxes,\n",
        "                                    variances=variances,\n",
        "                                    matching_type='multi',\n",
        "                                    pos_iou_threshold=0.5,\n",
        "                                    neg_iou_limit=0.5,\n",
        "                                    normalize_coords=normalize_coords)\n",
        "\n",
        "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
        "\n",
        "train_generator = train_dataset.generate(batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         transformations=[ssd_data_augmentation],\n",
        "                                         label_encoder=ssd_input_encoder,\n",
        "                                         returns={'processed_images',\n",
        "                                                  'encoded_labels'},\n",
        "                                         keep_images_without_gt=False)\n",
        "\n",
        "val_generator = val_dataset.generate(batch_size=batch_size,\n",
        "                                     shuffle=False,\n",
        "                                     transformations=[convert_to_3_channels,\n",
        "                                                      resize],\n",
        "                                     label_encoder=ssd_input_encoder,\n",
        "                                     returns={'processed_images',\n",
        "                                              'encoded_labels'},\n",
        "                                     keep_images_without_gt=False)\n",
        "\n",
        "# Get the number of samples in the training and validations datasets.\n",
        "train_dataset_size = train_dataset.get_dataset_size()\n",
        "val_dataset_size   = val_dataset.get_dataset_size()\n",
        "\n",
        "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
        "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0cbf72552fa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m train_generator = train_dataset.generate(batch_size=batch_size,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                          \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                          \u001b[0mtransformations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mssd_data_augmentation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpcYPZHFLVni",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtjsbcuiOLVi",
        "colab_type": "text"
      },
      "source": [
        "# Tài liệu tham khảo\n",
        "\n",
        "https://arxiv.org/pdf/1512.02325.pdf\n"
      ]
    }
  ]
}